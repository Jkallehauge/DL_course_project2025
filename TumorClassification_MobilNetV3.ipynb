{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882fa7e5-32a3-4389-927d-a1fc6c3965f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up paths for data\n",
    "data_root = \"/home/jovyan/nnunet2-mig-7g-80gb-datavol-1/data/Brain-Tumor-Classification\" \n",
    "\n",
    "tr_base  = data_root+\"/Training\"\n",
    "te_base  = data_root+\"/Testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25045aee-19e2-4443-8225-c7eedb990b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Transforms \n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler # remember subset when you divide into training and validation\n",
    "from torchvision import datasets, transforms, utils, models # add models when you import pretrained models for transfer learning\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 512\n",
    "\n",
    "def zscore(x):  # MRI-friendly\n",
    "    return (x - x.mean()) / (x.std() + 1e-8)\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Lambda(lambda im: im.convert('L')),                 # use 'RGB' if truly 3-ch\n",
    "    transforms.Grayscale(num_output_channels=3), \n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), antialias=True),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.02,0.02), scale=(0.95,1.05)),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3, (0.1, 1.0))], p=0.2),\n",
    "    transforms.ColorJitter(brightness=0.10, contrast=0.10),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(zscore),\n",
    "])\n",
    "\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Lambda(lambda im: im.convert('L')),\n",
    "    transforms.Grayscale(num_output_channels=3), \n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(zscore),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2250b4ec-c036-46c3-a513-2fbf1e1bd012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 2902\n",
      "    Root location: /home/jovyan/nnunet2-mig-7g-80gb-datavol-1/data/Brain-Tumor-Classification/Training\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Lambda()\n",
      "               Grayscale(num_output_channels=3)\n",
      "               Resize(size=(512, 512), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n",
      "['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
      "['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
      "['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n"
     ]
    }
   ],
   "source": [
    "trainset=datasets.ImageFolder(tr_base, transform=train_tf)\n",
    "valset = datasets.ImageFolder(tr_base, transform=eval_tf)\n",
    "testset=datasets.ImageFolder(te_base, transform=eval_tf)\n",
    "\n",
    "print(valset)\n",
    "\n",
    "print(testset.classes)\n",
    "print(trainset.classes)\n",
    "print(valset.classes)\n",
    "\n",
    "val_fraction=0.2\n",
    "\n",
    "idxs = list(range(len(valset)))\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idxs)\n",
    "\n",
    "n_val = int(len(idxs) * val_fraction)\n",
    "val_idx = idxs[:n_val]\n",
    "train_idx = idxs[n_val:]\n",
    "\n",
    "trainset = Subset(trainset, train_idx)\n",
    "valset   = Subset(valset,   val_idx)\n",
    "\n",
    "class_names = datasets.ImageFolder(tr_base).classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b1f03f4-4bca-4fab-ad87-e604d7517907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_from_dataset(ds) -> np.ndarray:\n",
    "    \"\"\"Return class indices for ImageFolder or Subset(ImageFolder).\"\"\"\n",
    "    if isinstance(ds, Subset):\n",
    "        base, idxs = ds.dataset, ds.indices\n",
    "        if hasattr(base, \"targets\"):\n",
    "            return np.asarray(base.targets, dtype=int)[idxs]\n",
    "        return np.asarray([base.samples[i][1] for i in idxs], dtype=int)\n",
    "    # ImageFolder\n",
    "    if hasattr(ds, \"targets\"):\n",
    "        return np.asarray(ds.targets, dtype=int)\n",
    "    return np.asarray([y for _, y in ds.samples], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e360bd-166b-410c-b460-5a787a12c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assume you already created trainset (ImageFolder or Subset(ImageFolder))\n",
    "targets = get_targets_from_dataset(trainset)              # shape [N]\n",
    "num_classes = int(targets.max() + 1)\n",
    "\n",
    "# inverse-frequency class weights (simple & effective)\n",
    "class_counts  = np.bincount(targets, minlength=num_classes).astype(float)\n",
    "class_weights = 1.0 / np.maximum(class_counts, 1.0)      # avoid div/0\n",
    "sample_weights = class_weights[targets]                   # per-sample weight\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=torch.as_tensor(sample_weights, dtype=torch.double),\n",
    "    num_samples=len(sample_weights),   # one epoch ~= dataset size\n",
    "    replacement=True                   # allows oversampling of minority class\n",
    ")\n",
    "\n",
    "BATCH = 4\n",
    "WORKERS = 4\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    sampler=sampler,\n",
    "    num_workers=WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,                    # optional: keeps batch size stable\n",
    ")\n",
    "\n",
    "# validation/test loaders stay standard (no sampler)\n",
    "val_loader = DataLoader(valset, batch_size=64, shuffle=False, num_workers=WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01542d74-6dbb-4845-bcc9-024b6c1bb6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1280, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def build_model(n_classes, freeze_backbone=False):\n",
    "    \"\"\"\n",
    "    Build a MobileNetV3 model pretrained on ImageNet-1K,\n",
    "    replace classification head for custom number of classes,\n",
    "    and optionally freeze backbone layers.\n",
    "    \"\"\"\n",
    "    # 1️⃣ Load pretrained Swin Transformer Base\n",
    "    model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "\n",
    "    # 2️⃣ Replace classifier head\n",
    "    in_feats = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(in_feats, n_classes)\n",
    "    if freeze_backbone:\n",
    "        for p in model.features.parameters():\n",
    "            p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(n_classes=4, freeze_backbone=False)\n",
    "print(model)\n",
    "\n",
    "# Verify output\n",
    "import torch\n",
    "x = torch.randn(2, 3, 512, 512)\n",
    "out = model(x)\n",
    "print(out.shape)  # → [2, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05dd577a-db40-4a9f-8dfe-b67597b43788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Training utilities\n",
    "# ----------------------------\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=8, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.count = 0\n",
    "\n",
    "    def step(self, metric):\n",
    "        if self.best is None or metric > self.best + self.min_delta:\n",
    "            self.best = metric\n",
    "            self.count = 0\n",
    "            return False  # do not stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f69ba58b-ae3d-49c4-87e9-520e396056bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def epoch_run(model, loader, criterion, device, train: bool, use_amp: bool,\n",
    "              optimizer=None, scaler=None):\n",
    "    assert (train and optimizer is not None and scaler is not None) or (not train)\n",
    "\n",
    "    model.train() if train else model.eval()\n",
    "\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    # no graph during evaluation\n",
    "    cm = contextlib.nullcontext() if train else torch.no_grad()\n",
    "\n",
    "    with cm:\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=(use_amp and device.type == 'cuda')):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            running_loss += loss.detach().float().item() * xb.size(0)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca483300-4165-4768-abeb-0bfdf364358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def to_device(batch, device):\n",
    "    x, y = batch\n",
    "    return x.to(device, non_blocking=True), y.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e7d48e-5868-4b2d-bd4e-74f87f419db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_f1\u001b[39m(model, loader, device, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/__init__.py:73\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_missing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_indexing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     resample,\n\u001b[1;32m     19\u001b[0m     shuffle,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_chunking.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/validation.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/_array_api.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[1;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpx\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/fixes.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     pd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/__init__.py:46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     ArrowDtype,\n\u001b[1;32m     49\u001b[0m     Int8Dtype,\n\u001b[1;32m     50\u001b[0m     Int16Dtype,\n\u001b[1;32m     51\u001b[0m     Int32Dtype,\n\u001b[1;32m     52\u001b[0m     Int64Dtype,\n\u001b[1;32m     53\u001b[0m     UInt8Dtype,\n\u001b[1;32m     54\u001b[0m     UInt16Dtype,\n\u001b[1;32m     55\u001b[0m     UInt32Dtype,\n\u001b[1;32m     56\u001b[0m     UInt64Dtype,\n\u001b[1;32m     57\u001b[0m     Float32Dtype,\n\u001b[1;32m     58\u001b[0m     Float64Dtype,\n\u001b[1;32m     59\u001b[0m     CategoricalDtype,\n\u001b[1;32m     60\u001b[0m     PeriodDtype,\n\u001b[1;32m     61\u001b[0m     IntervalDtype,\n\u001b[1;32m     62\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     63\u001b[0m     StringDtype,\n\u001b[1;32m     64\u001b[0m     BooleanDtype,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     NA,\n\u001b[1;32m     67\u001b[0m     isna,\n\u001b[1;32m     68\u001b[0m     isnull,\n\u001b[1;32m     69\u001b[0m     notna,\n\u001b[1;32m     70\u001b[0m     notnull,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     Index,\n\u001b[1;32m     73\u001b[0m     CategoricalIndex,\n\u001b[1;32m     74\u001b[0m     RangeIndex,\n\u001b[1;32m     75\u001b[0m     MultiIndex,\n\u001b[1;32m     76\u001b[0m     IntervalIndex,\n\u001b[1;32m     77\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     78\u001b[0m     DatetimeIndex,\n\u001b[1;32m     79\u001b[0m     PeriodIndex,\n\u001b[1;32m     80\u001b[0m     IndexSlice,\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     NaT,\n\u001b[1;32m     83\u001b[0m     Period,\n\u001b[1;32m     84\u001b[0m     period_range,\n\u001b[1;32m     85\u001b[0m     Timedelta,\n\u001b[1;32m     86\u001b[0m     timedelta_range,\n\u001b[1;32m     87\u001b[0m     Timestamp,\n\u001b[1;32m     88\u001b[0m     date_range,\n\u001b[1;32m     89\u001b[0m     bdate_range,\n\u001b[1;32m     90\u001b[0m     Interval,\n\u001b[1;32m     91\u001b[0m     interval_range,\n\u001b[1;32m     92\u001b[0m     DateOffset,\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     to_numeric,\n\u001b[1;32m     95\u001b[0m     to_datetime,\n\u001b[1;32m     96\u001b[0m     to_timedelta,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     Flags,\n\u001b[1;32m     99\u001b[0m     Grouper,\n\u001b[1;32m    100\u001b[0m     factorize,\n\u001b[1;32m    101\u001b[0m     unique,\n\u001b[1;32m    102\u001b[0m     value_counts,\n\u001b[1;32m    103\u001b[0m     NamedAgg,\n\u001b[1;32m    104\u001b[0m     array,\n\u001b[1;32m    105\u001b[0m     Categorical,\n\u001b[1;32m    106\u001b[0m     set_eng_float_format,\n\u001b[1;32m    107\u001b[0m     Series,\n\u001b[1;32m    108\u001b[0m     DataFrame,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_f1(model, loader, device, average=\"macro\"):\n",
    "    model.eval()\n",
    "    all_preds, all_tgts = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(1)\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "        all_tgts.append(yb.detach().cpu())\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_tgts).numpy()\n",
    "    return f1_score(y_true, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "97b654cb-f493-43e4-a90f-c2b43f2b3e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Classes: ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
      "Train uses sampler: WeightedRandomSampler\n",
      "Val uses sampler:   SequentialSampler\n",
      "Test uses sampler:  SequentialSampler\n",
      "\u001b[96m\u001b[1mEpoch 01/500 | train_loss=0.5428 acc=0.8467 | val_loss=1.2459 acc=0.4539 F1=0.3875 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 02/500 | train_loss=0.3315 acc=0.9527 | val_loss=1.1344 acc=0.5235 F1=0.5085 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 03/500 | train_loss=0.3020 acc=0.9640 | val_loss=1.0364 acc=0.5304 F1=0.5352 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 04/500 | train_loss=0.2748 acc=0.9783 | val_loss=0.6586 acc=0.7757 F1=0.7635 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 05/500 | train_loss=0.2372 acc=0.9922 | val_loss=0.3968 acc=0.9217 F1=0.9218 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[1mEpoch 06/500 | train_loss=0.2222 acc=0.9957 | val_loss=0.4528 acc=0.8974 F1=0.8914\u001b[0m\n",
      "\u001b[1mEpoch 07/500 | train_loss=0.2253 acc=0.9952 | val_loss=0.7238 acc=0.8000 F1=0.8000\u001b[0m\n",
      "\u001b[1mEpoch 08/500 | train_loss=0.2330 acc=0.9913 | val_loss=0.5569 acc=0.8748 F1=0.8912\u001b[0m\n",
      "\u001b[1mEpoch 09/500 | train_loss=0.2231 acc=0.9944 | val_loss=0.5085 acc=0.8887 F1=0.8926\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 10/500 | train_loss=0.2150 acc=0.9965 | val_loss=0.2868 acc=0.9565 F1=0.9561 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 11/500 | train_loss=0.2139 acc=0.9978 | val_loss=0.2780 acc=0.9652 F1=0.9714 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 12/500 | train_loss=0.2134 acc=0.9974 | val_loss=0.2634 acc=0.9739 F1=0.9704 (✅ best loss & 💙 best acc)\u001b[0m\n",
      "\u001b[1mEpoch 13/500 | train_loss=0.2247 acc=0.9935 | val_loss=0.2844 acc=0.9704 F1=0.9691\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 14/500 | train_loss=0.2157 acc=0.9974 | val_loss=0.2332 acc=0.9896 F1=0.9903 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[1mEpoch 15/500 | train_loss=0.2101 acc=1.0000 | val_loss=0.3038 acc=0.9530 F1=0.9531\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 16/500 | train_loss=0.2079 acc=0.9996 | val_loss=0.2201 acc=0.9948 F1=0.9946 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[92m\u001b[1mEpoch 17/500 | train_loss=0.2044 acc=1.0000 | val_loss=0.2167 acc=0.9930 F1=0.9922 (✅ best loss)\u001b[0m\n",
      "\u001b[92m\u001b[1mEpoch 18/500 | train_loss=0.2047 acc=0.9996 | val_loss=0.2158 acc=0.9948 F1=0.9936 (✅ best loss)\u001b[0m\n",
      "\u001b[1mEpoch 19/500 | train_loss=0.2041 acc=1.0000 | val_loss=0.2238 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 20/500 | train_loss=0.2102 acc=0.9987 | val_loss=0.2441 acc=0.9791 F1=0.9767\u001b[0m\n",
      "\u001b[1mEpoch 21/500 | train_loss=0.2151 acc=0.9948 | val_loss=0.2420 acc=0.9861 F1=0.9871\u001b[0m\n",
      "\u001b[1mEpoch 22/500 | train_loss=0.2159 acc=0.9957 | val_loss=0.2573 acc=0.9757 F1=0.9738\u001b[0m\n",
      "\u001b[1mEpoch 23/500 | train_loss=0.2154 acc=0.9965 | val_loss=0.2570 acc=0.9809 F1=0.9832\u001b[0m\n",
      "\u001b[1mEpoch 24/500 | train_loss=0.2062 acc=0.9996 | val_loss=0.2360 acc=0.9896 F1=0.9902\u001b[0m\n",
      "\u001b[1mEpoch 25/500 | train_loss=0.2113 acc=0.9965 | val_loss=0.2474 acc=0.9826 F1=0.9832\u001b[0m\n",
      "\u001b[1mEpoch 26/500 | train_loss=0.2166 acc=0.9952 | val_loss=0.2437 acc=0.9826 F1=0.9864\u001b[0m\n",
      "\u001b[1mEpoch 27/500 | train_loss=0.2055 acc=0.9996 | val_loss=0.2253 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 28/500 | train_loss=0.2112 acc=0.9970 | val_loss=0.2436 acc=0.9826 F1=0.9801\u001b[0m\n",
      "\u001b[1mEpoch 29/500 | train_loss=0.2121 acc=0.9983 | val_loss=0.2483 acc=0.9809 F1=0.9815\u001b[0m\n",
      "\u001b[1mEpoch 30/500 | train_loss=0.2211 acc=0.9939 | val_loss=0.2496 acc=0.9791 F1=0.9802\u001b[0m\n",
      "\u001b[1mEpoch 31/500 | train_loss=0.2167 acc=0.9957 | val_loss=0.2473 acc=0.9896 F1=0.9913\u001b[0m\n",
      "\u001b[1mEpoch 32/500 | train_loss=0.2133 acc=0.9965 | val_loss=0.2469 acc=0.9809 F1=0.9808\u001b[0m\n",
      "\u001b[1mEpoch 33/500 | train_loss=0.2071 acc=0.9987 | val_loss=0.2440 acc=0.9809 F1=0.9776\u001b[0m\n",
      "\u001b[1mEpoch 34/500 | train_loss=0.2041 acc=1.0000 | val_loss=0.2291 acc=0.9896 F1=0.9883\u001b[0m\n",
      "\u001b[1mEpoch 35/500 | train_loss=0.2029 acc=1.0000 | val_loss=0.2221 acc=0.9913 F1=0.9898\u001b[0m\n",
      "\u001b[1mEpoch 36/500 | train_loss=0.2109 acc=0.9970 | val_loss=0.2396 acc=0.9774 F1=0.9786\u001b[0m\n",
      "\u001b[1mEpoch 37/500 | train_loss=0.2054 acc=0.9996 | val_loss=0.2595 acc=0.9704 F1=0.9725\u001b[0m\n",
      "\u001b[1mEpoch 38/500 | train_loss=0.2040 acc=1.0000 | val_loss=0.2320 acc=0.9843 F1=0.9848\u001b[0m\n",
      "\u001b[1mEpoch 39/500 | train_loss=0.2040 acc=0.9996 | val_loss=0.2932 acc=0.9617 F1=0.9647\u001b[0m\n",
      "\u001b[1mEpoch 40/500 | train_loss=0.2064 acc=0.9991 | val_loss=0.2277 acc=0.9843 F1=0.9847\u001b[0m\n",
      "\u001b[1mEpoch 41/500 | train_loss=0.2036 acc=1.0000 | val_loss=0.2531 acc=0.9774 F1=0.9786\u001b[0m\n",
      "\u001b[96m\u001b[1mEpoch 42/500 | train_loss=0.2046 acc=0.9996 | val_loss=0.2082 acc=0.9983 F1=0.9976 (✅ best loss & 💙 best acc & 🟣 best F1)\u001b[0m\n",
      "\u001b[1mEpoch 43/500 | train_loss=0.2042 acc=0.9996 | val_loss=0.2396 acc=0.9861 F1=0.9863\u001b[0m\n",
      "\u001b[1mEpoch 44/500 | train_loss=0.2138 acc=0.9961 | val_loss=0.2646 acc=0.9739 F1=0.9731\u001b[0m\n",
      "\u001b[1mEpoch 45/500 | train_loss=0.2129 acc=0.9974 | val_loss=0.2426 acc=0.9809 F1=0.9815\u001b[0m\n",
      "\u001b[1mEpoch 46/500 | train_loss=0.2055 acc=0.9987 | val_loss=0.2470 acc=0.9774 F1=0.9732\u001b[0m\n",
      "\u001b[1mEpoch 47/500 | train_loss=0.2141 acc=0.9957 | val_loss=0.2500 acc=0.9757 F1=0.9700\u001b[0m\n",
      "\u001b[1mEpoch 48/500 | train_loss=0.2084 acc=0.9974 | val_loss=0.2614 acc=0.9791 F1=0.9804\u001b[0m\n",
      "\u001b[1mEpoch 49/500 | train_loss=0.2089 acc=0.9978 | val_loss=0.2447 acc=0.9826 F1=0.9841\u001b[0m\n",
      "\u001b[1mEpoch 50/500 | train_loss=0.2104 acc=0.9974 | val_loss=0.2807 acc=0.9652 F1=0.9641\u001b[0m\n",
      "\u001b[1mEpoch 51/500 | train_loss=0.2118 acc=0.9965 | val_loss=0.2328 acc=0.9878 F1=0.9886\u001b[0m\n",
      "\u001b[1mEpoch 52/500 | train_loss=0.2035 acc=1.0000 | val_loss=0.2232 acc=0.9896 F1=0.9873\u001b[0m\n",
      "\u001b[1mEpoch 53/500 | train_loss=0.2028 acc=1.0000 | val_loss=0.2193 acc=0.9896 F1=0.9873\u001b[0m\n",
      "\u001b[1mEpoch 54/500 | train_loss=0.2050 acc=0.9991 | val_loss=0.3118 acc=0.9513 F1=0.9497\u001b[0m\n",
      "\u001b[1mEpoch 55/500 | train_loss=0.2065 acc=0.9996 | val_loss=0.2435 acc=0.9826 F1=0.9832\u001b[0m\n",
      "\u001b[1mEpoch 56/500 | train_loss=0.2113 acc=0.9965 | val_loss=0.2526 acc=0.9809 F1=0.9807\u001b[0m\n",
      "\u001b[1mEpoch 57/500 | train_loss=0.2048 acc=0.9996 | val_loss=0.2420 acc=0.9843 F1=0.9811\u001b[0m\n",
      "\u001b[1mEpoch 58/500 | train_loss=0.2047 acc=0.9991 | val_loss=0.2430 acc=0.9861 F1=0.9863\u001b[0m\n",
      "\u001b[1mEpoch 59/500 | train_loss=0.2025 acc=1.0000 | val_loss=0.2288 acc=0.9878 F1=0.9895\u001b[0m\n",
      "\u001b[1mEpoch 60/500 | train_loss=0.2021 acc=1.0000 | val_loss=0.2232 acc=0.9878 F1=0.9896\u001b[0m\n",
      "\u001b[1mEpoch 61/500 | train_loss=0.2021 acc=1.0000 | val_loss=0.2254 acc=0.9878 F1=0.9896\u001b[0m\n",
      "\u001b[1mEpoch 62/500 | train_loss=0.2025 acc=1.0000 | val_loss=0.2864 acc=0.9635 F1=0.9648\u001b[0m\n",
      "\u001b[1mEpoch 63/500 | train_loss=0.2093 acc=0.9970 | val_loss=0.2722 acc=0.9757 F1=0.9743\u001b[0m\n",
      "\u001b[1mEpoch 64/500 | train_loss=0.2043 acc=0.9996 | val_loss=0.2385 acc=0.9861 F1=0.9863\u001b[0m\n",
      "\u001b[1mEpoch 65/500 | train_loss=0.2064 acc=0.9983 | val_loss=0.2371 acc=0.9861 F1=0.9862\u001b[0m\n",
      "\u001b[1mEpoch 66/500 | train_loss=0.2024 acc=1.0000 | val_loss=0.2179 acc=0.9965 F1=0.9970\u001b[0m\n",
      "\u001b[1mEpoch 67/500 | train_loss=0.2026 acc=0.9996 | val_loss=0.2230 acc=0.9913 F1=0.9917\u001b[0m\n",
      "\u001b[1mEpoch 68/500 | train_loss=0.2081 acc=0.9974 | val_loss=0.2437 acc=0.9843 F1=0.9847\u001b[0m\n",
      "\u001b[1mEpoch 69/500 | train_loss=0.2163 acc=0.9957 | val_loss=0.2636 acc=0.9722 F1=0.9733\u001b[0m\n",
      "\u001b[1mEpoch 70/500 | train_loss=0.2084 acc=0.9983 | val_loss=0.2351 acc=0.9861 F1=0.9873\u001b[0m\n",
      "\u001b[1mEpoch 71/500 | train_loss=0.2084 acc=0.9970 | val_loss=0.2270 acc=0.9878 F1=0.9896\u001b[0m\n",
      "\u001b[1mEpoch 72/500 | train_loss=0.2060 acc=0.9983 | val_loss=0.2346 acc=0.9843 F1=0.9839\u001b[0m\n",
      "\u001b[1mEpoch 73/500 | train_loss=0.2121 acc=0.9961 | val_loss=0.2340 acc=0.9861 F1=0.9844\u001b[0m\n",
      "\u001b[1mEpoch 74/500 | train_loss=0.2031 acc=1.0000 | val_loss=0.2116 acc=0.9965 F1=0.9971\u001b[0m\n",
      "\u001b[1mEpoch 75/500 | train_loss=0.2025 acc=1.0000 | val_loss=0.2201 acc=0.9913 F1=0.9927\u001b[0m\n",
      "\u001b[1mEpoch 76/500 | train_loss=0.2024 acc=1.0000 | val_loss=0.2219 acc=0.9913 F1=0.9924\u001b[0m\n",
      "\u001b[1mEpoch 77/500 | train_loss=0.2020 acc=1.0000 | val_loss=0.2116 acc=0.9965 F1=0.9970\u001b[0m\n",
      "\u001b[1mEpoch 78/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2112 acc=0.9965 F1=0.9970\u001b[0m\n",
      "\u001b[1mEpoch 79/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2100 acc=0.9965 F1=0.9970\u001b[0m\n",
      "\u001b[1mEpoch 80/500 | train_loss=0.2029 acc=0.9996 | val_loss=0.2114 acc=0.9948 F1=0.9956\u001b[0m\n",
      "\u001b[1mEpoch 81/500 | train_loss=0.2020 acc=1.0000 | val_loss=0.2174 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 82/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2147 acc=0.9948 F1=0.9956\u001b[0m\n",
      "\u001b[1mEpoch 83/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2141 acc=0.9948 F1=0.9956\u001b[0m\n",
      "\u001b[1mEpoch 84/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2109 acc=0.9965 F1=0.9971\u001b[0m\n",
      "\u001b[1mEpoch 85/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2118 acc=0.9948 F1=0.9956\u001b[0m\n",
      "\u001b[95m\u001b[1mEpoch 86/500 | train_loss=0.2024 acc=1.0000 | val_loss=0.2104 acc=0.9983 F1=0.9985 (🟣 best F1)\u001b[0m\n",
      "\u001b[1mEpoch 87/500 | train_loss=0.2024 acc=1.0000 | val_loss=0.2185 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 88/500 | train_loss=0.2022 acc=1.0000 | val_loss=0.2225 acc=0.9913 F1=0.9898\u001b[0m\n",
      "\u001b[1mEpoch 89/500 | train_loss=0.2060 acc=0.9983 | val_loss=0.2395 acc=0.9843 F1=0.9846\u001b[0m\n",
      "\u001b[1mEpoch 90/500 | train_loss=0.2058 acc=0.9991 | val_loss=0.2412 acc=0.9843 F1=0.9861\u001b[0m\n",
      "\u001b[1mEpoch 91/500 | train_loss=0.2125 acc=0.9948 | val_loss=0.3506 acc=0.9461 F1=0.9503\u001b[0m\n",
      "\u001b[1mEpoch 92/500 | train_loss=0.2301 acc=0.9913 | val_loss=0.2848 acc=0.9635 F1=0.9649\u001b[0m\n",
      "\u001b[1mEpoch 93/500 | train_loss=0.2118 acc=0.9974 | val_loss=0.2241 acc=0.9896 F1=0.9892\u001b[0m\n",
      "\u001b[1mEpoch 94/500 | train_loss=0.2034 acc=1.0000 | val_loss=0.2195 acc=0.9913 F1=0.9926\u001b[0m\n",
      "\u001b[1mEpoch 95/500 | train_loss=0.2042 acc=0.9996 | val_loss=0.2356 acc=0.9861 F1=0.9871\u001b[0m\n",
      "\u001b[1mEpoch 96/500 | train_loss=0.2033 acc=1.0000 | val_loss=0.2289 acc=0.9913 F1=0.9915\u001b[0m\n",
      "\u001b[1mEpoch 97/500 | train_loss=0.2021 acc=1.0000 | val_loss=0.2250 acc=0.9896 F1=0.9900\u001b[0m\n",
      "\u001b[1mEpoch 98/500 | train_loss=0.2033 acc=1.0000 | val_loss=0.2185 acc=0.9896 F1=0.9891\u001b[0m\n",
      "\u001b[1mEpoch 99/500 | train_loss=0.2021 acc=1.0000 | val_loss=0.2215 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 100/500 | train_loss=0.2053 acc=0.9983 | val_loss=0.2331 acc=0.9878 F1=0.9894\u001b[0m\n",
      "\u001b[1mEpoch 101/500 | train_loss=0.2041 acc=0.9996 | val_loss=0.2173 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 102/500 | train_loss=0.2076 acc=0.9983 | val_loss=0.2603 acc=0.9739 F1=0.9765\u001b[0m\n",
      "\u001b[1mEpoch 103/500 | train_loss=0.2028 acc=1.0000 | val_loss=0.2551 acc=0.9757 F1=0.9771\u001b[0m\n",
      "\u001b[1mEpoch 104/500 | train_loss=0.2026 acc=0.9996 | val_loss=0.2388 acc=0.9826 F1=0.9840\u001b[0m\n",
      "\u001b[1mEpoch 105/500 | train_loss=0.2022 acc=1.0000 | val_loss=0.2235 acc=0.9930 F1=0.9941\u001b[0m\n",
      "\u001b[1mEpoch 106/500 | train_loss=0.2019 acc=1.0000 | val_loss=0.2185 acc=0.9930 F1=0.9942\u001b[0m\n",
      "\u001b[1mEpoch 107/500 | train_loss=0.2021 acc=1.0000 | val_loss=0.2253 acc=0.9878 F1=0.9878\u001b[0m\n",
      "\u001b[1mEpoch 108/500 | train_loss=0.2023 acc=0.9996 | val_loss=0.2323 acc=0.9843 F1=0.9867\u001b[0m\n",
      "\u001b[1mEpoch 109/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2161 acc=0.9948 F1=0.9956\u001b[0m\n",
      "\u001b[1mEpoch 110/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2137 acc=0.9948 F1=0.9956\u001b[0m\n",
      "\u001b[1mEpoch 111/500 | train_loss=0.2022 acc=1.0000 | val_loss=0.2428 acc=0.9826 F1=0.9831\u001b[0m\n",
      "\u001b[1mEpoch 112/500 | train_loss=0.2050 acc=0.9991 | val_loss=0.2254 acc=0.9896 F1=0.9902\u001b[0m\n",
      "\u001b[1mEpoch 113/500 | train_loss=0.2021 acc=1.0000 | val_loss=0.2218 acc=0.9878 F1=0.9877\u001b[0m\n",
      "\u001b[1mEpoch 114/500 | train_loss=0.2091 acc=0.9970 | val_loss=0.4250 acc=0.9200 F1=0.9283\u001b[0m\n",
      "\u001b[1mEpoch 115/500 | train_loss=0.2187 acc=0.9952 | val_loss=0.2907 acc=0.9722 F1=0.9654\u001b[0m\n",
      "\u001b[1mEpoch 116/500 | train_loss=0.2112 acc=0.9978 | val_loss=0.2346 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 117/500 | train_loss=0.2164 acc=0.9957 | val_loss=0.2453 acc=0.9826 F1=0.9795\u001b[0m\n",
      "\u001b[1mEpoch 118/500 | train_loss=0.2058 acc=0.9983 | val_loss=0.2312 acc=0.9878 F1=0.9860\u001b[0m\n",
      "\u001b[1mEpoch 119/500 | train_loss=0.2037 acc=1.0000 | val_loss=0.2561 acc=0.9809 F1=0.9775\u001b[0m\n",
      "\u001b[1mEpoch 120/500 | train_loss=0.2023 acc=1.0000 | val_loss=0.2359 acc=0.9861 F1=0.9845\u001b[0m\n",
      "\u001b[1mEpoch 121/500 | train_loss=0.2031 acc=0.9991 | val_loss=0.2413 acc=0.9843 F1=0.9847\u001b[0m\n",
      "\u001b[1mEpoch 122/500 | train_loss=0.2041 acc=0.9996 | val_loss=0.2226 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 123/500 | train_loss=0.2039 acc=0.9996 | val_loss=0.2333 acc=0.9843 F1=0.9857\u001b[0m\n",
      "\u001b[1mEpoch 124/500 | train_loss=0.2088 acc=0.9974 | val_loss=0.2329 acc=0.9861 F1=0.9863\u001b[0m\n",
      "\u001b[1mEpoch 125/500 | train_loss=0.2047 acc=0.9991 | val_loss=0.2247 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 126/500 | train_loss=0.2034 acc=0.9991 | val_loss=0.2358 acc=0.9843 F1=0.9865\u001b[0m\n",
      "\u001b[1mEpoch 127/500 | train_loss=0.2028 acc=1.0000 | val_loss=0.2293 acc=0.9913 F1=0.9925\u001b[0m\n",
      "\u001b[1mEpoch 128/500 | train_loss=0.2019 acc=1.0000 | val_loss=0.2227 acc=0.9896 F1=0.9911\u001b[0m\n",
      "\u001b[1mEpoch 129/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2197 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 130/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2210 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 131/500 | train_loss=0.2019 acc=1.0000 | val_loss=0.2332 acc=0.9861 F1=0.9861\u001b[0m\n",
      "\u001b[1mEpoch 132/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2230 acc=0.9896 F1=0.9891\u001b[0m\n",
      "\u001b[1mEpoch 133/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2211 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 134/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2182 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 135/500 | train_loss=0.2028 acc=0.9996 | val_loss=0.2282 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 136/500 | train_loss=0.2019 acc=1.0000 | val_loss=0.2244 acc=0.9878 F1=0.9886\u001b[0m\n",
      "\u001b[1mEpoch 137/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2315 acc=0.9861 F1=0.9871\u001b[0m\n",
      "\u001b[1mEpoch 138/500 | train_loss=0.2071 acc=0.9983 | val_loss=0.2571 acc=0.9774 F1=0.9795\u001b[0m\n",
      "\u001b[1mEpoch 139/500 | train_loss=0.2043 acc=0.9996 | val_loss=0.2748 acc=0.9687 F1=0.9673\u001b[0m\n",
      "\u001b[1mEpoch 140/500 | train_loss=0.2027 acc=0.9996 | val_loss=0.2375 acc=0.9843 F1=0.9856\u001b[0m\n",
      "\u001b[1mEpoch 141/500 | train_loss=0.2056 acc=0.9987 | val_loss=0.2506 acc=0.9809 F1=0.9837\u001b[0m\n",
      "\u001b[1mEpoch 142/500 | train_loss=0.2022 acc=1.0000 | val_loss=0.2306 acc=0.9913 F1=0.9916\u001b[0m\n",
      "\u001b[1mEpoch 143/500 | train_loss=0.2062 acc=0.9987 | val_loss=0.2513 acc=0.9826 F1=0.9853\u001b[0m\n",
      "\u001b[1mEpoch 144/500 | train_loss=0.2041 acc=0.9991 | val_loss=0.2490 acc=0.9809 F1=0.9816\u001b[0m\n",
      "\u001b[1mEpoch 145/500 | train_loss=0.2038 acc=0.9996 | val_loss=0.2607 acc=0.9791 F1=0.9801\u001b[0m\n",
      "\u001b[1mEpoch 146/500 | train_loss=0.2028 acc=0.9996 | val_loss=0.2335 acc=0.9861 F1=0.9871\u001b[0m\n",
      "\u001b[1mEpoch 147/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2350 acc=0.9878 F1=0.9885\u001b[0m\n",
      "\u001b[1mEpoch 148/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2294 acc=0.9896 F1=0.9900\u001b[0m\n",
      "\u001b[1mEpoch 149/500 | train_loss=0.2033 acc=0.9996 | val_loss=0.2523 acc=0.9809 F1=0.9827\u001b[0m\n",
      "\u001b[1mEpoch 150/500 | train_loss=0.2042 acc=0.9987 | val_loss=0.2426 acc=0.9809 F1=0.9841\u001b[0m\n",
      "\u001b[1mEpoch 151/500 | train_loss=0.2077 acc=0.9978 | val_loss=0.2649 acc=0.9739 F1=0.9703\u001b[0m\n",
      "\u001b[1mEpoch 152/500 | train_loss=0.2036 acc=0.9996 | val_loss=0.2321 acc=0.9878 F1=0.9878\u001b[0m\n",
      "\u001b[1mEpoch 153/500 | train_loss=0.2076 acc=0.9974 | val_loss=0.2659 acc=0.9757 F1=0.9744\u001b[0m\n",
      "\u001b[1mEpoch 154/500 | train_loss=0.2079 acc=0.9970 | val_loss=0.3717 acc=0.9322 F1=0.9414\u001b[0m\n",
      "\u001b[1mEpoch 155/500 | train_loss=0.2031 acc=0.9996 | val_loss=0.2366 acc=0.9861 F1=0.9872\u001b[0m\n",
      "\u001b[1mEpoch 156/500 | train_loss=0.2023 acc=1.0000 | val_loss=0.2291 acc=0.9843 F1=0.9848\u001b[0m\n",
      "\u001b[1mEpoch 157/500 | train_loss=0.2020 acc=1.0000 | val_loss=0.2300 acc=0.9843 F1=0.9839\u001b[0m\n",
      "\u001b[1mEpoch 158/500 | train_loss=0.2065 acc=0.9987 | val_loss=0.2322 acc=0.9843 F1=0.9858\u001b[0m\n",
      "\u001b[1mEpoch 159/500 | train_loss=0.2021 acc=1.0000 | val_loss=0.2238 acc=0.9878 F1=0.9878\u001b[0m\n",
      "\u001b[1mEpoch 160/500 | train_loss=0.2026 acc=0.9996 | val_loss=0.2340 acc=0.9843 F1=0.9847\u001b[0m\n",
      "\u001b[1mEpoch 161/500 | train_loss=0.2019 acc=1.0000 | val_loss=0.2165 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 162/500 | train_loss=0.2043 acc=0.9983 | val_loss=0.4430 acc=0.9200 F1=0.9298\u001b[0m\n",
      "\u001b[1mEpoch 163/500 | train_loss=0.2165 acc=0.9952 | val_loss=0.2430 acc=0.9843 F1=0.9847\u001b[0m\n",
      "\u001b[1mEpoch 164/500 | train_loss=0.2024 acc=1.0000 | val_loss=0.2479 acc=0.9826 F1=0.9822\u001b[0m\n",
      "\u001b[1mEpoch 165/500 | train_loss=0.2020 acc=1.0000 | val_loss=0.2338 acc=0.9878 F1=0.9877\u001b[0m\n",
      "\u001b[1mEpoch 166/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2309 acc=0.9878 F1=0.9877\u001b[0m\n",
      "\u001b[1mEpoch 167/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2225 acc=0.9896 F1=0.9883\u001b[0m\n",
      "\u001b[1mEpoch 168/500 | train_loss=0.2028 acc=0.9996 | val_loss=0.2353 acc=0.9878 F1=0.9859\u001b[0m\n",
      "\u001b[1mEpoch 169/500 | train_loss=0.2041 acc=0.9991 | val_loss=0.2388 acc=0.9826 F1=0.9850\u001b[0m\n",
      "\u001b[1mEpoch 170/500 | train_loss=0.2024 acc=1.0000 | val_loss=0.2270 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 171/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2283 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 172/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2167 acc=0.9948 F1=0.9946\u001b[0m\n",
      "\u001b[1mEpoch 173/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2142 acc=0.9948 F1=0.9946\u001b[0m\n",
      "\u001b[1mEpoch 174/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2169 acc=0.9948 F1=0.9945\u001b[0m\n",
      "\u001b[1mEpoch 175/500 | train_loss=0.2039 acc=0.9991 | val_loss=0.2318 acc=0.9861 F1=0.9870\u001b[0m\n",
      "\u001b[1mEpoch 176/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2362 acc=0.9896 F1=0.9900\u001b[0m\n",
      "\u001b[1mEpoch 177/500 | train_loss=0.2065 acc=0.9987 | val_loss=0.2278 acc=0.9861 F1=0.9862\u001b[0m\n",
      "\u001b[1mEpoch 178/500 | train_loss=0.2075 acc=0.9974 | val_loss=0.2606 acc=0.9739 F1=0.9721\u001b[0m\n",
      "\u001b[1mEpoch 179/500 | train_loss=0.2035 acc=1.0000 | val_loss=0.2187 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 180/500 | train_loss=0.2023 acc=1.0000 | val_loss=0.2168 acc=0.9930 F1=0.9930\u001b[0m\n",
      "\u001b[1mEpoch 181/500 | train_loss=0.2058 acc=0.9978 | val_loss=0.2453 acc=0.9826 F1=0.9843\u001b[0m\n",
      "\u001b[1mEpoch 182/500 | train_loss=0.2057 acc=0.9983 | val_loss=0.2948 acc=0.9635 F1=0.9667\u001b[0m\n",
      "\u001b[1mEpoch 183/500 | train_loss=0.2061 acc=0.9983 | val_loss=0.2182 acc=0.9948 F1=0.9945\u001b[0m\n",
      "\u001b[1mEpoch 184/500 | train_loss=0.2033 acc=0.9996 | val_loss=0.2230 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 185/500 | train_loss=0.2028 acc=1.0000 | val_loss=0.2262 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 186/500 | train_loss=0.2020 acc=1.0000 | val_loss=0.2290 acc=0.9878 F1=0.9861\u001b[0m\n",
      "\u001b[1mEpoch 187/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2310 acc=0.9878 F1=0.9877\u001b[0m\n",
      "\u001b[1mEpoch 188/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2204 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 189/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2182 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 190/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2200 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 191/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2195 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 192/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2178 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 193/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2193 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 194/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2206 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 195/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2190 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 196/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2167 acc=0.9948 F1=0.9937\u001b[0m\n",
      "\u001b[1mEpoch 197/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2172 acc=0.9948 F1=0.9937\u001b[0m\n",
      "\u001b[1mEpoch 198/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2175 acc=0.9948 F1=0.9937\u001b[0m\n",
      "\u001b[1mEpoch 199/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2183 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 200/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2183 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 201/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2204 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 202/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2189 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 203/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2235 acc=0.9896 F1=0.9890\u001b[0m\n",
      "\u001b[1mEpoch 204/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2214 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 205/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2202 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 206/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2191 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 207/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2179 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 208/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2169 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 209/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2168 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 210/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2171 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 211/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2209 acc=0.9896 F1=0.9892\u001b[0m\n",
      "\u001b[1mEpoch 212/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2177 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 213/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2163 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 214/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2171 acc=0.9948 F1=0.9937\u001b[0m\n",
      "\u001b[1mEpoch 215/500 | train_loss=0.2124 acc=0.9961 | val_loss=0.3780 acc=0.9304 F1=0.9260\u001b[0m\n",
      "\u001b[1mEpoch 216/500 | train_loss=0.2101 acc=0.9961 | val_loss=0.2773 acc=0.9722 F1=0.9714\u001b[0m\n",
      "\u001b[1mEpoch 217/500 | train_loss=0.2046 acc=0.9996 | val_loss=0.2278 acc=0.9878 F1=0.9886\u001b[0m\n",
      "\u001b[1mEpoch 218/500 | train_loss=0.2020 acc=1.0000 | val_loss=0.2233 acc=0.9878 F1=0.9887\u001b[0m\n",
      "\u001b[1mEpoch 219/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2198 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 220/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2126 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 221/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2142 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 222/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2162 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 223/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2168 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 224/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2143 acc=0.9948 F1=0.9945\u001b[0m\n",
      "\u001b[1mEpoch 225/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2136 acc=0.9948 F1=0.9945\u001b[0m\n",
      "\u001b[1mEpoch 226/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2148 acc=0.9948 F1=0.9945\u001b[0m\n",
      "\u001b[1mEpoch 227/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2149 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 228/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2161 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 229/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2203 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 230/500 | train_loss=0.2033 acc=0.9991 | val_loss=0.2369 acc=0.9878 F1=0.9877\u001b[0m\n",
      "\u001b[1mEpoch 231/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2251 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 232/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2247 acc=0.9913 F1=0.9892\u001b[0m\n",
      "\u001b[1mEpoch 233/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2240 acc=0.9913 F1=0.9915\u001b[0m\n",
      "\u001b[1mEpoch 234/500 | train_loss=0.2025 acc=1.0000 | val_loss=0.2418 acc=0.9843 F1=0.9847\u001b[0m\n",
      "\u001b[1mEpoch 235/500 | train_loss=0.2051 acc=0.9983 | val_loss=0.2458 acc=0.9826 F1=0.9820\u001b[0m\n",
      "\u001b[1mEpoch 236/500 | train_loss=0.2144 acc=0.9952 | val_loss=0.2386 acc=0.9878 F1=0.9875\u001b[0m\n",
      "\u001b[1mEpoch 237/500 | train_loss=0.2026 acc=1.0000 | val_loss=0.2208 acc=0.9930 F1=0.9930\u001b[0m\n",
      "\u001b[1mEpoch 238/500 | train_loss=0.2036 acc=0.9996 | val_loss=0.2496 acc=0.9809 F1=0.9818\u001b[0m\n",
      "\u001b[1mEpoch 239/500 | train_loss=0.2032 acc=0.9996 | val_loss=0.2313 acc=0.9878 F1=0.9877\u001b[0m\n",
      "\u001b[1mEpoch 240/500 | train_loss=0.2019 acc=1.0000 | val_loss=0.2291 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 241/500 | train_loss=0.2020 acc=1.0000 | val_loss=0.2287 acc=0.9896 F1=0.9893\u001b[0m\n",
      "\u001b[1mEpoch 242/500 | train_loss=0.2026 acc=0.9996 | val_loss=0.2584 acc=0.9791 F1=0.9800\u001b[0m\n",
      "\u001b[1mEpoch 243/500 | train_loss=0.2020 acc=1.0000 | val_loss=0.2285 acc=0.9896 F1=0.9892\u001b[0m\n",
      "\u001b[1mEpoch 244/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2230 acc=0.9896 F1=0.9882\u001b[0m\n",
      "\u001b[1mEpoch 245/500 | train_loss=0.2020 acc=0.9996 | val_loss=0.2189 acc=0.9948 F1=0.9946\u001b[0m\n",
      "\u001b[1mEpoch 246/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2196 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 247/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2190 acc=0.9913 F1=0.9916\u001b[0m\n",
      "\u001b[1mEpoch 248/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2209 acc=0.9913 F1=0.9915\u001b[0m\n",
      "\u001b[1mEpoch 249/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2205 acc=0.9913 F1=0.9916\u001b[0m\n",
      "\u001b[1mEpoch 250/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2209 acc=0.9913 F1=0.9916\u001b[0m\n",
      "\u001b[1mEpoch 251/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2201 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 252/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2190 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 253/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2199 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 254/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2189 acc=0.9965 F1=0.9961\u001b[0m\n",
      "\u001b[1mEpoch 255/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2180 acc=0.9965 F1=0.9961\u001b[0m\n",
      "\u001b[1mEpoch 256/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2237 acc=0.9913 F1=0.9917\u001b[0m\n",
      "\u001b[1mEpoch 257/500 | train_loss=0.2085 acc=0.9970 | val_loss=0.2370 acc=0.9878 F1=0.9876\u001b[0m\n",
      "\u001b[1mEpoch 258/500 | train_loss=0.2025 acc=1.0000 | val_loss=0.2243 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 259/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2215 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 260/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2206 acc=0.9913 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 261/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2229 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 262/500 | train_loss=0.2021 acc=0.9996 | val_loss=0.2214 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 263/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2222 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 264/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2228 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 265/500 | train_loss=0.2031 acc=0.9987 | val_loss=0.2218 acc=0.9948 F1=0.9946\u001b[0m\n",
      "\u001b[1mEpoch 266/500 | train_loss=0.2019 acc=1.0000 | val_loss=0.2243 acc=0.9896 F1=0.9892\u001b[0m\n",
      "\u001b[1mEpoch 267/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2191 acc=0.9913 F1=0.9907\u001b[0m\n",
      "\u001b[1mEpoch 268/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2134 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 269/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2123 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 270/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2136 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 271/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2147 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 272/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2150 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 273/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2154 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 274/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2154 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 275/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2152 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 276/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2138 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 277/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2205 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 278/500 | train_loss=0.2017 acc=0.9996 | val_loss=0.2199 acc=0.9913 F1=0.9907\u001b[0m\n",
      "\u001b[1mEpoch 279/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2182 acc=0.9913 F1=0.9907\u001b[0m\n",
      "\u001b[1mEpoch 280/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2181 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 281/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2192 acc=0.9913 F1=0.9907\u001b[0m\n",
      "\u001b[1mEpoch 282/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2194 acc=0.9913 F1=0.9907\u001b[0m\n",
      "\u001b[1mEpoch 283/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2203 acc=0.9948 F1=0.9946\u001b[0m\n",
      "\u001b[1mEpoch 284/500 | train_loss=0.2030 acc=0.9996 | val_loss=0.2856 acc=0.9704 F1=0.9712\u001b[0m\n",
      "\u001b[1mEpoch 285/500 | train_loss=0.2042 acc=0.9991 | val_loss=0.2203 acc=0.9913 F1=0.9907\u001b[0m\n",
      "\u001b[1mEpoch 286/500 | train_loss=0.2019 acc=0.9996 | val_loss=0.2390 acc=0.9896 F1=0.9902\u001b[0m\n",
      "\u001b[1mEpoch 287/500 | train_loss=0.2029 acc=0.9987 | val_loss=0.2315 acc=0.9878 F1=0.9885\u001b[0m\n",
      "\u001b[1mEpoch 288/500 | train_loss=0.2022 acc=0.9996 | val_loss=0.2442 acc=0.9826 F1=0.9829\u001b[0m\n",
      "\u001b[1mEpoch 289/500 | train_loss=0.2019 acc=0.9996 | val_loss=0.2158 acc=0.9948 F1=0.9945\u001b[0m\n",
      "\u001b[1mEpoch 290/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2163 acc=0.9930 F1=0.9930\u001b[0m\n",
      "\u001b[1mEpoch 291/500 | train_loss=0.2021 acc=0.9996 | val_loss=0.2346 acc=0.9843 F1=0.9845\u001b[0m\n",
      "\u001b[1mEpoch 292/500 | train_loss=0.2019 acc=1.0000 | val_loss=0.2171 acc=0.9896 F1=0.9882\u001b[0m\n",
      "\u001b[1mEpoch 293/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2139 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 294/500 | train_loss=0.2018 acc=0.9996 | val_loss=0.2379 acc=0.9843 F1=0.9848\u001b[0m\n",
      "\u001b[1mEpoch 295/500 | train_loss=0.2017 acc=1.0000 | val_loss=0.2227 acc=0.9896 F1=0.9901\u001b[0m\n",
      "\u001b[1mEpoch 296/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2182 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 297/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2166 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 298/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2162 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 299/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2152 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 300/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2151 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 301/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2159 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 302/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2138 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 303/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2140 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 304/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2132 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 305/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2117 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 306/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2119 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 307/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2119 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 308/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2117 acc=0.9948 F1=0.9955\u001b[0m\n",
      "\u001b[1mEpoch 309/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2242 acc=0.9878 F1=0.9858\u001b[0m\n",
      "\u001b[1mEpoch 310/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2200 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 311/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2206 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 312/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2193 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 313/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2185 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 314/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2182 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 315/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2174 acc=0.9913 F1=0.9898\u001b[0m\n",
      "\u001b[1mEpoch 316/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2195 acc=0.9913 F1=0.9898\u001b[0m\n",
      "\u001b[1mEpoch 317/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2189 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 318/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2193 acc=0.9913 F1=0.9898\u001b[0m\n",
      "\u001b[1mEpoch 319/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2189 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 320/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2185 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 321/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2186 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 322/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2191 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 323/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2189 acc=0.9913 F1=0.9906\u001b[0m\n",
      "\u001b[1mEpoch 324/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2175 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 325/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2178 acc=0.9930 F1=0.9931\u001b[0m\n",
      "\u001b[1mEpoch 326/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2171 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 327/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2172 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 328/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2186 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 329/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2211 acc=0.9913 F1=0.9898\u001b[0m\n",
      "\u001b[1mEpoch 330/500 | train_loss=0.2034 acc=0.9987 | val_loss=0.2134 acc=0.9930 F1=0.9922\u001b[0m\n",
      "\u001b[1mEpoch 331/500 | train_loss=0.2029 acc=0.9996 | val_loss=0.2135 acc=0.9965 F1=0.9961\u001b[0m\n",
      "\u001b[1mEpoch 332/500 | train_loss=0.2023 acc=0.9996 | val_loss=0.2627 acc=0.9739 F1=0.9742\u001b[0m\n",
      "\u001b[1mEpoch 333/500 | train_loss=0.2039 acc=0.9991 | val_loss=0.2383 acc=0.9861 F1=0.9850\u001b[0m\n",
      "\u001b[1mEpoch 334/500 | train_loss=0.2033 acc=0.9996 | val_loss=0.2163 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 335/500 | train_loss=0.2016 acc=1.0000 | val_loss=0.2155 acc=0.9913 F1=0.9882\u001b[0m\n",
      "\u001b[1mEpoch 336/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2159 acc=0.9896 F1=0.9882\u001b[0m\n",
      "\u001b[1mEpoch 337/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2153 acc=0.9913 F1=0.9896\u001b[0m\n",
      "\u001b[1mEpoch 338/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2149 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 339/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2139 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 340/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2137 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 341/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2133 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 342/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2139 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 343/500 | train_loss=0.2024 acc=0.9996 | val_loss=0.2151 acc=0.9913 F1=0.9888\u001b[0m\n",
      "\u001b[1mEpoch 344/500 | train_loss=0.2018 acc=1.0000 | val_loss=0.2150 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 345/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2151 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 346/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2109 acc=0.9948 F1=0.9927\u001b[0m\n",
      "\u001b[1mEpoch 347/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2113 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 348/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2116 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 349/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2115 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 350/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2115 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 351/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2137 acc=0.9913 F1=0.9905\u001b[0m\n",
      "\u001b[1mEpoch 352/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2133 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 353/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2131 acc=0.9930 F1=0.9921\u001b[0m\n",
      "\u001b[1mEpoch 354/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2134 acc=0.9930 F1=0.9905\u001b[0m\n",
      "\u001b[1mEpoch 355/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2155 acc=0.9896 F1=0.9890\u001b[0m\n",
      "\u001b[1mEpoch 356/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2159 acc=0.9896 F1=0.9890\u001b[0m\n",
      "\u001b[1mEpoch 357/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2151 acc=0.9913 F1=0.9905\u001b[0m\n",
      "\u001b[1mEpoch 358/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2183 acc=0.9878 F1=0.9867\u001b[0m\n",
      "\u001b[1mEpoch 359/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2192 acc=0.9878 F1=0.9867\u001b[0m\n",
      "\u001b[1mEpoch 360/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2188 acc=0.9878 F1=0.9867\u001b[0m\n",
      "\u001b[1mEpoch 361/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2180 acc=0.9896 F1=0.9873\u001b[0m\n",
      "\u001b[1mEpoch 362/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2188 acc=0.9878 F1=0.9858\u001b[0m\n",
      "\u001b[1mEpoch 363/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2181 acc=0.9878 F1=0.9858\u001b[0m\n",
      "\u001b[1mEpoch 364/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2181 acc=0.9878 F1=0.9858\u001b[0m\n",
      "\u001b[1mEpoch 365/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2175 acc=0.9878 F1=0.9858\u001b[0m\n",
      "\u001b[1mEpoch 366/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2189 acc=0.9878 F1=0.9867\u001b[0m\n",
      "\u001b[1mEpoch 367/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2164 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 368/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2168 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 369/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2172 acc=0.9913 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 370/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2166 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 371/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2167 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 372/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2170 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 373/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2162 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 374/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2166 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 375/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2161 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 376/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2166 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 377/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2172 acc=0.9913 F1=0.9889\u001b[0m\n",
      "\u001b[1mEpoch 378/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2142 acc=0.9930 F1=0.9904\u001b[0m\n",
      "\u001b[1mEpoch 379/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2135 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 380/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2134 acc=0.9930 F1=0.9904\u001b[0m\n",
      "\u001b[1mEpoch 381/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2139 acc=0.9930 F1=0.9904\u001b[0m\n",
      "\u001b[1mEpoch 382/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2134 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 383/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2141 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 384/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2134 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 385/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2136 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 386/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2136 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 387/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2136 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 388/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2127 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 389/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2128 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 390/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2127 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 391/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2129 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 392/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2136 acc=0.9913 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 393/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2130 acc=0.9913 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 394/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2126 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 395/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2135 acc=0.9930 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 396/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2130 acc=0.9913 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 397/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2136 acc=0.9913 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 398/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2137 acc=0.9913 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 399/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2132 acc=0.9930 F1=0.9897\u001b[0m\n",
      "\u001b[1mEpoch 400/500 | train_loss=0.2015 acc=1.0000 | val_loss=0.2127 acc=0.9948 F1=0.9927\u001b[0m\n",
      "\u001b[1mEpoch 401/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2085 acc=0.9948 F1=0.9927\u001b[0m\n",
      "\u001b[1mEpoch 402/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2082 acc=0.9948 F1=0.9927\u001b[0m\n",
      "\u001b[1mEpoch 403/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2084 acc=0.9965 F1=0.9951\u001b[0m\n",
      "\u001b[92m\u001b[1mEpoch 404/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2080 acc=0.9983 F1=0.9976 (✅ best loss)\u001b[0m\n",
      "\u001b[92m\u001b[1mEpoch 405/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2077 acc=0.9983 F1=0.9976 (✅ best loss)\u001b[0m\n",
      "\u001b[92m\u001b[1mEpoch 406/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2075 acc=0.9983 F1=0.9976 (✅ best loss)\u001b[0m\n",
      "\u001b[1mEpoch 407/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2077 acc=0.9983 F1=0.9976\u001b[0m\n",
      "\u001b[92m\u001b[1mEpoch 408/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2074 acc=0.9983 F1=0.9976 (✅ best loss)\u001b[0m\n",
      "\u001b[1mEpoch 409/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2077 acc=0.9983 F1=0.9976\u001b[0m\n",
      "\u001b[1mEpoch 410/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2076 acc=0.9983 F1=0.9976\u001b[0m\n",
      "\u001b[1mEpoch 411/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2075 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 412/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2081 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 413/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2080 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 414/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2086 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 415/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2086 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 416/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2084 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 417/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2084 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 418/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2079 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 419/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2084 acc=0.9965 F1=0.9976\u001b[0m\n",
      "\u001b[1mEpoch 420/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2088 acc=0.9948 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 421/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2086 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 422/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2089 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 423/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2091 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 424/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2087 acc=0.9965 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 425/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2088 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 426/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2084 acc=0.9965 F1=0.9952\u001b[0m\n",
      "\u001b[1mEpoch 427/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2116 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 428/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2116 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 429/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2120 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 430/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2117 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 431/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2119 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 432/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2123 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 433/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2125 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 434/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2112 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 435/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2103 acc=0.9948 F1=0.9928\u001b[0m\n",
      "\u001b[1mEpoch 436/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2097 acc=0.9983 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 437/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2110 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 438/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2109 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 439/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2109 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 440/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2109 acc=0.9930 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 441/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2106 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 442/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2110 acc=0.9930 F1=0.9912\u001b[0m\n",
      "\u001b[1mEpoch 443/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2110 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 444/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2101 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 445/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2106 acc=0.9948 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 446/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2104 acc=0.9965 F1=0.9936\u001b[0m\n",
      "\u001b[1mEpoch 447/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2094 acc=0.9983 F1=0.9976\u001b[0m\n",
      "\u001b[1mEpoch 448/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2098 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 449/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2095 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 450/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2097 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 451/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2102 acc=0.9948 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 452/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2100 acc=0.9948 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 453/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2099 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 454/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2092 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 455/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2098 acc=0.9948 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 456/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2101 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 457/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2105 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 458/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2102 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 459/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2103 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 460/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2100 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 461/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2100 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 462/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2102 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 463/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2105 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 464/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2101 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 465/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2102 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 466/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2101 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 467/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2099 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 468/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2102 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 469/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2102 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 470/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2095 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 471/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2091 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 472/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2096 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 473/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2097 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 474/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2091 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 475/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2096 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 476/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2093 acc=0.9983 F1=0.9976\u001b[0m\n",
      "\u001b[1mEpoch 477/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2088 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 478/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2093 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 479/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2092 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 480/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2090 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 481/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2093 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 482/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2093 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 483/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2094 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 484/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2096 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 485/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2093 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 486/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2100 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 487/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2097 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 488/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2100 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 489/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2100 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 490/500 | train_loss=0.2014 acc=1.0000 | val_loss=0.2098 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 491/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2093 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 492/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2094 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 493/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2096 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 494/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2098 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 495/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2105 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 496/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2103 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 497/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2102 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 498/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2101 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 499/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2099 acc=0.9965 F1=0.9960\u001b[0m\n",
      "\u001b[1mEpoch 500/500 | train_loss=0.2013 acc=1.0000 | val_loss=0.2094 acc=0.9965 F1=0.9960\u001b[0m\n",
      "Best val loss: 0.2074. Saved to: best_MobileNetV3_Loss_brain_tumor.pt\n",
      "Best val acc : 0.9983. Saved to: best_MobileNetV3_Acc_brain_tumor.pt\n",
      "Best val F1  : 0.9985. Saved to: best_MobileNetV3_F1_brain_tumor.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAByyUlEQVR4nO3dd3gU1foH8O9syaYXAmlAIEjvTSAgAiJBmgW9cgUFFFREQS7qtV0FsYAV9Kdgo1gQEEFEDUgsdJEapAkoJZSE0NKTzWb3/P6YbMtu+uxOEr6f58mT7Ozs7Nmzm8yb97znjCSEECAiIiKqIzRqN4CIiIhISQxuiIiIqE5hcENERER1CoMbIiIiqlMY3BAREVGdwuCGiIiI6hQGN0RERFSnMLghIiKiOoXBDREREdUpDG6IyiBJUoW+Nm7cWK3nmTlzJiRJqtJjN27cqEgbapo77rgDfn5+yMjIKHWfMWPGQK/X48KFCxU+riRJmDlzpu12Zfpv/PjxaNq0aYWfy9H8+fOxZMkSl+2nTp2CJElu7/M06+fu0qVLXn9uIk/Sqd0Aoprs999/d7r98ssv47fffsOvv/7qtL1t27bVep6JEyfilltuqdJju3btit9//73abahpJkyYgDVr1uCrr77C5MmTXe7PzMzEt99+i+HDhyMyMrLKz+Ot/ps/fz7q16+P8ePHO22Pjo7G77//juuuu86jz090LWFwQ1SGXr16Od1u0KABNBqNy/aS8vLy4O/vX+HnadSoERo1alSlNgYHB5fbntpoyJAhiImJwaJFi9wGN8uWLUN+fj4mTJhQredRu/8MBkOdfP+I1MRhKaJq6t+/P9q3b4/Nmzejd+/e8Pf3xwMPPAAAWLFiBRISEhAdHQ0/Pz+0adMGzzzzDHJzc52O4W5YqmnTphg+fDjWr1+Prl27ws/PD61bt8aiRYuc9nM3rDJ+/HgEBgbi77//xtChQxEYGIjGjRvjiSeegNFodHr82bNncddddyEoKAihoaEYM2YMdu3aVe5Qyf79+yFJEhYuXOhy37p16yBJEtauXQsAuHjxIh566CE0btwYBoMBDRo0QJ8+ffDzzz+XenytVotx48Zhz549OHDggMv9ixcvRnR0NIYMGYKLFy9i8uTJaNu2LQIDAxEREYGbbroJW7ZsKfX4VqUNSy1ZsgStWrWCwWBAmzZt8Pnnn7t9/EsvvYSePXuiXr16CA4ORteuXbFw4UI4XpO4adOmOHToEDZt2mQbyrQOb5U2LLV161YMHDgQQUFB8Pf3R+/evfHjjz+6tFGSJPz222945JFHUL9+fYSHh2PkyJE4f/58ua+9otauXYv4+Hj4+/sjKCgIgwYNcslqVuQ93rdvH4YPH46IiAgYDAbExMRg2LBhOHv2rGJtJQKYuSFSRGpqKu69917897//xWuvvQaNRv6/4fjx4xg6dCimTZuGgIAA/PXXX3j99dexc+dOl6Etd/bv348nnngCzzzzDCIjI/Hpp59iwoQJaN68OW688cYyH2symXDrrbdiwoQJeOKJJ7B582a8/PLLCAkJwYsvvggAyM3NxYABA3DlyhW8/vrraN68OdavX49Ro0aV27ZOnTqhS5cuWLx4sUv2ZMmSJYiIiMDQoUMBAPfddx/27t2LV199FS1btkRGRgb27t2Ly5cvl/kcDzzwAObMmYNFixZh7ty5tu2HDx/Gzp078cwzz0Cr1eLKlSsAgBkzZiAqKgo5OTn49ttv0b9/f/zyyy/o379/ua+nZPvvv/9+3HbbbXj77beRmZmJmTNnwmg02t5bq1OnTuHhhx9GbGwsAGDHjh2YMmUKzp07Z+vnb7/9FnfddRdCQkIwf/58AHLGpjSbNm3CoEGD0LFjRyxcuBAGgwHz58/HiBEjsGzZMpf3Z+LEiRg2bBi++uornDlzBk899RTuvffeCn3GyvPVV19hzJgxSEhIwLJly2A0GvHGG2/Y+vaGG24AUP57nJubi0GDBiEuLg4ffPABIiMjkZaWht9++w3Z2dnVbieRE0FEFTZu3DgREBDgtK1fv34CgPjll1/KfKzFYhEmk0ls2rRJABD79++33TdjxgxR8texSZMmwtfXV5w+fdq2LT8/X9SrV088/PDDtm2//fabACB+++03p3YCEF9//bXTMYcOHSpatWplu/3BBx8IAGLdunVO+z388MMCgFi8eHGZr+m9994TAMTRo0dt265cuSIMBoN44oknbNsCAwPFtGnTyjxWafr16yfq168vCgsLbdueeOIJAUAcO3bM7WOKioqEyWQSAwcOFHfccYfTfQDEjBkzbLdL9p/ZbBYxMTGia9euwmKx2PY7deqU0Ov1okmTJqW21Ww2C5PJJGbNmiXCw8OdHt+uXTvRr18/l8ecPHnSpa979eolIiIiRHZ2ttNrat++vWjUqJHtuIsXLxYAxOTJk52O+cYbbwgAIjU1tdS2CmH/3F28eLHU1xMTEyM6dOggzGazbXt2draIiIgQvXv3tm0r7z3evXu3ACDWrFlTZpuIlMBhKSIFhIWF4aabbnLZfuLECYwePRpRUVHQarXQ6/Xo168fAODIkSPlHrdz5862jAAA+Pr6omXLljh9+nS5j5UkCSNGjHDa1rFjR6fHbtq0CUFBQS7FzPfcc0+5xwfk2UoGg8FpSMX63/39999v29ajRw8sWbIEr7zyCnbs2AGTyVSh4wNyYfGlS5dsQ1xFRUX48ssv0bdvX7Ro0cK234cffoiuXbvC19cXOp0Oer0ev/zyS4X62dHRo0dx/vx5jB492mmosEmTJujdu7fL/r/++ituvvlmhISE2N7jF198EZcvX0Z6enqlnhuQMxx//PEH7rrrLgQGBtq2a7Va3HfffTh79iyOHj3q9Jhbb73V6XbHjh0BoEKfk7JY++K+++5zylgFBgbizjvvxI4dO5CXlweg/Pe4efPmCAsLw9NPP40PP/wQhw8frlbbiMrC4IZIAdHR0S7bcnJy0LdvX/zxxx945ZVXsHHjRuzatQurV68GAOTn55d73PDwcJdtBoOhQo/19/eHr6+vy2MLCgpsty9fvux2plFFZx/Vq1cPt956Kz7//HOYzWYA8pBOjx490K5dO9t+K1aswLhx4/Dpp58iPj4e9erVw9ixY5GWllbuc1iHcxYvXgwASExMxIULF5yGwt555x088sgj6NmzJ1atWoUdO3Zg165duOWWWyrUV46swyhRUVEu95XctnPnTiQkJAAAPvnkE2zbtg27du3C888/D6Bi73FJV69ehRDC7WcqJibGqY1WJT8n1iGvqjy/I+vzlNYWi8WCq1evAij/PQ4JCcGmTZvQuXNnPPfcc2jXrh1iYmIwY8aMSgW7RBXBmhsiBbhbo+bXX3/F+fPnsXHjRlu2BkCZ67Z4W3h4OHbu3OmyvSJBh9X999+PlStXIikpCbGxsdi1axcWLFjgtE/9+vUxb948zJs3DykpKVi7di2eeeYZpKenY/369WUe38/PD/fccw8++eQTpKamYtGiRQgKCsK//vUv2z5ffvkl+vfv7/K8VanlsAYK7vqg5Lbly5dDr9fjhx9+cAok16xZU+nntQoLC4NGo0FqaqrLfdYi4fr161f5+JVh7YvS2qLRaBAWFmZrU3nvcYcOHbB8+XIIIfDnn39iyZIlmDVrFvz8/PDMM8945TXRtYGZGyIPsQY8JQtHP/roIzWa41a/fv2QnZ2NdevWOW1fvnx5hY+RkJCAhg0bYvHixVi8eDF8fX3LHNaKjY3FY489hkGDBmHv3r0Veo4JEybAbDbjzTffRGJiIv797387TbWXJMmln//880+XGT0V0apVK0RHR2PZsmVOM55Onz6N7du3O+0rSRJ0Oh20Wq1tW35+Pr744guX41Y04xYQEICePXti9erVTvtbLBZ8+eWXaNSoEVq2bFnp11UVrVq1QsOGDfHVV1859UVubi5WrVplm0FVUnnvsSRJ6NSpE+bOnYvQ0NAKfw6IKoqZGyIP6d27N8LCwjBp0iTMmDEDer0eS5cuxf79+9Vums24ceMwd+5c3HvvvXjllVfQvHlzrFu3Dj/99BMAuMwMcker1WLs2LF45513EBwcjJEjRyIkJMR2f2ZmJgYMGIDRo0ejdevWCAoKwq5du7B+/XqMHDmyQu3s3r07OnbsiHnz5kEI4TI7a/jw4Xj55ZcxY8YM9OvXD0ePHsWsWbMQFxeHoqKiSvSI/JpffvllTJw4EXfccQcefPBBZGRkYObMmS7DUsOGDcM777yD0aNH46GHHsLly5fx1ltvuZ0JZc1arFixAs2aNYOvry86dOjgtg2zZ8/GoEGDMGDAADz55JPw8fHB/PnzcfDgQSxbtqzKq1mX5vvvv0dQUJDL9rvuugtvvPEGxowZg+HDh+Phhx+G0WjEm2++iYyMDMyZMwdAxd7jH374AfPnz8ftt9+OZs2aQQiB1atXIyMjA4MGDVL09RBxthRRJZQ2W6pdu3Zu99++fbuIj48X/v7+okGDBmLixIli7969LrNjSpstNWzYMJdj9uvXz2nWTWmzpUq2s7TnSUlJESNHjhSBgYEiKChI3HnnnSIxMVEAEN99911pXeHk2LFjAoAAIJKSkpzuKygoEJMmTRIdO3YUwcHBws/PT7Rq1UrMmDFD5ObmVuj4Qgjx7rvvCgCibdu2LvcZjUbx5JNPioYNGwpfX1/RtWtXsWbNGjFu3DiX2U0oZ7aU1aeffipatGghfHx8RMuWLcWiRYvcHm/RokWiVatWwmAwiGbNmonZs2eLhQsXCgDi5MmTtv1OnTolEhISRFBQkABgO4672VJCCLFlyxZx0003iYCAAOHn5yd69eolvv/+e6d9rLOldu3a5bS9tNdUkvXzUNqX1Zo1a0TPnj2Fr6+vCAgIEAMHDhTbtm2z3V+R9/ivv/4S99xzj7juuuuEn5+fCAkJET169BBLliwps41EVSEJ4ZBrJCIC8Nprr+F///sfUlJSqrxyMhGRWjgsRXSNe//99wEArVu3hslkwq+//or33nsP9957LwMbIqqVGNwQXeP8/f0xd+5cnDp1CkajEbGxsXj66afxv//9T+2mERFVCYeliIiIqE7hVHAiIiKqUxjcEBERUZ3C4IaIiIjqlGuuoNhiseD8+fMICgpSfCEsIiIi8gwhBLKzsxETE1PuAqPXXHBz/vx5NG7cWO1mEBERURWcOXOm3GUqrrngxrrE+JkzZxAcHKzYcU0mEzZs2ICEhATo9XrFjkvO2M/ew772Dvazd7CfvcdTfZ2VlYXGjRu7vVRISddccGMdigoODlY8uPH390dwcDB/cTyI/ew97GvvYD97B/vZezzd1xUpKWFBMREREdUpDG6IiIioTmFwQ0RERHXKNVdzQ0REdZvZbIbJZHLaZjKZoNPpUFBQALPZrFLLrg3V6WsfH59yp3lXBIMbIiKqE4QQSEtLQ0ZGhtv7oqKicObMGa5x5mHV6WuNRoO4uDj4+PhUqw0MboiIqE6wBjYRERHw9/d3OrFaLBbk5OQgMDBQkcwAla6qfW1dZDc1NRWxsbHVCkIZ3BARUa1nNpttgU14eLjL/RaLBYWFhfD19WVw42HV6esGDRrg/PnzKCoqqtY0cr7DRERU61lrbPz9/VVuCVWHdTiqunVRDG6IiKjOYD1N7abU+8fghoiIiOoUBjdERER1SP/+/TFt2jTVj6EmFhQTERGpoLwhmHHjxmHJkiWVPu7q1auv+etnMbhRisUM38IrwNVTQEQLtVtDREQ1XGpqqu3nFStW4MUXX8TRo0dt2/z8/Jz2N5lMFQpa6tWrp1wjaykOSykl5wIGH5oG3YfxareEiIhqgaioKNtXSEgIJEmy3S4oKEBoaCi+/vpr9O/fH76+vvjyyy9x+fJl3HPPPWjUqBH8/f3RoUMHLFu2zOm4JYeUmjZtitdeew0PPPAAgoKCEBsbi48//rhSbb169SrGjh2LsLAw+Pv7Y8iQITh+/Ljt/tOnT2PEiBEICwtDUFAQ4uPjkZiYaHvsmDFj0KBBA/j5+aFFixZYvHhx1TuuApi5UYpWnr4mWUyAxQJwHQUiItUIIZBvsk8ntlgsyC80Q1dY5PF1bvz0WsVm/Tz99NN4++23sXjxYhgMBhQUFKBbt254+umnERwcjB9//BH33XcfmjVrhp49e5Z6nLfffhsvv/wynnvuOXzzzTd45JFHcOONN6J169YVasf48eNx/PhxrF27FsHBwXj66acxdOhQHD58GHq9Ho8++igKCwuxefNm+Pn5Yffu3QgMDAQAvPDCCzh8+DDWrVuH+vXr4++//0Z+fr4i/VMaBjdK0RnsP5sLAY2vem0hIrrG5ZvMaPviT6o89+FZg+Hvo8zpddq0aRg5cqTTtieffNL285QpU7B+/XqsXLmyzOBm6NChmDx5MgA5YJo7dy42btxYoeDGGtRs27YNvXv3BgAsXboUjRs3xpo1a/Cvf/0LKSkpuPPOO9GhQwdYLBbUr18fwcHBAICUlBR06dIF3bt3ByBnkjyN6QWlaB2ug2E2qtcOIiKqM6wBgZXZbMarr76Kjh07Ijw8HIGBgdiwYQNSUlLKPE7Hjh1tP1uHv9LT0yvUhiNHjkCn0zkFT+Hh4WjVqhWOHDkCAJg6dSpeeeUV9OnTBzNnzsTBgwdt+z7yyCNYvnw5OnfujP/+97/Yvn17hZ63Opi5UYpjcFNUqF47iIgIfnotDs8abLttsViQnZWNoOAgrwxLKSUgIMDp9ttvv425c+di3rx56NChAwICAjBt2jQUFpZ93ilZiCxJEiwWS4XaIIQodbt1+G3ixIkYPHgwfvzxR/z000+YM2cO3nrrLUydOhVDhgzB6dOn8eOPP+Lnn3/GwIED8eijj+Ktt96q0PNXBTM3SpEkWKTiDzQzN0REqpIkCf4+OqcvPx+tyzZPfHlyleQtW7bgtttuw7333otOnTqhWbNmToW9ntC2bVsUFRXhjz/+sG27fPkyjh07hjZt2ti2NW7cGJMmTcKqVavw6KOP4tNPP7Xd16BBA4wfPx5ffvkl5s2bV+mC5spicKMgi1QcGRcxuCEiIuU1b94cSUlJ2L59O44cOYKHH34YaWlpHn3OFi1a4LbbbsODDz6IrVu3Yv/+/bj33nvRsGFD3HbbbQDk2qCffvoJJ0+exN69e7FlyxZbPc+LL76I7777Dn///TcOHTqEH374wSko8gQGNwoyS8WjfGaTug0hIqI66YUXXkDXrl0xePBg9O/fH1FRUbj99ts9/ryLFy9Gt27dMHz4cMTHx0MIgcTERNtwl9lsxqOPPoo2bdpg6NChaN68OT744AMA8sUwn332WXTs2BE33ngjtFotli9f7tH2SqK0wbQ6KisrCyEhIcjMzLRVcivBZDKh6I0W8DNdBR7eDER3UuzYZGcymZCYmIihQ4de8ytwehr72jvYz8ooKCjAyZMnERcXB19f19mqFosFWVlZCA4O9njNzbWuOn1d1vtYmfM332EFWayZGxYUExERqYbBjYJswQ0LiomIiFSjanCzefNmjBgxAjExMZAkCWvWrClz/9WrV2PQoEFo0KABgoODER8fj59+UmeRJncsGhYUExERqU3V4CY3NxedOnXC+++/X6H9N2/ejEGDBiExMRF79uzBgAEDMGLECOzbt8/DLa0Ye+aGw1JERERqUXURvyFDhmDIkCEV3n/evHlOt1977TV89913+P7779GlSxeFW1d59pobZm6IiIjUUqtrbiwWC7Kzs2vM5d2ZuSEiIlJfrb78wttvv43c3Fzcfffdpe5jNBphNNozKVlZWQDk6Zcmk3Lr0ZhMJpiLa26KCvMhFDw22VnfMyXfO3KPfe0d7GdlmEwmCCFgsVjcXlbAuuqJdR/ynOr0tcVigRACJpMJWq3zZSwq8ztSa4ObZcuWYebMmfjuu+8QERFR6n6zZ8/GSy+95LJ9w4YN8Pf3V7RN1xdnbg7t34tT50IVPTY5S0pKUrsJ1wz2tXewn6tHp9MhKioKOTk5ZV5nKTs724uturZVpa8LCwuRn5+PzZs3o6ioyOm+vLy8Ch+nVgY3K1aswIQJE7By5UrcfPPNZe777LPPYvr06bbbWVlZaNy4MRISEhRfxO/Kx/MBAO1bt0DbHkMVOzbZmUwmJCUlYdCgQVzwzMPY197BflZGQUEBzpw5g8DAQLeL+AkhkJ2djaCgII9e+4mq19cFBQXw8/PDjTfe6HYRv4qqdcHNsmXL8MADD2DZsmUYNmxYufsbDAYYDAaX7Xq9XvE/JBaN3J1aUQQt/0h5lCfeP3KPfe0d7OfqMZvNkCQJGo3G7aq41uER6z51Sf/+/dG5c2eXSTdWM2fOxJo1a5CcnOyV9lSnrzUaDSRJcvv7UJnfD1Xf4ZycHCQnJ9s6/OTJk0hOTkZKSgoAOesyduxY2/7Lli3D2LFj8fbbb6NXr15IS0tDWloaMjMz1Wi+C9uFM1lQTERE5RgxYkSpow+///47JEnC3r17vdyqukHV4Gb37t3o0qWLbRr39OnT0aVLF7z44osAgNTUVFugAwAfffQRioqK8OijjyI6Otr29fjjj6vS/pKsmRtOBSciovJMmDABv/76K06fPu1y36JFi9C5c2d07dpVhZbVfqoGN/3794cQwuVryZIlAIAlS5Zg48aNtv03btxY5v5q4+UXiIioooYPH46IiAiXc1heXp6ttvTy5cu455570KhRI/j7+6NDhw5YtmxZtZ7XYrFg1qxZaNSoEQwGAzp37oz169fb7i8sLMRjjz2G6Oho+Pr6omnTppg9e7bt/pkzZyI2NhYGgwExMTGYOnVqtdrjCbWu5qYmswc3nNJJRKQqIQCTw+wai0W+XagFPF1zo/cHKlBIq9PpMHbsWCxZsgQvvviirfh25cqVKCwsxJgxY5CXl4du3brh6aefRnBwMH788Ufcd999aNasGXr27Fml5r377rt4++238dFHH6FLly5YtGgRbr31Vhw6dAgtWrTAe++9h7Vr1+Lrr79GbGwszpw5gzNnzgAAvvnmG8ydOxfLly9Hu3btkJaWhv3791epHZ7E4EZBZonXliIiqhFMecBrMbabGgCh3nru584DPgEV2vWBBx7Am2++iY0bN2LAgAEA5CGpkSNHIiwsDGFhYXjyySdt+0+ZMgXr16/HypUrqxzcvPXWW3j66afx73//GwDw+uuv47fffsO8efPwwQcfICUlBS1atMANN9wASZLQpEkT22NTUlIQFRWFm2++GXq9HrGxsejRo0eV2uFJdatkXGW2mhsWFBMRUQW0bt0avXv3xqJFiwAA//zzD7Zs2YIHHngAgDwL7NVXX0XHjh0RHh6OwMBAbNiwwaketTKysrJw/vx59OnTx2l7nz59cOTIEQDA+PHjkZycjFatWmHq1KnYsGGDbb9//etfyM/PR7NmzfDggw/i22+/dVmPpiZg5kZBvLYUEVENofeXMyjFLBYLsrKzERwU5Pmp4PrKLRA7YcIEPPbYY/jggw+wePFiNGnSBAMHDgQgr8Q/d+5czJs3Dx06dEBAQACmTZtW5kKFFVFy/RkhhG1b165dcfLkSaxbtw4///wz7r77btx888345ptv0LhxYxw9ehRJSUn4+eefMXnyZLz55pvYtGlTjVrKgJkbBdmngjO4ISJSlSTJQ0OOX3p/122e+KrkwnV33303tFotvvrqK3z22We4//77bYHGli1bcNttt+Hee+9Fp06d0KxZMxw/frzK3RIcHIyYmBhs3brVafv27dvRpk0bp/1GjRqFTz75BCtWrMCqVatw5coVAICfnx9uvfVWvPfee9i4cSN+//13HDhwoMpt8gRmbhRknwrOYSkiIqqYwMBAjBo1Cs899xwyMzMxfvx4233NmzfHqlWrsH37doSFheGdd95BWlqaUyBSWU899RRmzJiB6667Dp07d8bixYuRnJyMpUuXAgDmzp2L6OhodO7cGRqNBitXrkRUVBRCQ0OxZMkSmM1m9OzZE/7+/vjiiy/g5+fnVJdTEzC4URCnghMRUVVMmDABCxcuREJCAmJjY23bX3jhBZw8eRKDBw+Gv78/HnroIdx+++3VWrx26tSpyMrKwhNPPIH09HS0bdsWa9euRYsWLQDIwdbrr7+O48ePQ6vV4vrrr0diYiI0Gg1CQ0MxZ84cTJ8+HWazGR06dMD333+P8PDwaveBkiRhvXznNSIrKwshISHIzMxU/NpSyUtn4vpT7wOxvYEH1il2bLIzmUxITEzE0KFDa9T4bl3EvvYO9rMyCgoKcPLkScTFxbm9tpTFYkFWVhaCg4Pr3OUXaprq9HVZ72Nlzt98hxXE2VJERETqY3CjIDOHpYiIiFTH4EZBttlSLCgmIiJSDYMbBdmHpZi5ISIiUguDGwUxc0NEpK5rbI5MnaPU+8fgRkGcCk5EpA7rTLO8vLxy9qSazLryslarrdZxuM6NguyXX2DmhojIm7RaLUJDQ5Geng4A8Pf3d7rEgMViQWFhIQoKCjgV3MOq2tcWiwUXL16Ev78/dLrqhScMbhTEmhsiIvVERUUBgC3AcSSEQH5+Pvz8/Fyuq0TKqk5fazQaxMbGVvs9YnCjIPu1pQoBISp9fREiIqo6SZIQHR2NiIgImEwmp/tMJhM2b96MG2+8kYslelh1+trHx0eRzBqDGwXZ1rkBALMJ0Pmo1xgiomuUVqt1qdnQarUoKiqCr68vgxsPqwl9zYFHBVk0Dm8ih6aIiIhUweBGQRbHzA2LiomIiFTB4EZJkgaCRcVERESqYnCjNK1B/l7E4IaIiEgNDG6UpnWYMUVERERex+BGadriGVIMboiIiFTB4EZpOuuwFIMbIiIiNTC4UZotc8OaGyIiIjUwuFGajgXFREREamJwozDBmhsiIiJVMbhRGqeCExERqYrBjdJ0zNwQERGpicGN0jTFwQ0zN0RERKpgcKM0Zm6IiIhUxeBGaSwoJiIiUhWDG6VxKjgREZGqGNwojYv4ERERqYrBjcKElpdfICIiUhODG6XpmLkhIiJSE4MbpVmHpZi5ISIiUgWDG6VxthQREZGqGNwozVpzw2EpIiIiVTC4UZqOw1JERERqYnCjNGZuiIiIVMXgRmGCBcVERESqYnCjNB0zN0RERGpicKM0Zm6IiIhUxeBGabz8AhERkaoY3ChNq5e/c50bIiIiVTC4UZqklb8LoW47iIiIrlEMbpQmFXcpgxsiIiJVMLhRmi24sajbDiIiomsUgxulScXfGdwQERGpgsGN0qyZG3BYioiISA0MbpTGYSkiIiJVMbhRGoMbIiIiVaka3GzevBkjRoxATEwMJEnCmjVryn3Mpk2b0K1bN/j6+qJZs2b48MMPPd/QymBwQ0REpCpVg5vc3Fx06tQJ77//foX2P3nyJIYOHYq+ffti3759eO655zB16lSsWrXKwy2tBAY3REREqtKp+eRDhgzBkCFDKrz/hx9+iNjYWMybNw8A0KZNG+zevRtvvfUW7rzzTg+1srIY3BAREalJ1eCmsn7//XckJCQ4bRs8eDAWLlwIk8kEvV7v8hij0Qij0X6dp6ysLACAyWSCyWRSrG3WYxWZzdABEBYLihQ8Psms/azke0fusa+9g/3sHexn7/FUX1fmeLUquElLS0NkZKTTtsjISBQVFeHSpUuIjo52eczs2bPx0ksvuWzfsGED/P39FW/jjp270B9AQX4eNiQmKn58kiUlJandhGsG+9o72M/ewX72HqX7Oi8vr8L71qrgBgAkSXK6LYovc1Byu9Wzzz6L6dOn225nZWWhcePGSEhIQHBwsGLtMplMSEpKQs9e8cBRwNfgg6FDhyp2fJJZ+3nQoEFuM3WkHPa1d7CfvYP97D2e6mvryEtF1KrgJioqCmlpaU7b0tPTodPpEB4e7vYxBoMBBoPBZbter/fIB1xXfEwJgr9AHuSp949csa+9g/3sHexn71G6rytzrFq1zk18fLxLmmvDhg3o3r17zfmw8sKZREREqlI1uMnJyUFycjKSk5MByFO9k5OTkZKSAkAeUho7dqxt/0mTJuH06dOYPn06jhw5gkWLFmHhwoV48skn1Wi+e5wKTkREpCpVh6V2796NAQMG2G5ba2PGjRuHJUuWIDU11RboAEBcXBwSExPxn//8Bx988AFiYmLw3nvv1aBp4GBwQ0REpDJVg5v+/fvbCoLdWbJkicu2fv36Ye/evR5sVTVZC5sZ3BAREamiVtXc1AqsuSEiIlIVgxulcViKiIhIVQxulMbghoiISFUMbpTG4IaIiEhVDG6UxuCGiIhIVQxuFMfZUkRERGpicKM02zWuOFuKiIhIDQxulMZhKSIiIlUxuFEagxsiIiJVMbhRmuTQpVzIj4iIyOsY3CjNKbhh9oaIiMjbGNwojcENERGRqhjcKI3BDRERkaoY3CjNNhUcDG6IiIhUwOBGaczcEBERqYrBjeKYuSEiIlITgxulcSo4ERGRqhjcKI3DUkRERKpicKM0BjdERESqYnCjNA5LERERqYrBjdI4FZyIiEhVDG48gRfPJCIiUg2DG09gcENERKQaBjeewOCGiIhINQxuPIHBDRERkWoY3HhEcVExgxsiIiKvY3DjCbbp4JwKTkRE5G0MbjyBw1JERESqYXDjCbbghpkbIiIib2Nw4wkSa26IiIjUwuDGEzgsRUREpBoGN57A4IaIiEg1DG48gcENERGRahjceAKDGyIiItUwuPEEBjdERESqYXDjCZwtRUREpBoGN57AzA0REZFqGNx4gi1zo24ziIiIrkUMbjyBmRsiIiLVMLjxBAY3REREqmFw4wkMboiIiFTD4MYTGNwQERGphsGNJzC4ISIiUg2DG09gcENERKQaBjeewOCGiIhINQxuPIIrFBMREamFwY0n2Bbx4yp+RERE3sbgxhOsw1JcopiIiMjrGNx4AmtuiIiIVMPgxhMY3BAREamGwY0nMLghIiJSDYMbT2BwQ0REpBoGN57A4IaIiEg1qgc38+fPR1xcHHx9fdGtWzds2bKlzP2XLl2KTp06wd/fH9HR0bj//vtx+fJlL7W2ghjcEBERqUbV4GbFihWYNm0ann/+eezbtw99+/bFkCFDkJKS4nb/rVu3YuzYsZgwYQIOHTqElStXYteuXZg4caKXW14OiYv4ERERqUXV4Oadd97BhAkTMHHiRLRp0wbz5s1D48aNsWDBArf779ixA02bNsXUqVMRFxeHG264AQ8//DB2797t5ZaXg4v4ERERqUan1hMXFhZiz549eOaZZ5y2JyQkYPv27W4f07t3bzz//PNITEzEkCFDkJ6ejm+++QbDhg0r9XmMRiOMRqPtdlZWFgDAZDLBZDIp8EpgO571uxYSNACKigohFHwOcu5n8iz2tXewn72D/ew9nurryhxPteDm0qVLMJvNiIyMdNoeGRmJtLQ0t4/p3bs3li5dilGjRqGgoABFRUW49dZb8X//93+lPs/s2bPx0ksvuWzfsGED/P39q/ci3EhKSkL8pcuIALA/ORlnUwIUfw6S+5m8g33tHexn72A/e4/SfZ2Xl1fhfVULbqwk6xBOMSGEyzarw4cPY+rUqXjxxRcxePBgpKam4qmnnsKkSZOwcOFCt4959tlnMX36dNvtrKwsNG7cGAkJCQgODlbsdZhMJiQlJWHQoEHwzfwMyD6ITh07omPHoYo9Bzn3s16vV7s5dRr72jvYz97BfvYeT/W1deSlIlQLburXrw+tVuuSpUlPT3fJ5ljNnj0bffr0wVNPPQUA6NixIwICAtC3b1+88soriI6OdnmMwWCAwWBw2a7X6z3yAdfr9dBotQAAnUYC+EvkEZ56/8gV+9o72M/ewX72HqX7ujLHUq2g2MfHB926dXNJWyUlJaF3795uH5OXlweNxrnJ2uJAQtSk4l1OBSciIlKNqrOlpk+fjk8//RSLFi3CkSNH8J///AcpKSmYNGkSAHlIaezYsbb9R4wYgdWrV2PBggU4ceIEtm3bhqlTp6JHjx6IiYlR62W4YnBDRESkGlVrbkaNGoXLly9j1qxZSE1NRfv27ZGYmIgmTZoAAFJTU53WvBk/fjyys7Px/vvv44knnkBoaChuuukmvP7662q9BPcY3BAREalG9YLiyZMnY/LkyW7vW7Jkicu2KVOmYMqUKR5uVTVxET8iIiLVqH75hTqJmRsiIiLVMLjxCK5QTEREpBYGN57AzA0REZFqGNx4gjW4ATM3RERE3sbgxhOYuSEiIlINgxtPYHBDRESkmioFN2fOnMHZs2dtt3fu3Ilp06bh448/VqxhtRqDGyIiItVUKbgZPXo0fvvtNwBAWloaBg0ahJ07d+K5557DrFmzFG1grcTghoiISDVVCm4OHjyIHj16AAC+/vprtG/fHtu3b8dXX33lduG9aw4X8SMiIlJNlYIbk8lku9L2zz//jFtvvRUA0Lp1a6SmpirXutqKmRsiIiLVVCm4adeuHT788ENs2bIFSUlJuOWWWwAA58+fR3h4uKINrJVswQ2nghMREXlblYKb119/HR999BH69++Pe+65B506dQIArF271jZcdU3jsBQREZFqqnThzP79++PSpUvIyspCWFiYbftDDz0Ef39/xRpXa3FYioiISDVVytzk5+fDaDTaApvTp09j3rx5OHr0KCIiIhRtYK3EYSkiIiLVVCm4ue222/D5558DADIyMtCzZ0+8/fbbuP3227FgwQJFG1grMXNDRESkmioFN3v37kXfvn0BAN988w0iIyNx+vRpfP7553jvvfcUbWCtxOCGiIhINVUKbvLy8hAUFAQA2LBhA0aOHAmNRoNevXrh9OnTijawVnIMboQAci+r2x4iIqJrSJWCm+bNm2PNmjU4c+YMfvrpJyQkJAAA0tPTERwcrGgDayXH2VI/PgG82Qw4uk7dNhEREV0jqhTcvPjii3jyySfRtGlT9OjRA/Hx8QDkLE6XLl0UbWCt5Ji52b1Q/vmXl9VrDxER0TWkSlPB77rrLtxwww1ITU21rXEDAAMHDsQdd9yhWONqLXezpVh/Q0RE5BVVCm4AICoqClFRUTh79iwkSULDhg25gJ+Vu4JiBjdEREReUaVhKYvFglmzZiEkJARNmjRBbGwsQkND8fLLL8Ni4UkccLdCMde8ISIi8oYqZW6ef/55LFy4EHPmzEGfPn0ghMC2bdswc+ZMFBQU4NVXX1W6nbULMzdERESqqVJw89lnn+HTTz+1XQ0cADp16oSGDRti8uTJDG6swQ1Yc0NERORtVRqWunLlClq3bu2yvXXr1rhy5Uq1G1Xruc3ccFiKiIjIG6oU3HTq1Anvv/++y/b3338fHTt2rHajaj0OSxEREammSsNSb7zxBoYNG4aff/4Z8fHxkCQJ27dvx5kzZ5CYmKh0G2sfyU1BMTM3REREXlGlzE2/fv1w7Ngx3HHHHcjIyMCVK1cwcuRIHDp0CIsXL1a6jbWP22tLMbghIiLyhiqvcxMTE+NSOLx//3589tlnWLRoUbUbVqtxET8iIiLVVClzQ+VgzQ0REZFqGNx4AmdLERERqYbBjScwc0NERKSaStXcjBw5ssz7MzIyqtOWusPtbCkGN0RERN5QqeAmJCSk3PvHjh1brQbVCe4KijlbioiIyCsqFdxwmncFcViKiIhINay58QQOSxEREamGwY0nuM3cqNMUIiKiaw2DG0/gIn5ERESqYXDjCbbgxmzfxuCGiIjIKxjceAILiomIiFTD4MYTJK383eKQuWHRDRERkVcwuPEEZm6IiIhUw+DGEzTFmRvW3BAREXkdgxtPsGZuLLxwJhERkbcxuPEEZm6IiIhUw+DGE9zV3LCgmIiIyCsY3HiC29lSRERE5A0MbjzB3bAUEREReQWDG09g5oaIiEg1DG48wd1VwYmIiMgrGNx4gnVYymxStx1ERETXIAY3nmAblmJwQ0RE5G0MbjyhtMwNF/IjIiLyONWDm/nz5yMuLg6+vr7o1q0btmzZUub+RqMRzz//PJo0aQKDwYDrrrsOixYt8lJrK8i6zo1LcMMaHCIiIk/TqfnkK1aswLRp0zB//nz06dMHH330EYYMGYLDhw8jNjbW7WPuvvtuXLhwAQsXLkTz5s2Rnp6OoqIiL7e8HKUNS1mK7FkdIiIi8ghVg5t33nkHEyZMwMSJEwEA8+bNw08//YQFCxZg9uzZLvuvX78emzZtwokTJ1CvXj0AQNOmTb3Z5IrRlJK5sRQBMHi9OURERNcS1YalCgsLsWfPHiQkJDhtT0hIwPbt290+Zu3atejevTveeOMNNGzYEC1btsSTTz6J/Px8bzS54qyZG3Oh83bOniIiIvI41TI3ly5dgtlsRmRkpNP2yMhIpKWluX3MiRMnsHXrVvj6+uLbb7/FpUuXMHnyZFy5cqXUuhuj0Qij0Wi7nZWVBQAwmUwwmZQLNqzHMplMgNkCPQBhNkFy3KewANAxwKkOp34mj2Jfewf72TvYz97jqb6uzPFUHZYCAEmSnG4LIVy2WVksFkiShKVLlyIkJASAPLR111134YMPPoCfn5/LY2bPno2XXnrJZfuGDRvg7++vwCtwlpSUhKD8M7gJgFTi8gu/JP0Eoz5U8ee8FiUlJandhGsG+9o72M/ewX72HqX7Oi8vr8L7qhbc1K9fH1qt1iVLk56e7pLNsYqOjkbDhg1tgQ0AtGnTBkIInD17Fi1atHB5zLPPPovp06fbbmdlZaFx48ZISEhAcHCwQq9GjiiTkpIwaNAg6DNOAH+57jNwQD8guKFiz3ktcupnvV7t5tRp7GvvYD97B/vZezzV19aRl4pQLbjx8fFBt27dkJSUhDvuuMO2PSkpCbfddpvbx/Tp0wcrV65ETk4OAgMDAQDHjh2DRqNBo0aN3D7GYDDAYHAt4tXr9R75gOv1euh93BcN6zUA+EulCE+9f+SKfe0d7GfvYD97j9J9XZljqbrOzfTp0/Hpp59i0aJFOHLkCP7zn/8gJSUFkyZNAiBnXcaOHWvbf/To0QgPD8f999+Pw4cPY/PmzXjqqafwwAMPuB2SUo1USrfyQppEREQep2rNzahRo3D58mXMmjULqampaN++PRITE9GkSRMAQGpqKlJSUmz7BwYGIikpCVOmTEH37t0RHh6Ou+++G6+88opaL8G90oIbzpYiIiLyONULiidPnozJkye7vW/JkiUu21q3bl3zC8JKW6jPUsMWGyQiIqqDVL/8Qp0kMbghIiJSC4MbTyi15obBDRERkacxuPEEDksRERGphsGNJ3BYioiISDUMbjxBw9lSREREamFw4wlc54aIiEg1DG48gcNSREREqmFw4wmlFRSbje63ExERkWIY3HhCacNSxhzvtoOIiOgaxODGE0obljJW/IqmREREVDUMbjyhtGEpY7Z320FERHQNYnCjkPRsI6bv0KLdSz8DkgRAct2pINPr7SIiIrrWMLhRiEYCzEJCYZFF3uCu7obDUkRERB7H4EYhWo09U2OxCPdDUwUMboiIiDyNwY1CtJI9uCmyCPdFxczcEBEReRyDG4VoHDM3gpkbIiIitTC4UYjOIbgxW0QpNTecLUVERORpDG4UonEZlnLoWp8g+TuHpYiIiDyOwY1CdGUVFPuFyd85LEVERORxDG4U4lhz41JQ7BcqfzflAmZePJOIiMiTGNwoSAMBoLig2HFYyhrcAByaIiIi8jAGNwqyJm+KSg5LaQ2Azk/+mcENERGRRzG4UZA1uLGUHJbSaAHfYPlnzpgiIiLyKAY3CrIGN2aLADQOXStpAX1x5qYwz/sNIyIiuoYwuFGQtTNdpoJLEqDRyT8Ls9fbRUREdC1hcKMg61I3ckFxiWEp620LgxsiIiJPYnCjIK21oNhcoqBY0tpvM3NDRETkUQxuFFRq5kbSMHNDRETkJQxuFKR1LCh2rLnRaO0FxsLi/YYRERFdQxjcKMi6RnGRu9lS1oJiC1coJiIi8iQGNwrScFiKiIhIdQxuFKQpraBYo2FBMRERkZcwuFGQtTNdri0lcSo4ERGRtzC4UZDTCsUlh6U0DG6IiIi8gcGNgpwvv1BiET8OSxEREXkFgxsFaUqbCs6CYiIiIq9hcKMgW0GxS3DDzA0REZG3MLhRkFNBsctsKa5zQ0RE5A0MbhSkkQSAUgqKrZkcDksRERF5FIMbBUmlFRQ7DUvx8gtERESexOBGQdbOdHttKRYUExEReQWDGwWVOltKa2DNDRERkZcwuFGQLbgpWVBsCORsKSIiIi9hcKOgUjM3PoEsKKaKObkZWDQESD+idkuIiGotBjcKKvXyCz4BLCimivlsBJCyHVg+Ru2WEBHVWgxuFORUUOw0LBXEmhuqnJx0tVtARFRrMbhRUOmZm0DOlqJKEmo3gIio1mJwoyCngmLrojcAC4qJiIi8iMGNgkq9KjgLitUlBPudiOgawuBGQU41N3DM3DjW3PAk63WLhwLvdwfMJrVbUnGCw1JERFXF4EZBTpdfKCqw3+HDYSlVpWwHrpwALhxSuyVEROQFDG4UpHUMbkx59jt0BhYUq8XiMPW+Vk3DZ+aGiKiqGNwoSHIsKDblO9/BzI06HKfec6iHiOiawOBGQdYSYjlzk+98py1zw3VuvMoxmKxVmRsiIqoq1YOb+fPnIy4uDr6+vujWrRu2bNlSocdt27YNOp0OnTt39mwDK0EqbVgKsGduLDzBepVT5qYW9T2zTEREVaZqcLNixQpMmzYNzz//PPbt24e+fftiyJAhSElJKfNxmZmZGDt2LAYOHOilllaM01TwkpkbDkupw8LMDRHRtUbV4Oadd97BhAkTMHHiRLRp0wbz5s1D48aNsWDBgjIf9/DDD2P06NGIj4/3UksrRlNW5oYFxepw6u/alA2pTW0lIqpZdGo9cWFhIfbs2YNnnnnGaXtCQgK2b99e6uMWL16Mf/75B19++SVeeeWVcp/HaDTCaDTabmdlZQEATCYTTCbl1j0xmUzQFJ+QTGYzRGGebaUbk8kEjZBrcizmQpgVfN5rjfU9q/B7V1gAffGPRYUFEDW8761tFQCKVG5rpfuaqoT97B3sZ+/xVF9X5niqBTeXLl2C2WxGZGSk0/bIyEikpaW5fczx48fxzDPPYMuWLdDpKtb02bNn46WXXnLZvmHDBvj7+1e+4WXQFBfdnDqdgqKCHNuJKjExEXEXj6IjgNTz57A7MVHR570WJSUlVWg/38IrGFz8884d23DxcLbnGqWA24q/W8xmJNaQz0lF+5qqh/3sHexn71G6r/Py8srfqZhqwY2V5HgNJgBCCJdtAGA2mzF69Gi89NJLaNmyZYWP/+yzz2L69Om221lZWWjcuDESEhIQHBxc9YaXYDKZ8POSnwEAMQ0bQer8OcTKe2FOmI2hXYZCs+cCcBaIjozA0KFDFXvea43JZEJSUhIGDRoEvV5f/gMyzwLFa/f16NYFosXgsvdX2z75m0ajUf1zUum+piphP3sH+9l7PNXX1pGXilAtuKlfvz60Wq1LliY9Pd0lmwMA2dnZ2L17N/bt24fHHnsMAGCxWCCEgE6nw4YNG3DTTTe5PM5gMMBgMLhs1+v1in/ANbaYTIKu1SDg2XPQaYu7WO8j7wMBDX+xqq3C759DVZkOFqCW9L0E1Jg/wJ74XSFX7GfvYD97j9J9XZljqVZQ7OPjg27durmkrZKSktC7d2+X/YODg3HgwAEkJyfbviZNmoRWrVohOTkZPXv29FbTS2UNboosxcWgWofYkevcqMOxoNhSm8baWVBMRFRVqg5LTZ8+Hffddx+6d++O+Ph4fPzxx0hJScGkSZMAyENK586dw+effw6NRoP27ds7PT4iIgK+vr4u29Viu3CmuzVKeOFMdThOvTczsCQiuhaoGtyMGjUKly9fxqxZs5Camor27dsjMTERTZo0AQCkpqaWu+ZNTWKbCm52F9xwnRtVOGbKzIXqtaOyuIgfEVGVqb5C8eTJk3Hq1CkYjUbs2bMHN954o+2+JUuWYOPGjaU+dubMmUhOTvZ8IytI43htqZKk4q5m5sa7OCzlXbsXAe91Ba6cVLslRHQNUz24qUus9cQWSxmZGwY33uWUueGwlMf98B/gyj/AuqfVbgkRXcMY3ChIW7Kg2JG15obDUt7leMmFmjIsteVt4JOBgLFmr7lTLSVX6CYi8iIGNwqyLs9jcTssxcyNKhwzNzVlWOqXWcC53cDuxWq3xHP4OSciFTG4UZAtc8OC4prD8SRrriHBjVWRsfT7antBMT/nRKQiBjcKksosKOY6N6pwqrmpYcGNm5W47Wp5cMPMDRGpiMGNgqydWXZBscX1PvIcUYNnS0l1+NdP8HNOROqpw39dvc9lhWKnOzkspYqaPCxVZuamluPnnIhUxOBGQRoWFFefEMrOIqrJwQ3KCG5qfc0NMzdEpB4GNwrSVKSgmDU3ZUt8CnijGXDxmDLHq2mzpRyDrbo8LMXhVyJSUR3+6+p9tpqbsq4txXR92c7vldejSftTmePVtMsvOM6QqssFxczcEJGKGNwoyHb5BXc1N7bLL/CPfpmKigMQpRaBq2kXzjQ7Bjclfv1q+1CUIwbxRKQiBjcK0kjyycltcMOC4oqxZlcKc5U5Xk27tlRRGdmjupTtYG0ZEamIwY2Cyr5wJmtuKsSa2fBEcFMThqUcMzclC5zr0meDQTwRqYjBjYKsnVl2QTH/6JfJesJXLLipYRfOdAxoSmaS6tJnoy5loYio1mFwo6Aya25YUFwx1oJbj9Tc1IDMTVEZmZu69NlgbRkRqYjBjYJ8ipMzOUY3GQIWFFeMJzM3NaHmpsxhqToU3DBzQ0QqYnCjoBC9/D3HWIS8whIBDguKK0bxmhuHk2xNGJZyLCguGWyVDAhq2+wpx77m55yIVMTgRkEGLeBfnL5JzypxxWcWFJdPCOWHpWraOjeVydzUtuyHY7BWl7JQRFTrMLhRkCQBDQINAIALWQXOd1prbvhHv3QWM2yL1ymVualpF850DLDKmy1V2wJhx/Yyc0NEKmJwo7CIYDm4Sc8ukbnhsFT5HLMadXW2VJnDUiU+G7UtEHYKbmpZ1omI6hQGNwqLCCwluLEOSwE1s6hYCODXV4AjP6jXBseshmLDUjVstlRlhqVqW+bGMXisiZ9xIrpm6NRuQF3TIMgHAJDuMizlEEdaigCNjxdbVQHHNwCb35R/npmpThscsxqFHghuasKwVFEZw1IlMze1LctX02amEdE1i5kbhZU+LOUQRzqtvVJDTgLZqWq3oMSwVI4yx6xpfe34Gl0W8SuR7ah1w1IOr6fIWPtmexFRncHgRmH2YakSmRunYanik9aprcBrDYGdn3ipdWVwbJ9aHIMPj8yWqgnBTSUyN7VtWMqpvaL2tZ+I6gwGNwprEGSdLVVKQTFgP4mtmij/J5/4pJdaVwbH9qlVeOu4em9RgTKZi5o2VFLWsJTLbKlalrkp+bkpMrrfj4jIwxjcKCyiOLhxqblxl7mpSTNKJIePglJZk8oqWfCrxIwpS20allI4c5Pl5aHGku1lcENEKmFwozBrcJNVUIQCk8PJSuMuuPFQTcKpbcD6Z6telGvKV7Y9FVUyuFEiyKppwY23Coq3vQe80xrY/n9VP0ZllQzWzAxuiEgdDG4UFuSrg69e7lanVYolyZ4dER7O3CwZCuyYD2x7t+KPcfwv26TQGjOVVfI/fSUyNzVuEb+ypoIrWFCc9IL8fcP/qn6MymLmhohqCAY3CpMkCRFBvgDKKCq2nbQ8PJvk0rGK7+u0xoybzM3p34FjP1W/TRVtA6DQsFSJheWKVF7rxlyZRfxqWUFuyZqbmrCukDdd/qdmZAeJiMGNJ0SUVlTs4y9/v/w38PltQN5lzzZEkiq+b5FDIOZuOGvxLcBXdwOX/q5+u0rj6WEpQLkp5lVVZkFxHVqhGLi2Mjd/JQL/1xVYPlrtlhARGNx4RGRwKZmbuH7y989vBU5s9EJLyglu/vkNSJoh/8ftGNyUDCocT8gp25VrXkkumRsFApGaFtyUOSxV268tVbLm5hrK3OyYL38/vkHddhARAK5Q7BHW6eAuC/m1vQ04stZ7DSkvc/PF7fL3es1K1NyUCG4ca3AuHVekaW6VHDJSYpXikkM9xhqUuSlvWKo2r1AMXFuZGyKqUZi58QDrKsUuVwZvMch5yrUnOM3AquCwVMbpsjM3jrUvFw5VuWnlKjm7RpFhqRInXCUzNzkXga/HASc2VfwxTpmbcta1UWJYSuPF/19KtrdIpVl3RHTNY3DjAdaC4ks5JTIRviFAeHPPPnlFp3E7BkEafdnXdXIMblKTPTeF3RvDUsbs6h/T6sf/AIfXyMOMFeW0QnGJ11ty9pwSw1IaffWPUVElh9nUzpIR0TWLwY0HhPnLJ5SMPDc1B1Ed3T9IqYChojOMChwujqnVlcjclAiQHI+Zd9lzdSueGJbyZOYmZUflH1PWsJRSmRvHz5LWixdoLdnXxizvPTcRkQMGNx4Q6i+fUK66DW46uH+QUsWXjidv67COEMDBVUD6Eft9jjO1ioxlr3NTMmDKvWT/WQgg70r12mzlidlSJbMhSkwvt3Lsh/JkpwGb3gQyz9q3uQxLKVRQ7Phear05LFUyc6Nglqymq8zMRCLyOAY3HmDL3OS6WfMitLH7Bym1KrBjQGD9+cRG4JsHgPm95Nv5GcAfH9n3K8iqeOYGcD6pb34TeCMO+Pvn6rbcteamvCxLRdYUcckmKJl1KiXblndFDmYcLR8D/PYKkO5Qs+QyLFXNguLcy3K2yzFj4ukaL0clM00F12jmpuRijDXR+meB1Q/zyu1UZzG48YCw4sxNtrEIhUUl/tC1HAJEtnd9UMmZJUVGOYiozB+flB3AhzfYb1uHdc7utm+zWIAV9wI7HYIbY5bz85cMZkoGGXkOwc3JzfL3M7sq3s7SVGJYSvvjf4C3WgLpf5V9TOsJ13qSL1Qom1Aya2PNwlgswP91A97r6py5OLcbLpQclsq9DLzZTA5gHYMKby5a6FJz44HMjcUM7P1CXjCvplLr2mwVZTbJU9f/XA5cPal2a4g8gsGNBwT76W1Z6oz8EicXH3/gkW1ARDvn7Y4zS4w5wLyOwJvXAd89WvEnXnSL8zCMdXjJMWOecwE4tcX5cQVZzlmTkpmbkn+sHU/sV4r/OGYrcJHGSgxLaZK/APKvyJeaKIs1c+MbKn835gBXT8uZrHN7qt7Wi0edb+dfLd7+l9wuU2750+YtRc7Ba3UKiq3vacZpwOhQT+XNE61LlizT/X7Vse9LYO1jcgBZkzhma2p6cOMYdGacUa8dRB7E4MYDtBoJIX7WouJShk4cL6QJACaHYaHz+4Cc4mGNI99XIntTYr+0A8AP/wGyL9i3lQxsANfMze6Fzidvl2Gpi/Y2Z52Tf865gGqzBjeG4OLnrcAQUt7lsv+LtwU3IfZjfj9VrkH69OaqtzUjxfl2/hX5P+KD39i35V6UT3p7Piv9OI7ZjupkbhwDC8f322Ly3krH3qi5sS1+WcOGUxwDGiXrujzBcTJBJoMbqpsY3HiIdWjqam4pwwIl1x9xrHm5cND+szHLuWC3IAvYOhfY8ELFTh67FwF//WC/vWeJ6z4Fmc7PDwCfjbD/7DIsVVyMnHEatpNMyRqTqrAGN36hxc9byn/AwgLhmI5K3V/6Ma3ZEFtwkwuk/ul8X1VYgzqrD3oA73cHtrxt35adCiQvlYOp0jgGBCVrbCqTAXA8oZYMvLx1lXdP1dwc+lb+zNdk7mrdaipmbugawODGQ0KLi4qvVjRz4xhcpB10vu/NZnIBsMUCfHkn8PNMYPt7wNz2cmGguajsQlnHIaPT21zvL5m5AeRMjDW4KK2g+MoJh+dQILixtsEvzP3zFtOb8yE5/ueefrj0Y1ozGtaAKf2wc7akqgWVWeddt109VWKfVODsTtf9/OrZf3bK3JQY1vn2YfuwX3kc66BK/jfureDG+lp8AuXv1cnc5F0B9i+Xj7lyvPyZT/kDThmbmrSOjmMgrsQSBp7kWHDOzA3VUQxuPKRecebG7Vo3QOmZm6QZQPKXrvuv+y+w5hHnk2VBhlwYuP7pyhUGxt3ofLvkbCmrFffKJyhrkBHcUP5uHZZyPPHmpld/+MOaubHWx5Sckl5Mby6x3d2qyZnn5PoM6+uyZm7O73MuKrbWylSWu+CmpOxU98GTIdD+c1nDUgBwdF3F2uNYB5Vx2vk+b60UbAski4O3qq5zYzYBX9whB3cb59i3X/7bOeDNTa/a8T3B8bNqygV+nw980FP+HNY0TpmblNL3I6rFGNx4iHWtmysVDW5MBfLJbds8+7bGvZz3+XO5/P3mmcBzqcCQN+Tbe7+o3DWf7lwIDJ4NW6Wxu8wNAPzzC/D7B/b/RENj5e95bjI3wlK5dV9KKswFjv0k/2zL3Lj/D1hvLrHdXXDzxe1yMbZ1yMoaMJX0RlzVLilRcljK1rgAYMS78s/ZqaWfPKzvf1nDUoBz2/KuuK6NY+XY9yU/CyY3gasnWF+Lf/H7V9XMze7F8krYALDlLfv2rPNAjkNAk1ODghvHz2raQeCnZ+XicsdZiTVFATM3VPcxuPEQ+yrFlRiWclzgrcPdQOPr3T+2aV951tX1DwI+QfJMp2Pry2mRBPR8BBj/IxAYAcRPBp4t/sNmKZLXvgGAYe8AY7+zP+zsbnvNTVhT+XvqfuDYBtdsUXVmTG2dK2eiAKBR8esuzC1eYNA5QLRlbgIj5e8Zp11PpJeOOd+2Zm7c2fmx/efcyxWrFSktcxMaCwTFyD9np9pPHtGdnPezXhbBKXNjse973U3yz9aT/NF1wNutgG8fcn3OKyeAE7/Zb18sMT3eWzUg1syTNXNTmZqbP78GThUPmZZ25fnLx0sEN5UoYk/9E9j2nmeKqy0W5+zYhuftPzuuM5SVCnx0I7DrU+XbUBlOw1Lnase6PESVxODGQ+oXXxk8LbMA2QUm/JVW4g99yWv+FBXYg4XwFsCdnzjXZjiyXp9KowGiitfMOfCN+32tQmOBIXOApg7r4PgEuq7/EtcPaNYfmPiLfDs12R7cNOwG+IfLP2943mFYqjgD9N1jwMLBwM5Pym6LO2eKh9v6Pgm0GiL/nJMGzG0HfDbc/gfYlIeYjOI1Y8LigMAo+Wfr6sulnbzKugxB2gH55JqyA/i/LsDCQfYMiTEHOP5ziam+BfbsVf/nSjyPDgiOth/Xmt26+wv7PgKA3q/4NabLAY4xR64rAeT399b/s7+uzHPAsn/Lw3YHVzkHe1nngY/624cK3XE35OgJ1mEp/+LPrSm3YsHE+WRg9YPytH6L2Z550vk673fxqPNQVGUyNx/1BZJecA5klVJW8FiYC/z4hPy1/f/kfwx+fEL5NlSGY3BjMdlnZhLVIQxuPKRVZBAA4EhqFqYs24db5m3BHyccLnkQ0cb5AaZ8+4mwXjP5e8+HgfZ3Anc4/kGW7MWxgP1aVeWtZhsY4bpNkgBDkPM2nRyUIbIdIGnlk+blv+VtAQ2AB4qHji4dB64UT8Fu0lv+fuEAcGYHkPhk5WtZrFPPWw3B+TyHmVC5F4Ezf9gyE9ofpiLuUnHg5RcqtxOQh29Obwdei3E/s8YaBLpzbo98cl00WJ45dvEv+wyz9U8DS+8Etr9r39+aodL5Ajc+CTz4q/0+Iey1SY4ct2n1cgAJyMd+8zpgdkOHdYm08v5B0fL7Oj/e+ViOs8M2/K/89WRMeXIGbPObcvbCU6xZKMegvCJ1N46XBUndbw9u2oxw3i812bnouqKZG8eCauuik0oqK7g5v0/O1Oz61P77UrJN3lYyo8YZU1QHMbjxkLYx8lotx9NzsPGo/F/15zscCj37PQ30eNg+hFFktGdC6sXJ330CgLsWAZ1Glf5EJa9V1eU+QGtw3S/ATXADAJElHm/9b1nvBzRoLf9snQXkEwDUu654n+JCWa0B6DXZ9bhXT7tuK03+Vft/jw1aYcTH+1z3+f19QAhoDq+xb/MNBSLbyj9fOARsfkvOUuxe5Pr4wChg2kF7nVLzQcCjZayqbP0Pf19xcbc1q7Jxjv0yFuEt5OHFht3s/dvyFjlzcev7zsfT6oChb8k1Obd9AHQeI28vyHRedwSQ63EkSd5f0roGLwtvlocLLRbgaBnDkbri7FDuZWDJcODXV+TshWOtlJKsgYePv/0zWJG6m0sOayr9uUIeZtUa5PeoLBUdBrVO/QdKr5WqjrLWtTnr8BlzrJ8qa4afp5UMOFl3Q3UQgxsPiQgyIDzAeSjk4Dn5JGW2CLy7JRVbW/wXuG6AfGeRm8xNRVizJlbD3gGePeu6X2AD949vmeB8W+cQGDkOYQFycKPROLcvrAnQws1JqDKzMKyXUAhuhAKNP66a3Awh/fOr88wZoDhzU5yR+etHuQC6tOfW6OTrevV4SK47umuhvYbIndPbgH9+c9528RiwcbZ9mKfTv+33TfgJGPyanMkBgK73AU8cA2LjgUGz5G09HpTrnJrEy+97kz5ydqYka5DQZrgcLFk5BrKb35Q/L6XMKANgD5JXT3SeZWdbCK+ChJAza+UNMVnbrdEBQcX1UBWZ7eV4CY0/PpS/128BRLS2b28zAghtIv9sHUo9uq5iwZPjStRpB4truQrtM9ksFjl7sfdz94X15XGXubEOKzpyDCJKLvdQlkvHgeSvlKsXKtlnnDFFdRCDGw+RJAkxoX5O205fzsM/F3Pw/f7zmPvzMdy78A9YrP/hmgrs6fmwONcD3viU/H3Euzh4LhOJB1IhhADCr3PeT+cjf03+A2gx2L69tMyN4z6Ac3DT+zHHVyRnKgDn56zXTH7Mv5cBfaYBbW6Vtx/5Hvh6rPy9PGnF/1lHtMapy7mwQIMc4VBvYZ01tqlEcGMIAiKKMzfZ5UzN1hR/1CVJDtp8Q+R+snLMYFkzBl/c7nyMDxwKvLUGoNM99tv1mgHxj9praQD5BP/AeqDP4w7t0Nq/358IPPEXcMN05+dxXC/HcWjmtg+Axj3ln1N+txcbN+wGPPgb0HOSw/PogeAY+22fQPm6ZoAc3Bhz7LOoci4Cf//iOhNLCITknYJm3RPyIoWf3SqvNeM4vd1ikWuSci46BDd6oHfxwoW/vCwHhWVxl8Vo0NpeWwYAsb2BqcnAUyeA/6XLGcS8y8C2d+X2pP4J7FsKJC+TA92CLPn1CeEczAkz8HF/eSjwzebAB72AWWHAvPbA2ilA0otlt9WYIw9t5ToMMbvL3JRcbqGkylz6Y81keRmIJcPlYa6qrs105aS8iKd1UVBrYM3MDdVBuvJ3oaoa2iEaB85l4p4esTh7NQ9bjl9C4p+puJhj/+8wPV9CFCAHAVf+kU+asT1djiX6PQOp4yiYw67D/XN+xcVsI964qyPu7t4YGLUUWDEG6D7B/oCI1kDXscDx4hoZdzU3ANCglZzBsJ5QHQtvQ2PlYZwDK4FBL9uzP471I037yt9bD5W/rMM3B76Wv6f+aT9B51+VZ4Q5ZiByLwGbioeKmvTB3+ly8fK35htwn674SuMj3pWLTfMcTiiAPMMrsp3cTut/n1of+3o59VsBl45CSBp8edAIkXIKY+ObOh/jhulyAfRdi4pnvEjySfqfX0uvY4ruJGdpAsLd319ZNz4lnyCt04YvO0zlbjMc2BgrD8FFdQTGJwKvN5GHsv4s7uOoDkDDrvIwjTXz4R/unB3491fycOKxdcDh7+QvjQ4IaSwvwFiULweKt8yWL9+Quh/as7vQ3zHjc3orsCgBaNAG6DZeztj98ZHc7uCG9qUCNDqg+wPyysKntwFf3wc88rscWBYVOAeAV07a1+UZ/bX8Obz8D3D9RDlTaBXQQA5QrX1+8ww5eN78pvxVmpBYIDMFgCQ/5peXnWfSOS5+CAC7FgK9pwAhjdwf77tHgcNr5NfY9nY5MDjzh+t+1kL30iR/JWf+SmZeSyrMtWfdUrbLgVlEO6BxD7nQ//R2uYaryCj3f2CkvN2YDdRvKX9W6zWTa9cS/+u8xlNE2+LlChQObjLPAb+8JAdwIY3lz6LZKP+d0foAFw4DTfsA7UbKBeLBDeXsl0bnMExfIP89DG/u/HkhqiAGNx700I3NkNAuEs3qB+CbPWex5fglfLr1JDQO9bK/FnXAaABILx6PbzEIPx7Lw9I/jsAiBO7r1RRajYQnvk7GayM7IDo7Axez5eDo5e8Po/d14WjUZjgwZa9rIavD9GejIRzCZIZBp8GJS7loVj8AkiTJJ5wxq+QZQiGNYLvip1XPh+UvRw5XNf/eMARz396I9+/pKtcZWU9wVldPyiewsKbA0n/JNQg9HwH6TJUzC7sXySeYBm2AXpPxzyb5D+2bRaPQz+cvxEaGy3+kez0C/PoKRHhzSNYCZx9/OQMy9C3gq7vl527Uw359p4bdgHHf49Spv/HC0qsAruLlHw5jUr/r8ERCK3mfm2cAA190fd13fAh8/7h8jDsXytfkWlUcPI54D4jp7PJ+V5mPPzD0DXtwk38Vfeb8ii8m9ECzBkFybZC1Dkerk7M3//xiD1ytReX1W9qPGdBAPgHuXihnvpr1k4c1mt8M/F0cNFqKnKfzpx8GPr/NdtMprduwm5xNObgauHhELrRe/7T9/qxz9noWrU5+X/71mXyBy4t/yZegOJ4kn8w6/lsO2tIOyjU2gJzpaOmcRcwqMOHywI8Rl7MPaHeHc5+1uVU+OR5abd9Wv5UcxGeetb+uzOKgt/1I4Ib/yH2x9zNA7y/XaxlC5AA547QcJJ3bI9duDXkDOJooD8flXZaXKSjIkoNDa985XkfMJwjoMkauq7nxKUDvKwek1uUNHFnfg8VDgKAYaJv2Rc/Tf0H7faIc8ER1tAdx5x3qzwIi5LakH7L/vXBUsgbp7C758h+liWwrf47+TgI+HgAERcl90fIWOTtrLn6NV07K72ebEc7BmBBykG0Ilv9x2fia3F7HrJT1d7WkY+vkYngXkhwAWUzy2lnRnYC7FgMB9eXsoFYnB3IhjeR/YsxFcvBkzcxaLPJQrSFIbp8QcuBkzAJ8QkvvC6pzJCGqmuOsnbKyshASEoLMzEwEBwcrdlyTyYTExEQMHToUer3e5f7MfBN6vPozjEXOa0qE+uvxR98/Ydj0CgCBE4MW4abv7UMyQb46BPjokJZVgK6xoRAA9qVk2O5vFxOM7x+7ARqNhHMZ+YgJ8ZWDFkDOmnwkZ1bus8zEHrRBi8gg7D+Tgf/c3BJTBzbHxqMXYTJbkNA8AND5Qmh02HnyCtrGBCPI1/V1AJD/uGydC9F6GOLmySePttHBSHy8L7b/tAK9fy+xFkurofLJafWDDhsl+b//PYvlm7fNB7qMwZRl+/D9fnmIyU8ncPClodBqNfJMnMPfwRR7A/aumY/rdUehuf0D+Y8eAJzcIv/BS/ldTuED8tBIwsuYv/FvvLHe+Srex18dAr22nFFZIZyDnlNb5f9Kyyrwro4j3wPfTMDU/IlYa+mDhLaR+Hhsd9f9/vza3pe+IXJWJKSh/b7fXpOHqNrdLq9/1HGU/b9fc5G8AnZoEzkzkvyVfDJr2A34brI8HT6iDRDdGWa/cGxN9UGf3r2ga9xdzhDkZ8iZvH1fyCd+vR/Qeph8Ajy9DcaAGHzc6DXccctgNArzl6c9l7euiyEEeHijS63ZuEU7senYRXwxoQf6tnBTM1ZUKJ/AA+rLQ6PWGioh5CDDx1/+HUj7Uy56t35WSnN6uxxwAHLwU9osqLgb5SHYU1vkIPK6m+ShZH2Jqeuf3Qqc3CQHPo4Zk/+elPvl0LdwfwFQSc6KRXUAtrwj79P2NuDuz+UhpX9+lQPR9L/kjF2j7nKhesZp+X5DkFyXdHyDHHxcOSG/d3H9nBcVvGcFsGqic9vK4hMEDH/HPhPyz6+Bc7vloveSWc6gGGDI63JWMDtVfi/yrsiZrtQ/5UBYCPnzm39FLn43FzofR9JU7PpvARFyUFtklIPngkz5+U258qy04kyu0Bpw1dAIQf/+GPpYN79XSjixSZ7xZzbK7SnMK16p3Ax0Gi3PksxOlftF7yd/zjRaeVkDva889F+YU5yp1Jb9XEVG+e+iT4D975TFLAdyvqGu/7B5ihByptAn0BZklnc+rKrKnL9VD27mz5+PN998E6mpqWjXrh3mzZuHvn37ut139erVWLBgAZKTk2E0GtGuXTvMnDkTgwcPdru/O2oFNwCw6dhFzE06hgPnMnFD8/o4czUPJy7m4p4ejfHygHrQ5l/C3WvzsOvUVXRvEobdp0ufTj1jRFvMTTqGrIIifHxfN+SbzHh8eTIm9bsOzwyxznI6Dbwr/1c/wPg2Tgrn4tXYev5IuSL/AV/zaB90bhyKhVtP4uUfDmNwu0h8dJ/9D4AQAh9tPgG9VoMJN8g1QYfOZ2LYe1tt+zzS/zqs27QVGw3F63jEPybPcnLkV08uBHacsSNpgadPwqgLRJ85v+JSjn0dl/XT+qJ1lP19svbzoMG3oNAi2VaCdmiofPHKvZ8DIz8BYnviXx9ux65Tzn256pF4dGtin7K8+dhFXMw2YmTXhvbgUGGZ+SYE++pwODULBp0WzSMCXfa5kpWLrq9tBAD0aFoPX0+Kd9kHgFxfcmg1cNMLtixSgckMnUaCrrygrYIq/QcqPwO3fXoA+8/Js3G2PXMTGhb8Iw+lSJKcOYnqCGx+Q86uNOsvZ+w6j7YHZ8WyCkzoOHMDAGBAqwZYfH+Par2W1Mx8HDqXhYFtIsp+f1dNlIM3QB7iaZEgn2j8QuVg4uxO4PYFclasPAWZcgDYoLWcwfl2kny8G6bJ9xuzgdPbYT6zGyeOHsJ1ulRoMk67Dr8C8ori8W5mJVbW2iny70ZYHDBpqzz8c2qLHEhkp8nDiCc22TNOUR3k4O3vX+WlHsrSoLU8pJd3Rc7yWAva3Skyyr/3Wp18gtbq5Xom60V89f5yRvf7acWZIFGc0TEXD8NVcu0mh0BJ+IZCuul/8nMERcq1R1ofOUAuyJKPbzHLQ6QFGfKSA1of+Sugvvy+ZZyR98u7Ig/p+oXJQZt1UkN1+YYAwY3kYMFilv+Wa3Xy38/ghnIgm3lW7hdIcmAhiheTFBa5b0sGR0FR8lBkYa78T0pQlBz8hTQszsJlFNdzCTkzWVQoB2kWsxx86Qxy4CVp5Sy51kcOpLJTi9fZKl6mxK8eLH5hSM2VEPHoOtWCG1WHpVasWIFp06Zh/vz56NOnDz766CMMGTIEhw8fRmxsrMv+mzdvxqBBg/Daa68hNDQUixcvxogRI/DHH3+gS5cuKryCyunXsgH6tWyAIrMFOq0G6w+mYtKXe7Fs5xlcyDKib4v62HXqHHx0Gvzf6C54Z8MxrNzjOvPpqcGtMC6+KdKzjViw8R+8+8txnL0qr5vx4aZ/cFvnGGw+dhGnz53Ha8WPuSyCXI5jDWwA4Pv953E1txAv/yAXd/506AKSDl/AzcUng70pGZizTp7V8snmE3hqcCscOOc8RXnBxn8AROM50wTc1K0dbu57h/zhP7lFHiZo2lf+71OrB1Y/JNfJ5FwAOo/B31la3LdwIy7lFCIq2BetooKw6dhFrDuQ5hTcZOab8Ee6hHff345zGQVYOSkeHRuF2hshSfKMpRufRFaBCTsOpWFPcZA4sHUEfvlLXvht87FLtuDmxMUcTPhsF0xmOc6/s1sp9RYOTlzMwehP/kDfFvXx5OBW2HT0IvwNWuw8eQWP9L8O0SHOdQKvr/8LH276x1YLqpGAZQ/2QtuYYDz/7UF0bhyKB26Iw56z9otBnrnqnDkQQthPzF3GyF8ALBaBGWsPYcWuM2gdHYRPx3bH8l1ncCnHCK1GwoQb4uQsiodlIsAW2ADAez8fx+t3dQQe2S5fT8ta4NxmeLnH2nzMviihY41aVQgh8PAXe/Dn2Uy8/a9OZb+/d3wsZxSNOfJJXedT+r5uWH+3AcgnKGsQ5BssF5A7MgQBLQfDEncTDuckounQodDo9XIB9r7imVsavdyGrmMr1Y5S3fp/wC1z5EyJRiO/L45DftYhaFPxSdJa99R1nBz4SRr5ZChp5RNZ76n2Or2gaPvwUHkcJy5oi09+hkDn664FRwMP/iIHP6Z8ub8sRXIbTHlyfaKwyAGn2STXlBmC5Azuxb/k/fzry++DfzhMF4/BtHAY/AsuyWtxeUrzQXLbtQY5GxMYJf/9++tH+9BhaKycRTXlyZmlwAgg72rxsg+S+yUiTJC3uVxHULhm34QZMJfIpmWkOM+MKy9YLYvbhR+FnNXLvwoNgHr6Uhah9RJVMzc9e/ZE165dsWDBAtu2Nm3a4Pbbb8fs2bMrdIx27dph1KhRePHFcmY5FFMzc+PO2v3nMW35Plgc3oV7ejTG7JEdsfFoOsYv3oU20cGIDDZg49GL6BlXDyselv+bT88qwMB3NiG7oJTrDQGYrF0DAQ0WmG9FfLNw/F68kOCdXRuhabg/6gcZ8Ozq0j/kN7eJxA3Nw/HJlpM4l+F+4bEpNzXHwXOZ2HjsotNEjqcGt0KQrw5BBg1yMq/iWKYWHRqFIMzfBz46DYQQ+OdiLkxmC5bvTMGpy/LJ/MmElogO8cMTK+XF6mJCfNGrWTgu5RY6nfQA4LoGAbi5TSSC/fQI8tXBYhGIDvVDgcmMOev+Qmqm/B9efLNwLHuoF1bsSsHTqw5Ap5HQv1UEzlzJw9ELzn8YBrRqgI6NQuWhOYMOkAAJEoQQOJyahfxCM5bvOlNqfwDyIo7DO0ajUT0/fPVHikvmCAAaBBkQ4KO1ve7+rRrgzJU8/HPRPvvmjTs7IizABxuPpmPNvnPo0CgE43vHoV6ADzQSsHDrSaw7WPYKs6H+etzeuSGub1oPIX56GPQaSJBn9GkkQCNJ0EgSCorMyDEWIdhXD1+9BkVFRdi2dStuuKEv9HqdLcstFa9Ibb1tfc93nbqC/62xFzFrNRJeHN4WTcL9YdBp4aPTwKDTOGXLhQCy8k34Zu9ZNAg04Oa2kTifkY//+/VvW3E5ADQK80Pv68JxfdN6OHs1H6H+ejQND4CxyIxgPz2CffUu7bEIAYsATl/OxePLk23H+uyBHgg06HA+Ix9ajYQcYxH2pVxFx0ahaFY/ACazgL9BCx+tBhpJgmTrI+trlhzaLz9HZr4Jn/1+CusPpqFtdDA6Nw5Fy6ggNA33R4HJggKTGVEhvigsssAiBHy0Gvjo5C/JYsGWLZvR98YbodfpSowmOGeZHO+TnLY772cRAkIICAFYBCAgYLEASYcv4I+TlxEV4ovD57OQXVCEIe2jMLxTDAINWmgcjmMRwIFzGcgxmtEmKghhARUL9Nzlxdxly9zvV6GnqBTr59VUZMK+n77CYN0u6IxXASGgzb0AbW4qJGGBZMqDpfiK9pKlCKbQOAh9IIqCGkGymCCZjdDmpMFiCII5OBawmGHxC4PQ+kJTmAWpMBdF9Vogu8vD7l+IENBlnIDQ+cIc5GahT7MJmsIsWAwh8EnbA40xG5AkCEkLc7AckGtzUqHNPo+i0KYoCr0OQu8PqTAHmsJsCI0OQucHYQiCxmURVQt0mSnQXzkGi08QhN6/+LUEQ5sjlwFY/OrLzwcJ0OggtD5yrZ+5EObgxoDFBKEPBEQRdBmnAEgQPoEQfqEwhbWAVJQPTcFVaAquALmXcOjAAdwwbsa1NyxVWFgIf39/rFy5EnfcYf/P4fHHH0dycjI2bdpU7jEsFguaNm2K//73v3jsscfc7mM0GmE02v/zy8rKQuPGjXHp0iXFg5ukpCQMGjSo0m/mFztS8P7Gf1BYJKDVAKsm9UKTevJ/2jtOXEGLyEAUmMz4Zs85jItvglB/+/F3n76KiV/sRa7RjCb1/BEe6IO9DjU5AKDXShjQqgFeH9ken2w5hds7RyOuvvwfmdFkRr+3t+ByrjwUNLZXLG5u0wBz1h/D4dTyx+KvbxqGpQ90hyRJOH4hB0fSsvHf1QdhtlT+Y6XXSpg64Drc36cpjCYzBs7dioz8Uq7NVUnz7u6IYR2iYDJb8N9VB/HDAeeAwFevQWyYP445nFA95dH+zbD0jzOlvjaNBFSh+9CxUTAOnsuCRcjHGNmlIX47etH23nrLPdc3Qn6hGWv2V+NaYwAigw3IyDO51KkRUc0XrBfY8exNigc39evXr9nBzfnz59GwYUNs27YNvXvbK/Bfe+01fPbZZzh69GgZj5a9+eabmDNnDo4cOYKICPdTnWfOnImXXnrJZftXX30Ff3/Pp+orw+IwZFEZeUXAXxkSmgcLBPvIt3XFJ8gCMxDiU/Z/RJmFwJEMCZF+AnEOo1cpOcCOdA1yixNDrUIEekcKFJiBpLNy+rlPlAX1SiyIfDIbOHRVg6xCwGgG8s1yW0INQIYRKLRIKLLIo8X1DAL+OsAsgJtiLGjkMPs3wwhkm4BLBRKOZEjQaQCTBRgQbUG0P5CSK7/u/CIJeUXy80gArhjl/9ViAgTqGQTyiySMaGKB1uE/+6OZ8jEbBQg0DxYI0AE+WuBcLvBPloQzuRLO58ntBOxln4E6udA5xyShc7gFXcIF/rwiIdQAnMiSMLChBUUW4FimhH2XJWQWyu/LgBgLQn2ASwVAhB+Qng/8fkGD9AKgoT+QXQSYLUCWCegXLeCjEdh2Qe7DvCIJDXwF2oUJnM6RcCJbbpdFAAYt4KsFGvgJ/CvOgvN5wL5LGnQMtyAuCCgoAg5clfBPloS0fAkFRXJfW4r7QTh810rysfLNgGM8IRxev/WHkn80rAkNPy0wroUZMQHA+jMaHM+SUGgGioR8zCLh+mCNBPjr5M0FZiDUB2gYIHBTjAV5RfL7EeoD7L8iIasQCDMA+UXAJaMEg0Zub35RyUwGirNT8nc/HdAvyoI/r0i4WCCh0CK/l1qN/Ngwg0CBWUKOSf7dMVrg9N7b+srN65YkeWZZ82CB2ECBY5mS7bN+ufiz6K+Tf8/0Gnl/s8XeJ+ZS/gK7bBZl3FdiN8fXbn1vNMXt6FhPwCwAX61AmAHYmqZBRiFQaHY9brAPEOojcCFffh/LU167ytzXA2ei2jRbpja1tSKC9cCLXZW9UG1eXh5Gjx5dO4Kb7du3Iz7eXjT56quv4osvvsBff/1VxqOBZcuWYeLEifjuu+9w8803l7pfbcjcUMWxn72Hfe0d7GfvYD97j6f6ujKZG9UKiuvXrw+tVou0NOfhgfT0dERGRpb52BUrVmDChAlYuXJlmYENABgMBhgMrtda0uv1HvmAe+q45Iz97D3sa+9gP3sH+9l7lO7ryhxLtcsv+Pj4oFu3bkhKSnLanpSU5DRMVdKyZcswfvx4fPXVVxg2bJinm0lERES1jKpTwadPn4777rsP3bt3R3x8PD7++GOkpKRg0iT5GjnPPvsszp07h88//xyAHNiMHTsW7777Lnr16mXL+vj5+SEkJKTU5yEiIqJrh6rBzahRo3D58mXMmjULqampaN++PRITE9GkSRMAQGpqKlJS7PPyP/roIxQVFeHRRx/Fo48+ats+btw4LFmyxNvNJyIiohpI9WtLTZ48GZMnu195s2TAsnHjRs83iIiIiGo11WpuiIiIiDyBwQ0RERHVKQxuiIiIqE5hcENERER1CoMbIiIiqlMY3BAREVGdwuCGiIiI6hQGN0RERFSnMLghIiKiOkX1FYq9TQgBQL50upJMJhPy8vKQlZXFK856EPvZe9jX3sF+9g72s/d4qq+t523rebws11xwk52dDQBo3Lixyi0hIiKiysrOzi73YtmSqEgIVIdYLBacP38eQUFBkCRJseNmZWWhcePGOHPmDIKDgxU7LjljP3sP+9o72M/ewX72Hk/1tRAC2dnZiImJgUZTdlXNNZe50Wg0aNSokceOHxwczF8cL2A/ew/72jvYz97BfvYeT/R1eRkbKxYUExERUZ3C4IaIiIjqFAY3CjEYDJgxYwYMBoPaTanT2M/ew772Dvazd7Cfvacm9PU1V1BMREREdRszN0RERFSnMLghIiKiOoXBDREREdUpDG6IiIioTmFwo4D58+cjLi4Ovr6+6NatG7Zs2aJ2k2qdzZs3Y8SIEYiJiYEkSVizZo3T/UIIzJw5EzExMfDz80P//v1x6NAhp32MRiOmTJmC+vXrIyAgALfeeivOnj3rxVdRs82ePRvXX389goKCEBERgdtvvx1Hjx512of9rIwFCxagY8eOtkXM4uPjsW7dOtv97GfPmD17NiRJwrRp02zb2NfKmDlzJiRJcvqKioqy3V/j+llQtSxfvlzo9XrxySefiMOHD4vHH39cBAQEiNOnT6vdtFolMTFRPP/882LVqlUCgPj222+d7p8zZ44ICgoSq1atEgcOHBCjRo0S0dHRIisry7bPpEmTRMOGDUVSUpLYu3evGDBggOjUqZMoKiry8qupmQYPHiwWL14sDh48KJKTk8WwYcNEbGysyMnJse3DflbG2rVrxY8//iiOHj0qjh49Kp577jmh1+vFwYMHhRDsZ0/YuXOnaNq0qejYsaN4/PHHbdvZ18qYMWOGaNeunUhNTbV9paen2+6vaf3M4KaaevToISZNmuS0rXXr1uKZZ55RqUW1X8ngxmKxiKioKDFnzhzbtoKCAhESEiI+/PBDIYQQGRkZQq/Xi+XLl9v2OXfunNBoNGL9+vVea3ttkp6eLgCITZs2CSHYz54WFhYmPv30U/azB2RnZ4sWLVqIpKQk0a9fP1tww75WzowZM0SnTp3c3lcT+5nDUtVQWFiIPXv2ICEhwWl7QkICtm/frlKr6p6TJ08iLS3NqZ8NBgP69etn6+c9e/bAZDI57RMTE4P27dvzvShFZmYmAKBevXoA2M+eYjabsXz5cuTm5iI+Pp797AGPPvoohg0bhptvvtlpO/taWcePH0dMTAzi4uLw73//GydOnABQM/v5mrtwppIuXboEs9mMyMhIp+2RkZFIS0tTqVV1j7Uv3fXz6dOnbfv4+PggLCzMZR++F66EEJg+fTpuuOEGtG/fHgD7WWkHDhxAfHw8CgoKEBgYiG+//RZt27a1/SFnPytj+fLl2Lt3L3bt2uVyHz/TyunZsyc+//xztGzZEhcuXMArr7yC3r1749ChQzWynxncKECSJKfbQgiXbVR9VelnvhfuPfbYY/jzzz+xdetWl/vYz8po1aoVkpOTkZGRgVWrVmHcuHHYtGmT7X72c/WdOXMGjz/+ODZs2ABfX99S92NfV9+QIUNsP3fo0AHx8fG47rrr8Nlnn6FXr14AalY/c1iqGurXrw+tVusSdaanp7tEsFR11or8svo5KioKhYWFuHr1aqn7kGzKlClYu3YtfvvtNzRq1Mi2nf2sLB8fHzRv3hzdu3fH7Nmz0alTJ7z77rvsZwXt2bMH6enp6NatG3Q6HXQ6HTZt2oT33nsPOp3O1lfsa+UFBASgQ4cOOH78eI38TDO4qQYfHx9069YNSUlJTtuTkpLQu3dvlVpV98TFxSEqKsqpnwsLC7Fp0yZbP3fr1g16vd5pn9TUVBw8eJDvRTEhBB577DGsXr0av/76K+Li4pzuZz97lhACRqOR/ayggQMH4sCBA0hOTrZ9de/eHWPGjEFycjKaNWvGvvYQo9GII0eOIDo6umZ+phUvUb7GWKeCL1y4UBw+fFhMmzZNBAQEiFOnTqndtFolOztb7Nu3T+zbt08AEO+8847Yt2+fbUr9nDlzREhIiFi9erU4cOCAuOeee9xOM2zUqJH4+eefxd69e8VNN93E6ZwOHnnkERESEiI2btzoNJ0zLy/Ptg/7WRnPPvus2Lx5szh58qT4888/xXPPPSc0Go3YsGGDEIL97EmOs6WEYF8r5YknnhAbN24UJ06cEDt27BDDhw8XQUFBtnNdTetnBjcK+OCDD0STJk2Ej4+P6Nq1q21qLVXcb7/9JgC4fI0bN04IIU81nDFjhoiKihIGg0HceOON4sCBA07HyM/PF4899pioV6+e8PPzE8OHDxcpKSkqvJqayV3/AhCLFy+27cN+VsYDDzxg+5vQoEEDMXDgQFtgIwT72ZNKBjfsa2VY163R6/UiJiZGjBw5Uhw6dMh2f03rZ0kIIZTPBxERERGpgzU3REREVKcwuCEiIqI6hcENERER1SkMboiIiKhOYXBDREREdQqDGyIiIqpTGNwQERFRncLghogI8kX/1qxZo3YziEgBDG6ISHXjx4+HJEkuX7fccovaTSOiWkindgOIiADglltuweLFi522GQwGlVpDRLUZMzdEVCMYDAZERUU5fYWFhQGQh4wWLFiAIUOGwM/PD3FxcVi5cqXT4w8cOICbbroJfn5+CA8Px0MPPYScnBynfRYtWoR27drBYDAgOjoajz32mNP9ly5dwh133AF/f3+0aNECa9eu9eyLJiKPYHBDRLXCCy+8gDvvvBP79+/Hvffei3vuuQdHjhwBAOTl5eGWW25BWFgYdu3ahZUrV+Lnn392Cl4WLFiARx99FA899BAOHDiAtWvXonnz5k7P8dJLL+Huu+/Gn3/+iaFDh2LMmDG4cuWKV18nESnAI5fjJCKqhHHjxgmtVisCAgKcvmbNmiWEkK9oPmnSJKfH9OzZUzzyyCNCCCE+/vhjERYWJnJycmz3//jjj0Kj0Yi0tDQhhBAxMTHi+eefL7UNAMT//vc/2+2cnBwhSZJYt26dYq+TiLyDNTdEVCMMGDAACxYscNpWr14928/x8fFO98XHxyM5ORkAcOTIEXTq1AkBAQG2+/v06QOLxYKjR49CkiScP38eAwcOLLMNHTt2tP0cEBCAoKAgpKenV/UlEZFKGNwQUY0QEBDgMkxUHkmSAABCCNvP7vbx8/Or0PH0er3LYy0WS6XaRETqY80NEdUKO3bscLndunVrAEDbtm2RnJyM3Nxc2/3btm2DRqNBy5YtERQUhKZNm+KXX37xapuJSB3M3BBRjWA0GpGWlua0TafToX79+gCAlStXonv37rjhhhuwdOlS7Ny5EwsXLgQAjBkzBjNmzMC4ceMwc+ZMXLx4EVOmTMF9992HyMhIAMDMmTMxadIkREREYMiQIcjOzsa2bdswZcoU775QIvI4BjdEVCOsX78e0dHRTttatWqFv/76C4A8k2n58uWYPHkyoqKisHTpUrRt2xYA4O/vj59++gmPP/44rr/+evj7++POO+/EO++8YzvWuHHjUFBQgLlz5+LJJ59E/fr1cdddd3nvBRKR10hCCKF2I4iIyiJJEr799lvcfvvtajeFiGoB1twQERFRncLghoiIiOoU1twQUY3H0XMiqgxmboiIiKhOYXBDREREdQqDGyIiIqpTGNwQERFRncLghoiIiOoUBjdERERUpzC4ISIiojqFwQ0RERHVKQxuiIiIqE75fwFeIMDlnu85AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1FElEQVR4nO3dd3xTVf8H8E+SZnTTUrqgQNl7FWRvqbKHyFAZAgqiIOJ4BAeI/B6cgANwUXAgIDLEhyFFpoLsDbKhQAulrLaUtmlyfn/cJk2adMFNbls+79errzY3N/eenKQ533zPuCohhAARERFRKaFWugBEREREcmJwQ0RERKUKgxsiIiIqVRjcEBERUanC4IaIiIhKFQY3REREVKowuCEiIqJShcENERERlSoMboiIiKhUYXBDpYpKpSrUz5YtWx7oPFOnToVKpbqvx27ZskWWMhQ3ffv2haenJ27fvp3nPk8//TS0Wi2uXbtW6OOqVCpMnTrVerso9Td8+HBUrly50OeyNXfuXCxcuNBh+4ULF6BSqZze504TJ06ESqVCjx49FC0HUXHkoXQBiOS0c+dOu9vvv/8+Nm/ejE2bNtltr1OnzgOdZ9SoUXj88cfv67FNmjTBzp07H7gMxc3IkSOxatUq/Pzzzxg7dqzD/Xfu3MHKlSvRo0cPhISE3Pd53FV/c+fORVBQEIYPH263PSwsDDt37kTVqlVdev78GI1G/PTTTwCA9evX48qVKyhfvrxi5SEqbhjcUKnSokULu9vlypWDWq122J5bWloavLy8Cn2eChUqoEKFCvdVRj8/vwLLUxJ17doV4eHhiImJcRrcLF68GPfu3cPIkSMf6DxK159er1f89fvtt99w/fp1dO/eHWvWrMH333+PyZMnK1qmvBT1f4tIDuyWoodOhw4dUK9ePWzbtg2tWrWCl5cXRowYAQBYunQpoqOjERYWBk9PT9SuXRtvvvkm7t69a3cMZ91SlStXRo8ePbB+/Xo0adIEnp6eqFWrFmJiYuz2c9atMnz4cPj4+ODMmTPo1q0bfHx8EBERgVdffRUZGRl2j798+TL69+8PX19flClTBk8//TT27NlTYFfJoUOHoFKpMH/+fIf71q1bB5VKhdWrVwMArl+/jueffx4RERHQ6/UoV64cWrdujY0bN+Z5fI1Gg2HDhmHfvn04cuSIw/0LFixAWFgYunbtiuvXr2Ps2LGoU6cOfHx8EBwcjE6dOmH79u15Ht8ir26phQsXombNmtDr9ahduzZ++OEHp49/77330Lx5cwQGBsLPzw9NmjTB/PnzYXsN4cqVK+PYsWPYunWrtSvT0r2VV7fUX3/9hc6dO8PX1xdeXl5o1aoV1qxZ41BGlUqFzZs344UXXkBQUBDKli2Lfv36IT4+vsDnbjF//nzodDosWLAAERERWLBgAZxdA/nff//F4MGDERISAr1ej4oVK2Lo0KF276krV65YX2udTofw8HD079/f2nVoKfOFCxfsju3sdZDjfwsAdu3ahZ49e6Js2bIwGAyoWrUqJkyYAADYvn07VCoVFi9e7PC4H374ASqVCnv27Cl0XVLpxOCGHkoJCQl45pln8NRTT2Ht2rXWTMPp06fRrVs3zJ8/H+vXr8eECRPwyy+/oGfPnoU67qFDh/Dqq6/ilVdewW+//YYGDRpg5MiR2LZtW4GPNRqN6NWrFzp37ozffvsNI0aMwKxZs/Dhhx9a97l79y46duyIzZs348MPP8Qvv/yCkJAQDBw4sMDjN2zYEI0bN8aCBQsc7lu4cCGCg4PRrVs3AMCQIUOwatUqvPvuu9iwYQO+++47PProo7hx40a+5xgxYgRUKpVDQHf8+HHs3r0bw4YNg0ajwc2bNwEAU6ZMwZo1a7BgwQJUqVIFHTp0uK+xSAsXLsSzzz6L2rVrY/ny5Xj77bfx/vvvO3RHAlJwMnr0aPzyyy9YsWIF+vXrh3HjxuH999+37rNy5UpUqVIFjRs3xs6dO7Fz506sXLkyz/Nv3boVnTp1wp07dzB//nwsXrwYvr6+6NmzJ5YuXeqw/6hRo6DVavHzzz/jo48+wpYtW/DMM88U6rlevnwZGzZsQO/evVGuXDkMGzYMZ86ccXiPHTp0CM2aNcM///yDadOmYd26dZgxYwYyMjKQmZkJQApsmjVrhpUrV2LixIlYt24dZs+eDX9/f9y6datQ5cntQf+3/vjjD7Rt2xZxcXGYOXMm1q1bh7ffftsabLVt2xaNGzfGnDlzHM795ZdfolmzZmjWrNl9lZ1KEUFUig0bNkx4e3vbbWvfvr0AIP788898H2s2m4XRaBRbt24VAMShQ4es902ZMkXk/vepVKmSMBgM4uLFi9Zt9+7dE4GBgWL06NHWbZs3bxYAxObNm+3KCUD88ssvdsfs1q2bqFmzpvX2nDlzBACxbt06u/1Gjx4tAIgFCxbk+5w+//xzAUCcPHnSuu3mzZtCr9eLV1991brNx8dHTJgwId9j5aV9+/YiKChIZGZmWre9+uqrAoA4deqU08dkZWUJo9EoOnfuLPr27Wt3HwAxZcoU6+3c9WcymUR4eLho0qSJMJvN1v0uXLggtFqtqFSpUp5lNZlMwmg0imnTpomyZcvaPb5u3bqiffv2Do85f/68Q123aNFCBAcHi5SUFLvnVK9ePVGhQgXrcRcsWCAAiLFjx9od86OPPhIAREJCQp5ltZg2bZoAINavXy+EEOLcuXNCpVKJIUOG2O3XqVMnUaZMGZGYmJjnsUaMGCG0Wq04fvx4nvtYynz+/Hm77c7ex3L8b1WtWlVUrVpV3Lt3r8AyHThwwLpt9+7dAoD4/vvv8z03PRyYuaGHUkBAADp16uSw/dy5c3jqqacQGhoKjUYDrVaL9u3bAwBOnDhR4HEbNWqEihUrWm8bDAbUqFEDFy9eLPCxKpXK4VtsgwYN7B67detW+Pr6OgxmHjx4cIHHB6TZSnq93q5LZfHixcjIyMCzzz5r3fbII49g4cKFmD59Ov755x8YjcZCHR+QBhYnJSVZu7iysrLw008/oW3btqhevbp1v6+++gpNmjSBwWCAh4cHtFot/vzzz0LVs62TJ08iPj4eTz31lF1XYaVKldCqVSuH/Tdt2oRHH30U/v7+1tf43XffxY0bN5CYmFikcwNSNm3Xrl3o378/fHx8rNs1Gg2GDBmCy5cv4+TJk3aP6dWrl93tBg0aAECB7xMhhLUrqkuXLgCAyMhIdOjQAcuXL0dycjIAaZzL1q1bMWDAAJQrVy7P461btw4dO3ZE7dq1C/+EC/Ag/1unTp3C2bNnMXLkSBgMhjzPMXjwYAQHB9tlb7744guUK1euUFlMKv0Y3NBDKSwszGFbamoq2rZti127dmH69OnYsmUL9uzZgxUrVgAA7t27V+Bxy5Yt67BNr9cX6rFeXl4OH+h6vR7p6enW2zdu3HA606iws48CAwPRq1cv/PDDDzCZTACkLp1HHnkEdevWte63dOlSDBs2DN999x1atmyJwMBADB06FFevXi3wHP3794e/v7+1+2vt2rW4du2a3UDimTNn4oUXXkDz5s2xfPly/PPPP9izZw8ef/zxQtWVLUtXWWhoqMN9ubft3r0b0dHRAIBvv/0Wf//9N/bs2YO33noLQOFe49xu3boFIYTT91R4eLhdGS1yv0/0en2hzr9p0yacP38eTz75JJKTk3H79m3cvn0bAwYMQFpamnUcyq1bt2AymQoc9H79+vX7Hhiflwf537p+/ToAFFgmvV6P0aNH4+eff8bt27dx/fp1/PLLLxg1apS1LunhxtlS9FBytkbNpk2bEB8fjy1btli/UQLId90Wdytbtix2797tsL0wQYfFs88+i2XLliE2NhYVK1bEnj17MG/ePLt9goKCMHv2bMyePRtxcXFYvXo13nzzTSQmJmL9+vX5Ht/T0xODBw/Gt99+i4SEBMTExMDX1xdPPvmkdZ+ffvoJHTp0cDhvSkpKoZ+HhSVQcFYHubctWbIEWq0W//vf/+wCyVWrVhX5vBYBAQFQq9VISEhwuM8ySDgoKOi+j2/LMhh85syZmDlzptP7R48ejcDAQGg0Gly+fDnf45UrV67AfSz1lHtge1JSktP9H+R/y5JlKqhMAPDCCy/ggw8+QExMDNLT05GVlYUxY8YU+Dh6ODBzQ5TN8qGc+5vf119/rURxnGrfvj1SUlKwbt06u+1Lliwp9DGio6NRvnx5LFiwAAsWLIDBYMi3W6tixYp46aWX0KVLF+zfv79Q5xg5ciRMJhM+/vhjrF27FoMGDbKbDqxSqRzq+fDhww7rFBVGzZo1ERYWhsWLF9vNGLp48SJ27Nhht69KpYKHhwc0Go1127179/Djjz86HLewGTdvb280b94cK1assNvfbDbjp59+QoUKFVCjRo0iP6/cbt26hZUrV6J169bYvHmzw49l1tzRo0fh6emJ9u3bY9myZXkGIYA0fX/z5s0O3Wa2LLPEDh8+bLfd0u1YGIX936pRowaqVq2KmJgYh2Aqt7CwMDz55JOYO3cuvvrqK/Ts2dOuS5gebszcEGVr1aoVAgICMGbMGEyZMgVarRaLFi3CoUOHlC6a1bBhwzBr1iw888wzmD59OqpVq4Z169bhjz/+AACo1QV/X9FoNBg6dChmzpwJPz8/9OvXD/7+/tb779y5g44dO+Kpp55CrVq14Ovriz179mD9+vXo169focrZtGlTNGjQALNnz4YQwmFtmx49euD999/HlClT0L59e5w8eRLTpk1DZGQksrKyilAj0nN+//33MWrUKPTt2xfPPfccbt++jalTpzp0S3Xv3h0zZ87EU089heeffx43btzAJ5984rQro379+liyZAmWLl2KKlWqwGAwoH79+k7LMGPGDHTp0gUdO3bEa6+9Bp1Oh7lz5+Lo0aNYvHjxfa9mbWvRokVIT0/H+PHj0aFDB4f7y5Yti0WLFmH+/PmYNWsWZs6ciTZt2qB58+Z48803Ua1aNVy7dg2rV6/G119/DV9fX+ssqnbt2mHy5MmoX78+bt++jfXr12PixImoVasWmjVrhpo1a+K1115DVlYWAgICsHLlSvz111+FLntR/rfmzJmDnj17okWLFnjllVdQsWJFxMXF4Y8//sCiRYvs9n355ZfRvHlzAHA6C5AeYsqOZyZyrbxmS9WtW9fp/jt27BAtW7YUXl5eoly5cmLUqFFi//79DrNj8pot1b17d4djtm/f3m7WTV6zpXKXM6/zxMXFiX79+gkfHx/h6+srnnjiCbF27VoBQPz22295VYWdU6dOCQACgIiNjbW7Lz09XYwZM0Y0aNBA+Pn5CU9PT1GzZk0xZcoUcffu3UIdXwghPvvsMwFA1KlTx+G+jIwM8dprr4ny5csLg8EgmjRpIlatWiWGDRvmMLsJBcyWsvjuu+9E9erVhU6nEzVq1BAxMTFOjxcTEyNq1qwp9Hq9qFKlipgxY4aYP3++w4ygCxcuiOjoaOHr6ysAWI/jbLaUEEJs375ddOrUSXh7ewtPT0/RokUL8fvvv9vtY5nls2fPHrvteT0nW40aNRLBwcEiIyMjz31atGghgoKCrPscP35cPPnkk6Js2bJCp9OJihUriuHDh4v09HTrYy5duiRGjBghQkNDhVarFeHh4WLAgAHi2rVr1n1OnToloqOjhZ+fnyhXrpwYN26cWLNmjdPZUg/6vyWEEDt37hRdu3YV/v7+Qq/Xi6pVq4pXXnnF6XErV64sateunWed0MNJJYSTlZ+IqET573//i7fffhtxcXGyDxAlKq4OHz6Mhg0bYs6cOU5XxaaHF7uliEqYL7/8EgBQq1YtGI1GbNq0CZ9//jmeeeYZBjb0UDh79iwuXryIyZMnIywszOH6X0QMbohKGC8vL8yaNQsXLlxARkYGKlasiP/85z94++23lS4akVu8//77+PHHH1G7dm0sW7aM164iB+yWIiIiolKFU8GJiIioVGFwQ0RERKUKgxsiIiIqVR66AcVmsxnx8fHw9fWVZWEtIiIicj0hBFJSUhAeHl7ggqUPXXATHx+PiIgIpYtBRERE9+HSpUsFLnvx0AU3vr6+AKTK8fPzk+24RqMRGzZsQHR0NLRarWzHJXusZ/dhXbsH69k9WM/u46q6Tk5ORkREhLUdz89DF9xYuqL8/PxkD268vLzg5+fHfxwXYj27D+vaPVjP7sF6dh9X13VhhpRwQDERERGVKgxuiIiIqFRhcENERESlCoMbIiIiKlUY3BAREVGpwuCGiIiIShUGN0RERFSqMLghIiKiUoXBDREREZUqDG6IiIioVFE0uNm2bRt69uyJ8PBwqFQqrFq1qsDHbN26FVFRUTAYDKhSpQq++uor1xeUiIiISgxFg5u7d++iYcOG+PLLLwu1//nz59GtWze0bdsWBw4cwOTJkzF+/HgsX77cxSUlIiKikkLRC2d27doVXbt2LfT+X331FSpWrIjZs2cDAGrXro29e/fik08+wRNPPOGiUrpWakYWvLQapKRnwUuvQUaWGT76or0sqRlZSEk3ItTPACGAlIws+HtqIYRARpYZBq0m38dnZpmRmJKOED8DtJqceFcIgWvJGcgymwEAQT5667FS0o3w0nlAo3a8gFm60YSk1Ay7bToPNQK9dLianO60DAatBkE+eodymYVAhtGMlAwjAECrEtb7jSYzjCYzzAK4nZYJlUqFEF89bt7NRKZJqkd/Ty0yTWboPezr4HZaJrLMwuGcgFSft9MynZbTz1MLtUqFO/eMCPc32F3A7V6mCZ66nPOkpBtx557RoV5t979xNwNh/p5Iy8yCl84DqTavXcKddJiF9HwtdX87LROpGVnWY/gatPDWaZBlFjBoNUhJNyIlPQuhfgaos18bo8mMa3nUe36ysrJwMwO4cvsePDyMhXqMt84DAd46u+dXGOV89cjMMuPOPek8XjoPBHrrIIRAutEMT53G+roE+xqQnG5ElkmgnK8e15Jz6ikvPnoP+Bq0uJqcDpG9bzlfPe5mmJBlNiPY14DElHRkZpkLLKunVgNfgxaJKfZ16ux1zsgywWQWMJmF9b1wO82IjCyTdZ/7qef75WvQwt/T/kKGud9Ttsp4Sa+B5XUpydxZzw+7rKws3C7cv77LlKirgu/cuRPR0dF22x577DHMnz8fRqPR6dVHMzIykJGRU8vJyckApKuWGo3yvcEtxyrKMdcfu4bxSw8BAIQAVCpAp1Hjo3710K1+KK6nZOD15UfxeN0QVC7rhc82ncHb3WqhdqivteFafSgBb/12DOlGM2qG+OD2PSOuJWcgIsATt9KMSDcaEe7viS51QvBGdA2cuJqCmiE+8Mj+EM7MMqPnnB04l5QGb70GlQK9UC/cD6cSU5FwOx3XUnLqzluvQaMKZXAu6S4S7qQj0FuL//aui861gyGEgEqlwq20THT9fAdu3HUMDlQq6XnmpVGEP7rUDsawFhWRaTKj15yduHzbvgFRqwC9WoOvzu/AiauphTpHoLcWK8a0QPkyngCAn3bF4f/WnkSWWSAiwBMeahVC/Q2IrhOMvRdvY8PxazCa8m8sLWWpEeyDjCwzzt9Ig4dahf5R5XHp5j3E3UzDldv3YBaAr8EDA5tWQI1gH/x2KAGTHq+B3w4lYOHOi3bnsZT9hfaRuHIrHasPJ1jv03uoEe5vwIWbaU7rMMBLiwmdq+GD9Sdxz2hGzwah+G+fuoj5+yK++/sCUtKdN14F88B7+7cX6RFzBzfCyWspmLftfKGCBWfUKmBC52r492oK1h27hvL+BiQkZ8BkFnavcUHvKYvC7vcgfA0eqB/uh3tGE7LMAnczshB/Jx3pxpw6yLscRa/n+2XQqhHqZ0CAlxYZWWYcT0hxy3mLB/t69sY9hz2yoEEGdO4slAMVzBBQASj46tfynEv+Thw/rQZP9pA3iCxK+6oSwtX/8oWjUqmwcuVK9OnTJ899atSogeHDh2Py5MnWbTt27EDr1q0RHx+PsLAwh8dMnToV7733nsP2n3/+GV5eXrKUvbAyTcDKC2rczQLahgr8dEaN25mOb141BCY1MiH2ihq7r0tvOo1KwCSkfT01Aq/WN+F2pgpzjquz/wkcqWDGSt0UCKjQL3MqqvkBp5PVqOZnRogncCMdSM1S4fLd/P+BtCqBLAGn5zFoBOoGCOxLsv/nUEHAw2Z3y+PVKgFLbqMirmKx7n38bmqJ97OGWPftFmFChkmFP+Nzjumhks5uFM7L6qESMAvADBVUEFCrYK0vAHiknBkqAHezgGO3VHnWmYUaAppcuwgAWXmcP7/jmB/gA8pDJSBg/1w0KumjyJxre77lsKl3V7GUx18ncCf7fV2Y89q+ppb983qdbetTBSG9p5y8Vs7KZaFViTzrzjYzWFB5bZ9bYV4L27IX5jxyy+/966w8tvtb/v9KC0+kY552Jlqrjznclyk88IlpIBaYusl6Tg1MWKR9H1qYMMg4BcY8cgvBuIX52g+RDh1GGv+DZHjLWg4LHYz4zONz1FZfxPPG13BKVJT1+H464N0mpoJ3LIK0tDQ89dRTuHPnDvz8/PLdt0RlbgDYdQMAsKaYc2+3mDRpEiZOnGi9nZycjIiICERHRxdYOUVhNBoRGxuLLl26WDNIKelZ2HD8GhqU90dkkBeGLtyHPYm3AACHbkqPK+ejw1dPN0b5MgYkpWbi/bX/Ytf5W9iSEoKDt25B+ti0/+C8Z1Jh9fVAxN9Jh0Am+jYKw8lrqdZvYJsmtsHhy8lQp11Ho41nAQAhuIXTyWUBAGeS1TiTbF/+Me0iUS3YB6/9esS6bWz7KnjqkQoI8TNg3dGrGL/0MACgPK5jue/HWKntjg9vtse+JJuGA1lYqP0Q5cpXQWS3CfBYMgBIvwNhCMDl1v+H4KZ9oUm/Cc2yoVBf3gUAGOmxDqLzu5j+x3kAwIk03+zuKzPe6V4Lj9cNQbCv1H30xZ+nMWfrOZiECqF+ekztURu1wnxRvownskxmnEu6i/JlPOGt98CBS7fxw844/O/IVWuQaNG3URiebxuJXedvIiPLjA/+OAUhgDbVyuLZVpXQtlpZp++phDvpMJkFrqdk4KUlh9CgvB8eiQzEngu34K3T4E66ES2rlEXdcF9UKOOJED8Dtpy8jv+uP4m4m/bfEgc2LY8Jnavhoz9OoVqwD9pXD8LPey7h592XAQCRZb2wYUIbCCFwOjEVey7eRt0wXzSKKGM9xq7zN3HqWiq2nk7C1lNJ0GpU6N0wHL/uvwIA8NJpMPHRanimeUWnXYj5cfaezk/CnXS0/3SbNbBpHhmAH59tmuf/pvU8JjN+P5yAcr56tK0WBACY9r8T+HHXJQBAkI8OXw5qiBA/A8L9DTibdBfBvnroNGrE3UxDlXLeTrv9VBe2QbP8WQghMDTlRfxlro9+jcPxYb96EELgfFIaynhpcToxFT/+E4chLSqieWRggc/zdpoRSakZqBLkbc2gmswChy/fwYFLt5GUmomz1++ie/1Q1A7zhZdOAxWAUD+p7CG+evjZdA0VtZ7vlxACP+++hMjdU9BAcxHror6FWmtAu+pBKOfr2D0LSF04apUKYf4Gl5XLXaz1/GhnGH57DupTjoENAOhUWZjssQiTdEulDd7B0u+7idJvjQ7wLgcAMHWYDPX5bVAd+QWAgKjUBqYnFgB6X4fjqv79HR7LTwMAjuuHwdTkWZi7fizdKQTUG9+G+uRaICUBKrOUad1rGAN4GGBu8SKg84F6+0eAzhdQa4DUaw9WIcIMlZDal//pJkvHlNE9jR/Q5Zis72lLz0thlKjMTbt27dC4cWN89tln1m0rV67EgAEDkJaWVqhKTE5Ohr+/f6Eiv6IwGo1Yu3YtunXrZi3H68sOYdm+y9CoVWhaKQC7zt+EzkNtl6r/ok0Gel6eBRj8AeM9bK/1NoasSSv0easEeWP1uDaIv30P76w6ihc7VkO7GtI/Hq6fAuY0AwD0y5iKM/o6qBzkjcOX78DP4IHR7ati6Z5LyMwyY/W41gj2NWDH2SQMi9mN59pWwRuP17Kex2wWeGvVEeg0akzVxEC1dz4AoK55Ce5mSs+nQoAnyt0+jJX6KdKDajwOnFqfU1i1B+AZABjvAZm5upSG/obboa3Q7P82WrtqynrrsPftR+0aR6PRiKM/vIFGxr3AoJ+hKhNhf5zLe4EVz0l/948Bwhvj+R/2YsPxnA8CL50G29/oiLI24212nEnC1eR09G1cvsDG2MLSFVcYGVkmnEhIgYdahTmbz8BoMuPTJxvB38v+PZucbkSDqRsAAAObRuDD/g0cD3Z6I/C/V4Cse4BPCPDMCpi8g7F8/2VEBHihRZVA7I+7jaq3/ob/7k+h6v0lEFIXMBmBxYOAhMNAeGPg5lnAnAX4RwA3zwMmm07yKh1h7DYLazf8afeeztOpP4D1k5CQnIln776If0VF/DjyEbStXs5+v6MrgD/fAzLvSuft+zVQrobD4W7ezUST92MBACNaR+LdnnUcz3lyHbBpOvDY/wFVOjjev6AbcPFvAMDl0C54R/8GPurf0LEhv7gDWPs60O1j4Pw2YM98SHkLJzwDgK4fAlU75WxLvQ78+ixQuyfQfHTO9m2fAGc2Av2+lf6/fxkKVO0IlG8KrHoBMKYBvmEwdp+NDbtPouu9lVAnHHB+zkE/A0HVnZepKK4dA+a1kv4e8QdQsYXjPkIAse8Ah5YCwbWBp34BbpwBfnsRSL5iU65A4KklQGCVBy+XG2Qd/Q3pv78Ob3UmVPduSUHKkJVA+Sj7Hbd+CPw1G3m+Bwqi9wM8nASLGanS/6ytxz8EDi2W/g+vHb2/8xVT6R5loPnPGdmDm8K23yUquPnPf/6D33//HcePH7due+GFF3Dw4EHs3LmzUOdxV3BzIzUDLWdsQqbJfszBp082xOnEVHy1Vcqo/Nv0fzAc/dl6v/ANQw8xE+eT1RjXqTpGtKmM2OPX8OWmM/hP11poHhmIulP+gBBAp1rB+GxQI/ga8njzxO0CYqQxSmfazoZX1CCoVMC3285jaMtKqBwkpTuzTGbrGBxAaoh1GnXeDffqccD+H6Tj9o/F4gs+eLFjNQR665Dw108I2/ii/f59vgL+/Z/0k5dHRgPdPsKo7/di4wkpEHm8bii+GmL/wWM0GqH9P+nbPVq8CERPB1aNAeIPAo2fBpLjgV3ZywO0fhnoMg2pGVkYv/gA9B5qVAv2QfPIsmhTPfsYh5YCBxdJgZB3UN7lEwLYOBW4dR7o+w2gdcE32YTDwB+T8Wf50fjsZADmPNUEEYFOuk5/7Aec/TPnds/Pgahh0t8p16TXp9FTwPpJQEq8tL16NNDiBeDHvoUujvANR2qmgI9/IFQVm0vvJ3PuPm8VUKGpFLRkf3Afr/MKbjV5Ca2rBAArxwAJB3N2v3lO+iC31WAQ0GMmoLNPvy/adRF3tn+DZ8MuwnPgfMcGwxK86P2AkRukhjjtJvD7y0DiCeDG6Zx99X7A62ekYCjpNAAB3LoANHpaasgBwMNTen65y5ebzhcYsQ44uBi4dxO4fQm4+Jd039Q7OftN9c8+tz8QPU0ql3c5oFIr4PhvOfWs80GGWQNDls1jc2v/JtBxUv7lspV6HfjfBCDpVM42zwAg/Q5w/V/p9qDFUsMaVAPo9DZwdpNUP/duSnVTWEHZwWn1aCnQlEvCYWD9m8Dd64V/jE8I8PgM4NAS4PQGILId8PgHQPxBiIXdobIE8Got0Gcu0GCA8+Ok3QSy0oF7t4CfB0mB6OAlgH954MBPwGab56nRSf+D/hWAJU8DGfm8joWh8wGeXiYFXWk3gIM/S68LBNDoGeDqIeDGWeCJ+UB4owc7l8Ffet+nXn2w4+RiNGbhz82b0bn3U4oFN4p2S6WmpuLMmTPW2+fPn8fBgwcRGBiIihUrYtKkSbhy5Qp++EFqSMeMGYMvv/wSEydOxHPPPYedO3di/vz5WLx4sVJPIU+/H4pHpsmMMl5a3E6TGoTyZTzRr0l5JKVmYuup62hfoxwMRvuXQJWSgBUDVBDVu1hnJvVoEI4eDcKt+8we2AhHLt/Ba4/VlPa5HSc1vI2eBqp1zjnYvVvWP6vpbgHZA2pzfwv2yJXSzz2zyEFyzkDXasm78E6PcdbbYaYE+33VWukbbcNB0rc/U/ZA44DK0t/ntgDLhgN7vgVqdcOQlnWx8cQ1aGDCC5kxQOxqIOUqULE50HQEkGEz+FHjASSdBA5np483z5AaWovbUreGj94DMcObOX8uK5+Xfm+fCXiWkRq225ek4zQbmbPftk+Av2dnP+lHgSZD7Y9zbov0jb/bx4BvaB4VV4D5XYCsdHROOo3Or510vo/xnjUbgch2UqYh8UTO/ctHAhe2A6f/AAIic7af3mD3uln5hEjp7eZjcp7T9ZPAytFQpcTDFwASE4BE5yl8ANJrYKNOsCdQLUgKho784rh/3X5Aq5eA5aOkYOfwEulDXO8jBacaHdDpbTwd1QT483vgdIrU8J7dBHgFAe3fADKSgUtStyYykoGFPYCyVYGUBOn/waL+k8CZP6UG+8e+OXVnYQlsgJxv1eWbAr0+dyy3EFJje2E78FUb53Xxy1Cg2SigwiM52zLuAH9nZ5vvXpfKAwD9vgP+mgVV4jEYAAidN1RP/gD42YwdPLYK2PYRsPUD6T1W2GxCcjxw51L++/y7BjixOqdcR5ZJjTgAQAV4GOwzDUE1gX5fS69P4gnpvQbkBFBJp4CIR4AjvwJRw+0/i2ylXAN+H2/3+eRU0qmC93H2GNvXJukUcOEvqavHlIGrfo1QduDn0PqHAz7l8j6OV3b3pF84MG6v9LlgCb7bvwE88pwULN44KwUIli9GrxzNv949DMC3nYD02473tXwJ6DJNymoa/HLO3+41oOFgQJiAMhUBsxnITJHOKxe/8IL3KQqjERnaMvIes4gUDW727t2Ljh07Wm9bxsYMGzYMCxcuREJCAuLicj6oIiMjsXbtWrzyyiuYM2cOwsPD8fnnnxfLaeAXbkgfEoMfqYhj8cnYduo6pvWuC5VKhXK+eqwb0wiIfTf7AwtA05FSyvfUeujTrgJaDZCVKaXwa3YFfEKBf+YAHSajd6Py6G1cD+xYIzUSs+tLx7hxNs/gJs9/OCGkTMe5LdJ5ooYX/ORunrN5on8BrcY5vw+QyqP3kf52SKt7A3X6SN/cDy8B9nyHtk/+iFqhvuh/4ys0vLQGsBT78BLgxP/gYZu6VXvYny/rntTwWBxbIX1T7jhZ+iDYOAUIqZfzbc1kk4U4+2fON1oAOPSzlGnS+wIh9YHN03Pu2/WN9A1KbRMU/tA7p0xPLpD+zkgBNmV3mdR8HAXKyp4Zlm7zzc9skgIvgz/Q4EnpgzErHfCrIDXc57cBu+ZJH8YtXrB//n7lpUyTxbXs8VQRLYBL/0jB0dDVwO2LUrBpEVIXqNgSWYknseufHWh95sOc+4b9DqhsnveqF3KCCctxLQHsGalbCdUeBdq8Iv2t8wbCGknThl7YCRz4UQoYLPtabJ8JtB4vfYgDwO5vpOAGkF7nw79IDU6Zijnvg7Sk7HP4An3nSa99eGMpk3V4qX1g4+EpfeuNc5LxbTlWqgNnBvwAfPeo1KXnzPHfpG7DPnPtt9u+TzNTpYaxXj+gTm9kxe3CPzt2oHnP4dD6h9g/LitdCm4AqW6LwuAvdfvpfaX/8w1v22fRbLN/+7+XflfpKDWmfuFStuuzRlI26/EPpGygJXsWXAfY+SUQfwB4dCpwcacUUP+SHSBf2g1MPCG9rue2AO3/I315AKRAz7a7Oj/lagPdPrJ/z+XFbALWvJqTsYtsJ30+Zf9fi+B62Bs6Fo8F1wGKkk3w0APIlTX0DJB+l61qv93gBxjyeO9YvLRX+mKWmgikJwPlm0ifz5b/QUtgY8u/fM7farW8gU0ppWhw06FDB+TXK7Zw4UKHbe3bt8f+/ftdWCp5WNYVCfHV44WnGuPKrXuoHWbzpt08A9gbk3O7XE0ge3AXUrK/Ye+aJ32A7PwS8A2XuhhuXZT639dkD5K27TpKsknDA/bBze08gpsdX+R8ez23xT64STgM7PlOaqzKVgXaviZ9gNy+mLPPlX3SB+eZjcCV/Tnf4tq8AkQ0l1Lw+VGpgCZDpODlyn6o1Sosfq4F/GcNAHL3Dpz9037Gxr1bjsFUbru/lhq1bp/kfHtOOg3U6Q1obD7gbAMb6/myG1NLF0LjIcDR5VKQsGma9KGeW3z2mAmzCfh1hJQxObVeCm4S/5W6AVq/nPPNEACyMqTG3CKomvT737VSOtqSNdn5RU4gUaub1MBYbP4/KYNhK1kamIyqnXKeCwC0f13K2JSpKNW/bWBj4RcG4RmEpGN3YGr2PDR7vpFe08h29vs9OlV6npVaS4HEpX+kxvDkOmBb9mDJek8AlZ1kOrQG6Rtw3E6pXgEp2D2+CojfD5y2CXhsy//XrJy/6/SRGs4L26U6B6TsgU9wzj5NhuVk9wKrZAdoGilDcmU/cDcJOPqrtE/9J6XMUl68AoEx26WgMrAqoPWU/m9Wv5Szj/EusGxY3scApNdErQHUGoiIFrjhe9P+PWERUi/nb70f0Gde/se1VbGFfVfryA1SVm7X18DBn3I+Z8pUlN5X5WoBA763bzjHbHf+HlGpgKeXS1/IwhoAFVtJwY1F6lVgyWDpNRQm6fWxPJd/10i/H50KlM1nHJFGK73ftJ6Ff86jt2W/NpHSZ+qNs1KWSaNFVkQrmDZsKvgYrmbJGFkCJMD5/yA9kBI3W6qksAQ3of4G+Bm08AvL9U3h6mH72x6GnNTggZ+A8CbSgGALy9iJs3/aBxcnfrc/jmXBHKBwmZudNqtDZ6VL2QyNVgoAvu9hn0WoHi2lyS1jElRqKZ19aLH0Dd5WnT6F7w8OayQdK/kKkJyAAK9Ax4F3bV8FAiIh/pwGlWXWwr1b0ocXIKXMc3WPWF07mtOFAUjfhLd9JJ03L3V6A5XaAOtez9nW9SPpw3bFc1IDG1RD+jabZbOmT1Z2n/6Gd6TABpDGL2TeBWIek9LRKVel8UG346SuxL8/k7odLAIipWOueD4ncwHkBDbBdYDO7zqWeccX9rct4yZaTwCuHpFeK7WH9N5y1pDmwdxpCjTVOjvvZqj3hNRVFFIX2DlH2mYyAqvH5+xjO/jWmRZjpeDGUAboMUvKmN29DuxbmP/jAqsAHd6UskE181kMtFIrKft27Yg0dsW/Qs595ZtIvyu2kAKbKh3tvzA4k/t8DQfbBzeFUe3Rwu3noQcqNAMu7wGi3wdq9yjaeXIfK6xBTgbFot+30vs2vJFjRiAwEnnyLiv9AFK38bDfgTtXpEDm4CL77MzVI9KP9bhVgVYv22c/5aDzss+Slq2ak12RcV0zKv4Y3LjItWSpkQvxy2PgqTFX4631BHyz+9qTrwCLB0ppfmdsMzS2aWbjXSlzcmkXEFxXGsdicfuSfeADSLfvJuUqVxqQYQJ+lqZwI7yJ9A0vJUGaVfJXdoYhtIF0rIRDjoGNSlO0GRR6Hyn9nHhMmpHhbLxD+zcBDx2yqnbBze+eREjKYSm4ScueU99ijDSDKC/Ogjvbusut7WtSQ3BitfRh/cho6YOzwQCpjrd9LGVV6vSRZldYpN+Wxrb8k93Qa3RS5uv6yZx+9nNbgHObpbEuJ/4nNVy2Uq9Jg0Etgc2w36VxSWk3pNuP/V/OVFO9n2PGJjeDH/DMCum84Y2LFNgAkBrF/LrVqrSXfmuyFz7LSMmZNvvkwoLHIFVoKs1a8QmRyhZST3ptco9LKFtNCnosAXe71x0GITulUgGDF0vZoNq9nO9j8AOqdyn4WM5oPACtt/T/B0gDQa/sk/4uVxu4nj0mKqByTsBZUMBnq3+MFBjUlGndFUOZnL9VaiCsYdGyI3mxZPVq95SCxsw06TUNrSdl3qwrL6qlmZRyBzZENhjcuIDZLHK6pfIKbizjKyw8DI6NTl597KfW5X3yOY843268KwUDlnOk3ZQGNopciywdWyk1gjfPSenqp36R+tFTEqR+dIvec6RutYRDjudq84rzfuP8VGgqBTdn/5QyV0DO2AmfUMAju+H0Kotz5brYBDfZ2angOlJ3wrEV0oDQK3vtj+9sMG1ugVVzxlIE15Z+94+RZgJF2XQztH5ZCm6SrwBbZgA7bIIxY5pUh4AUYHoFSsGRbbeXOStnfIiz1/LSrpxMU/0BUqMR1jCnaya8cc6+w/8nBbu/T7DP8tjS+UjjncKcTCuXkyb74yQ5O8uo1gK1exfusbaNfYVmOYGnWpszQ6t8Uyk7Z+n6K0oAXSZC+nEVg19OcFOvf05wU6GpFAwLs5ShWveG9FoWZdB5mYrSj1xsMzd+5eUJbGzpfaRB1bbyGsNE5CIMblzgZvZ1i1Qq5Lk4lvPMTSFHrB9xcqFQnW/ejZvF0eVS5sGy5oZl8KlGJzUixrvSdFWL7jOl/mFLqtrS4NQfIDWUuQfTdc0e+NjsucI9D1vtXs8Z1GjpatL7St8CczF6ZA9QTo7PyTwFVpEGcYbWl76dz2uZM7AVyOnWy63NK9J4gaST0oyhI78CviE543F8gqWskC29L+BfEbgTZx/YWFjG9pRvIgWtF7YDWz/Kud9shLSsugBqdpfOVz1aGlibewquJZtgO27Gtq8+rKH0s/1TIDF7iQTPQGl2kEVhshtysGRu7mSP9fEJvr9v5y3H5mQdGz2V874IrS+9R+8nuHE1g3/OGJZQm3EyofWl7BUA1Ooudf/kNYvIXWy7nvxdGPARKYjBjQtcvSNlZYJ89E5XTgXgPHNjOwXUQqWRpuhmpUvf2Ne/6TgeRecrfUM8tzn/gq19TZpl0mOm/away4ed5ZsnIDXeVbM/hC1ZGMvgXUs/fO4PxibD7n/9lzIR0uP3f5+z5kIe2Z9MTXZjbVn/widUmhmjUgFtswdaj1gvPddTf0gzhiyZG41OCiTT70h1bhkUXCP7mmWNBheuvMG1pODGlkotfUO3lL98VM4gcdtZS8Z7sE7p7TMnJ1i5d8u+i69MJanbCwCavyB19+Qe0GvhH5ET3ITWB85vzblPyeDmfgRWAZ7bLI3lav9mTnATUsc+I+edz1Red9PbvFc9A4CRG4GTa6XlC2wHrrce7/hYd7PtlnJlNotIQQxuXMByteAQvzyyNoDNehLZtJ72HzoWNbsCPWfn3BZmKcABgMptpQa1/pNAXD7TRH1Ccxrcy7ulLgxbhjL2q9MCQO8vc751OwwyzP7GbPvB6Bn44AvbabMXrEvNHq/hZAlzAMj0yNVYl49yHARaPkr6ubxXCiwsz7/bJ1J//8Yp0roS96tcrZwBw4A05uKl3cDeBcD2T7LL0ETqTtj835xuKMA+o6SxeY9YggOLx2fkdMdpPKQp7XmxfS2C69gHN1p3BTfZjbglSPYJyXvfgpRvkjPYt/tMKZtXpSNwzuZ5FXJ1aLcw5ApuQisAEXmsraQ0Zm7oIcDgxgWu3pEChdC8xtsAgNFJ5kalkhZx2vJhTgORe1ZF8zHSN//EE9K0UMs4B9sZVLmVq2m/AmXu8SieZewXx6v3RM4gUcD+WymQE9z424wDyG9138Ky9P2nZJc1j+DGqMkd3DTJ+5i5AwbvclI3UN+v7rOQ2SxjciyGrJRm4XR8S+p2ykiRZuio1cALfwOf1nR+HNtVd9W5/h1zlz0/to2Ubdk8DDnvEVfLXd77zdzkZruYYptXpMHZea0sqxTb186227A4sh1zw8wNlVIMblzgakGDiYVw7FqyNOytXwYi2wPfZAcXufvnbbtebAXl0XjW6iFNhbX9Jp+bwd9+Qbvc3Rh5ZW5sAxo5ruJhydxYZsnkDqosci/olfvaMLZyN+xyBGGAlLmxZWnI1WopQLWV1+BRlcb+YnW23RfObufH9hy269+4q0sKcBLcPEDmJi+eZaTrGRU3lu5HIOd9XFzZZohtp8UTlSKci+cCifkFNxmpwCdOFq7ysNk3pB5QI3u14MLOknCWvShbDRi0yD470D9GmpJqe/kAQxn7D2Sdj/1xbFPuWm/AK3ttC9tugYKux1MYuWdt5JG5cRDRPO/7HDI3cgU3NQHbJQXvJ0uR+1pJ6tzBTREyN7avme1A76Ic40Hlzjy5IrgpriwLCALFq7vMGdsvK55FXBaAqIRgcOMClsyN026py7udXwjOtmHXeEjfTnt+5rhfXpzNHLF8Q6vTV+q2aPac1OX08iFp8TgLzzL2588vc1MmwvmHd+6G7X7kHrOTzxLjppbjpMBvzF/S+jN5yR0weMkU3Oi8gYBK2X/7FpwhGbhIakhUtpmaXIFH7ixTUQKTap2lbFK9/vbfzG3H97iaq7qlSoLcSyoUZ1qDNDsvvIm0XhVRKcRuKRewzJYKdjag2HbVYVseDzgY11nAYelb9y4LTDhiv4/tTBODf/7BjW33UO4BiI9/AMROKVoglpfc6fx8MjfmTlOgiX6/4G/Jdl07qsJngwqjXG1p6nZhGvHaPaSpwL8+m7MOToGZmyJ0S2k9gbH/ONZHVobz/V0hd3DjbIB8aWXbLVUSPJV9QdPinmUiuk/M3LhAYkr2gGJ/JwGLZbXS3B40uAGkWSW2bLsqcn+I2XbPeHgW0C1VJufv3AMQW7wATL4CVG5d5OI6KGq3VGE+mG0DBA+9vB/mwdnjbgrb/aJSSXVtLVuu4CZ3MJM72CnM8XPLveSAK+Uuf3EfeyKnjm9Lv+9njSclqFQMbKhUY3Ajs4wsM27elboCnHZLJWYHNz0/l7ozLORYirzZSOC1Mzm387uSrm02xmwsoFsqn8wNULQMQ34cMjdFXOXYGdtsQu5g4kFZ1pwJa1j4x9hmazxyd0s9wJibvMgxFqqwcpf3QZcGKEkqNgf+cxHo9rHSJSEisFtKdtezszY6DzX8PXM1VkJIV4YGpBk+rri2ik8hFzaz/dZmMubK3OQz5saVsyvud0BxfnJnbuRUtRPw8uGi1Yk2n8zNg3RLFQe5gxsPT+f7lVa5L0hJRIph5kZm12wGE6typ31TEoCMO9Kg0qDqsJttowR9dtBS7VH7b9n5jblx5RoeRRhzU2hqFwY3gDSo2HY6d0Fsux8LmvrtzplOcsg9IPphytwQUbHC4EZm8dmDiZ2Ot7F0SQVWkX/8hzMFHX/cXuDZ9dJ4mfzG3Ng2UrnXd5FT7sxNPrOlCs2uW6oYBAu2wY2cU8Fzq9tP+l2v//0fo6ge9swNERUb7JaS2aWb0uJ8FQOdDKa0XBnasoJsfmNi5FCpgEG+PsE5M33yG3MDAKO3AxnJrl3RNPegajnWpLHNJrgic1NUtoFigVPBH6BbqtcX0kVHLRfedIeHecwNERUrDG5kdvm2FNxEBDgJbiwXNrQuj++izM24/cCFv4DGzxT+MQUFN2FuWA8jd7eUHGvS2Da4xSG4KVLm5gGCG70PUK/f/T/+fuQuLzM3RKQQBjcyu3wrO7gJdPLBbhlMbOnacVW3VNmq9qvUFoZto5u7W8pdbAMslUaeSwfYBgxyz5a6H3ZjbgqYCl4cutGKIncXoCsGzBMRFQI/fWR26aZ0te8IZ91SN05Lv63jVorTOhM2ZXHn9Yhs5R73I0fwZzdbqhgEC7YBXEFTwYsyULk4sMuSMWtDRMphcCMjkwASkqWp4E67pTKzr/RtmTLq6jE3RWG7wqpSDZNt457fJRXu95jFInNjU4aCpoKXNLaX4HDXlciJiJwoRq1ryXc7AzCZBXQeagT75mq4zOacBdUs33D9y7u3gPmxvTaOUt0JKhdkj4rdmJsiZG5KGtu6VpWwrBMRlSoMbmSUlh27BHrpoFbn6lIxG3P+tnzD7fctUKkN8Mxy9xQwP8Uhq2FLruBGXZxnS5WyzI1tcFPSutSIqFRh7lhGJiH91uQObABpFWALSyNQtirw7BrXF6ww6vYF9i0EKrdRuiQSuQY1u/LyC/cj39lStt81itN4rEKyzTwxc0NECmJwI6Ps2AYeGmfBTWbO38Wx+0FrAEb+oXQpcsjWLVXMBhR75LPOTUln263ImVJEpCB+AsnImrlxNsvH9gKGasaUBXLFmJvikLmxmy2VT3lK+hWbmbkhIgUxuJGRWUgNkvNuqezMjUZX8hsuV2o6Qvrd7nV5jufKC2feD7vZUqUsc2OLY26ISEFMIcjIXJgxNyV90KirdZ8JdHlfWmFXDq6+cGZR2c6Wyje4KeEBMLOTRKQgZm5kVKjgpjiOtylOVCr5AhugGHZLFfJ6SyU9u8duKSJSEIMbGVmWwXMa3JgZ3CjC7sKZxaAbyHZAse3CiQ5KeHDDAcVEpCB+Asko/8yNzZgbch+7RfyKwVWqbYPb/IIbZm6IiO4bgxsZWYIbD6fBTfZsKY5FcC+7C2cWt8BSFLxLScUBxUSkIAY3MrIEN2pn37qZuVFGcZstZatUd0sxiCci5TC4kZE1c+NsET+OuVGGphhnbvINbko4dksRkYIY3MjI0lQ5z9wwuFFEcRtzY8s3PO/7SvqYG3ZLEZGCmDuWUf5jbrK7pbjOjXsVtwtnAsCgxcCFv4AGA/PZqYQHNwGVlS4BET3EGNzIqHDr3BSzrpHSzm6dm2JS97W6ST/5KamZm2dWAAcXAV2mKV0SInqIMbiRUeGCG1a5WxXnAcWlUbXO0g8RkYI45kZG+QY3ZmZuFGEb3JSoQa4lNHNDRFQMMLiRUc4KxU6qlVPBlWE7xklVgt7uJbVbioioGChBn/bFn92A4st7gc8aAif+J220XjiT3VJuZRtMMmAgInooMLiRkd0ifsuGA7cuAEufljZyQLEy7LqlSlJwU5LKSkRUvDC4kZFd5sacletOrnOjCJUK0PtLfwfVULYshdFmovS720fKloOIqARjH4mMrJkbtQrQ+djfaR1zw+DG7V47KWXOdN5Kl6Rgnd8FWowFfMopXRIiohKLwY2MzNldCR5qFaDPHdxYLpzJ4MbttJ7ST0mgUjGwISJ6QOyWkpHdVPA8Mzccc0NERORKDG5kZBfc6H1z3clF/IiIiNyBwY2M7DM3NuM7zGbOliIiInITBjcyyrNbKjOVF84kIiJyEwY3MrKsUOyhVtlnaDKSbTI3DG6IiIhcSfHgZu7cuYiMjITBYEBUVBS2b9+e7/5z5sxB7dq14enpiZo1a+KHH35wU0kLZreInzDn3JHO4IaIiMhdFB3dunTpUkyYMAFz585F69at8fXXX6Nr1644fvw4Klas6LD/vHnzMGnSJHz77bdo1qwZdu/ejeeeew4BAQHo2bOnAs/Ant0ifsKUc0f6HV44k4iIyE0UzdzMnDkTI0eOxKhRo1C7dm3Mnj0bERERmDdvntP9f/zxR4wePRoDBw5ElSpVMGjQIIwcORIffvihm0vunN0ifraZm4xkLuJHRETkJoplbjIzM7Fv3z68+eabdtujo6OxY8cOp4/JyMiAwWCw2+bp6Yndu3fDaDRCq3UMHDIyMpCRkWG9nZycDAAwGo0wGo0P+jSsjEajdcyNCmaYs4zWyDHr7g2oszKhBpAl1BAynvdhY3nN5HztyDnWtXuwnt2D9ew+rqrrohxPseAmKSkJJpMJISEhdttDQkJw9epVp4957LHH8N1336FPnz5o0qQJ9u3bh5iYGBiNRiQlJSEsLMzhMTNmzMB7773nsH3Dhg3w8vKS58lkMwspnDn177+4ZIxDpeztx/btROideIQAOHz0BC4lrJX1vA+j2NhYpYvw0GBduwfr2T1Yz+4jd12npaUVel/FV5RT5bpSsxDCYZvFO++8g6tXr6JFixYQQiAkJATDhw/HRx99BI1G4/QxkyZNwsSJE623k5OTERERgejoaPj5+cn2PIxGIxae+hMAUK9uHURcDwduSvfVqxYB1YXzQArQoEkU6tftJtt5HzZGoxGxsbHo0qWL00wdyYd17R6sZ/dgPbuPq+ra0vNSGIoFN0FBQdBoNA5ZmsTERIdsjoWnpydiYmLw9ddf49q1awgLC8M333wDX19fBAUFOX2MXq+HXq932K7VamV/g1vG3Oi0HlDbxGcaU4b1KuEeOk+A/1gPzBWvHznHunYP1rN7sJ7dR+66LsqxFBtQrNPpEBUV5ZC2io2NRatWrfJ9rFarRYUKFaDRaLBkyRL06NEDarXis9rtF/Ez28yWEqacqeBcxI+IiMilFO2WmjhxIoYMGYKmTZuiZcuW+OabbxAXF4cxY8YAkLqUrly5Yl3L5tSpU9i9ezeaN2+OW7duYebMmTh69Ci+//57JZ+GlWVAsUaVayq4OYsXziQiInITRYObgQMH4saNG5g2bRoSEhJQr149rF27FpUqSUNxExISEBcXZ93fZDLh008/xcmTJ6HVatGxY0fs2LEDlStXVugZ2LPL3NhOBTfbZG544UwiIiKXUrylHTt2LMaOHev0voULF9rdrl27Ng4cOOCGUt0f6yJ+mlzdUuYsLuJHRETkJsoPVClF7C+/IGzuyAKM96S/PRwHNxMREZF8GNzIyCykKVIearX9mBtTJpB6TfrbJ1SBkhERET08GNzIyDqgWA37MTcp16TsjUoN+Dif5k5ERETyYHAjo5wBxWr7MTd3Lkm/vYM5oJiIiMjFGNzIKCe4gX231O3s4MbP8fIQREREJC8GNzKyy9zYdksZ70q/fcPdXygiIqKHDIMbGVmDG5UKMJsdd2DmhoiIyOUY3MgoZ0BxrkX8LPyYuSEiInI1BjcyslvEz3bMjQW7pYiIiFyOwY2M7Bfxc5K5Mfi7t0BEREQPIQY3MrJmbnJfFdxCwyuCExERuRqDGxnZXzjTSXCj5ho3RERErsbgRkYFDihm5oaIiMjlGNzIyC5z42wquJrBDRERkasxuJGRfbeUs8wNu6WIiIhcjcGNjOwW8XM25kajc2+BiIiIHkIMbmRUYOaG3VJEREQux+BGRpZwxkPDqeBERERKYXAjowK7pTgVnIiIyOUY3MjEbBYQUAGwdEsJx52YuSEiInI5BjcyMdkEM5q8VijmmBsiIiKXY3AjE7M5V3DDqeBERESKYHAjkyyH4IaZGyIiIiUwuJGJOXe3FC+/QEREpAgGNzKxy9yoOOaGiIhIKQxuZFLgmBuVGlCzuomIiFyNra1MLJkbtQpQqZwEN8zaEBERuQWn78ikjKcWY2ub0LRZM2lD7m4pjrchIiJyC2ZuZKLXalCzjEDb6kHSBoduKY37C0VERPQQYnDjKrmngquUKQYREdHDhsGNqzhMBWd0Q0RE5A4MblxBCCfdUgxuiIiI3IHBjSs4W8CPmRsiIiK3YHDjCs6CG2ZuiIiI3ILBjSs4W52YmRsiIiK3YHDjCk4zN6xqIiIid2CL6wrOrgjObikiIiK3YHDjChxQTEREpBgGN67gbMwNMzdERERuweDGFZi5ISIiUgyDG1fgVHAiIiLFMLhxBUu3lN3FMhncEBERuQODG1ewZG5sp39zKjgREZFbsMV1BctUcLVN5oaJGyIiIrdgcOMKzjI3jG6IiIjcgsGNKzgbc8MBxURERG7B4MYVLJkbNTM3RERE7sbgxhWcDihmcENEROQODG5cgVPBiYiIFMPgxhU4FZyIiEgxbHFdwelUcGZuiIiI3EHx4Gbu3LmIjIyEwWBAVFQUtm/fnu/+ixYtQsOGDeHl5YWwsDA8++yzuHHjhptKW0jWzA27pYiIiNxN0eBm6dKlmDBhAt566y0cOHAAbdu2RdeuXREXF+d0/7/++gtDhw7FyJEjcezYMSxbtgx79uzBqFGj3FzyApg5oJiIiEgpigY3M2fOxMiRIzFq1CjUrl0bs2fPRkREBObNm+d0/3/++QeVK1fG+PHjERkZiTZt2mD06NHYu3evm0teAGu3FKeCExERuZuHUifOzMzEvn378Oabb9ptj46Oxo4dO5w+plWrVnjrrbewdu1adO3aFYmJifj111/RvXv3PM+TkZGBjIwM6+3k5GQAgNFohNFolOGZwHo8y2+VMRMeAARU1pBGqD2QJeP5Hla29Uyuxbp2D9aze7Ce3cdVdV2U4ykW3CQlJcFkMiEkJMRue0hICK5ever0Ma1atcKiRYswcOBApKenIysrC7169cIXX3yR53lmzJiB9957z2H7hg0b4OXl9WBPwonY2FiUTf0XbQCkpqXjbMRw1EpYgZ1lBiB57VrZz/ewio2NVboIDw3WtXuwnt2D9ew+ctd1WlpaofdVLLixUOUaiyKEcNhmcfz4cYwfPx7vvvsuHnvsMSQkJOD111/HmDFjMH/+fKePmTRpEiZOnGi9nZycjIiICERHR8PPz0+252E0GhEbG4suXbpAF+8HnAZ8fH1Rd+gngPgYbTjmRha29azVapUuTqnGunYP1rN7sJ7dx1V1bel5KQzFgpugoCBoNBqHLE1iYqJDNsdixowZaN26NV5//XUAQIMGDeDt7Y22bdti+vTpCAsLc3iMXq+HXq932K7Val3yBtdqtfBQS4GMSu3BfyIXcdXrR45Y1+7BenYP1rP7yF3XRTmWYgOKdTodoqKiHNJWsbGxaNWqldPHpKWlQa22L7JGI023FkK4pqD3w+lUcCIiInIHRWdLTZw4Ed999x1iYmJw4sQJvPLKK4iLi8OYMWMASF1KQ4cOte7fs2dPrFixAvPmzcO5c+fw999/Y/z48XjkkUcQHh6u1NNwZJ0Kzq4oIiIid1N0zM3AgQNx48YNTJs2DQkJCahXrx7Wrl2LSpUqAQASEhLs1rwZPnw4UlJS8OWXX+LVV19FmTJl0KlTJ3z44YdKPQXnrFcFZ+aGiIjI3RQfUDx27FiMHTvW6X0LFy502DZu3DiMGzfOxaV6QJZ1bng9KSIiIrdj6+sKTq8KTkRERO7A4MYV2C1FRESkGAY3rsBuKSIiIsWw9XUF4eTCmUREROQWbH1dwdlVwYmIiMgt2Pq6AsfcEBERKYbBjStwzA0REZFi2Pq6AqeCExERKYbBjStwQDEREZFiitz6Vq5cGdOmTbO7LALlln0RTwY3REREblfk1vfVV1/Fb7/9hipVqqBLly5YsmQJMjIyXFG2kkvwwplERERKKXJwM27cOOzbtw/79u1DnTp1MH78eISFheGll17C/v37XVHGkkdYMjcMboiIiNztvvtNGjZsiM8++wxXrlzBlClT8N1336FZs2Zo2LAhYmJiICwN/MPIkrkBgxsiIiJ3u++rghuNRqxcuRILFixAbGwsWrRogZEjRyI+Ph5vvfUWNm7ciJ9//lnOspY8HHNDRETkdkUObvbv348FCxZg8eLF0Gg0GDJkCGbNmoVatWpZ94mOjka7du1kLWiJwjE3REREiilycNOsWTN06dIF8+bNQ58+faDVah32qVOnDgYNGiRLAUskwdlSRERESilycHPu3DlUqlQp3328vb2xYMGC+y5UiccxN0RERIopcmohMTERu3btcti+a9cu7N27V5ZClXzM3BARESmlyK3viy++iEuXLjlsv3LlCl588UVZClXiccwNERGRYooc3Bw/fhxNmjRx2N64cWMcP35clkKVeBxzQ0REpJgit756vR7Xrl1z2J6QkAAPj/ueWV66cMwNERGRYooc3HTp0gWTJk3CnTt3rNtu376NyZMno0uXLrIWruRi5oaIiEgpRU61fPrpp2jXrh0qVaqExo0bAwAOHjyIkJAQ/Pjjj7IXsETimBsiIiLFFDm4KV++PA4fPoxFixbh0KFD8PT0xLPPPovBgwc7XfPmocTghoiISDH3NUjG29sbzz//vNxlKT0sl9VitxQREZHb3fcI4OPHjyMuLg6ZmZl223v16vXAhSrxOKCYiIhIMfe1QnHfvn1x5MgRqFQq69W/VdldMCaTSd4SlkgcUExERKSUIre+L7/8MiIjI3Ht2jV4eXnh2LFj2LZtG5o2bYotW7a4oIglEMfcEBERKabImZudO3di06ZNKFeuHNRqNdRqNdq0aYMZM2Zg/PjxOHDggCvKWbJwET8iIiLFFLn1NZlM8PHxAQAEBQUhPj4eAFCpUiWcPHlS3tKVVBxzQ0REpJgiZ27q1auHw4cPo0qVKmjevDk++ugj6HQ6fPPNN6hSpYorylgCMXNDRESklCIHN2+//Tbu3r0LAJg+fTp69OiBtm3bomzZsli6dKnsBSyROOaGiIhIMUUObh577DHr31WqVMHx48dx8+ZNBAQEWGdMPfQ45oaIiEgxRWp9s7Ky4OHhgaNHj9ptDwwMZGBjyzrmhoiIiNytSMGNh4cHKlWqxLVsCsTMDRERkVKK3Pq+/fbbmDRpEm7evOmK8pQO1m4pZrOIiIjcrchjbj7//HOcOXMG4eHhqFSpEry9ve3u379/v2yFK7E45oaIiEgxRQ5u+vTp44JilDJc54aIiEgxRQ5upkyZ4opylC7WqeDM3BAREbkbW1+XYLcUERGRUoqcuVGr1flO++ZMKnARPyIiIgUVObhZuXKl3W2j0YgDBw7g+++/x3vvvSdbwUo0DigmIiJSTJGDm969ezts69+/P+rWrYulS5di5MiRshSsROOAYiIiIsXIllpo3rw5Nm7cKNfhSjhmboiIiJQiS+t77949fPHFF6hQoYIchyv5rGNulC0GERHRw6jI3VK5L5AphEBKSgq8vLzw008/yVq4EotjboiIiBRT5OBm1qxZdsGNWq1GuXLl0Lx5cwQEBMhauBLLEtwwdUNEROR2RQ5uhg8f7oJilDbM3BARESmlyK3vggULsGzZMofty5Ytw/fffy9LoUo8rnNDRESkmCIHNx988AGCgoIctgcHB+O///2vLIUq8TjmhoiISDFFbn0vXryIyMhIh+2VKlVCXFxckQswd+5cREZGwmAwICoqCtu3b89z3+HDh0OlUjn81K1bt8jndSmuc0NERKSYIgc3wcHBOHz4sMP2Q4cOoWzZskU61tKlSzFhwgS89dZbOHDgANq2bYuuXbvmGSR99tlnSEhIsP5cunQJgYGBePLJJ4v6NFyMmRsiIiKlFLn1HTRoEMaPH4/NmzfDZDLBZDJh06ZNePnllzFo0KAiHWvmzJkYOXIkRo0ahdq1a2P27NmIiIjAvHnznO7v7++P0NBQ68/evXtx69YtPPvss0V9Gq7FMTdERESKKfJsqenTp+PixYvo3LkzPDykh5vNZgwdOrRIY24yMzOxb98+vPnmm3bbo6OjsWPHjkIdY/78+Xj00UdRqVKlwj8Bd7AGN8zcEBERuVuRgxudToelS5di+vTpOHjwIDw9PVG/fv0iBxhJSUkwmUwICQmx2x4SEoKrV68W+PiEhASsW7cOP//8c777ZWRkICMjw3o7OTkZgHTBT6PRWKQy58dyLKPRCI3ZDDUAk1nALOM5yL6eybVY1+7BenYP1rP7uKqui3K8Igc3FtWrV0f16tXv9+FWqlxdN0IIh23OLFy4EGXKlEGfPn3y3W/GjBlOr1a+YcMGeHl5FamshREbG4tmCfEIB3D02DFcSFwr+zlIqmdyD9a1e7Ce3YP17D5y13VaWlqh9y1ycNO/f380bdrUoTvp448/xu7du52ugeNMUFAQNBqNQ5YmMTHRIZuTmxACMTExGDJkCHQ6Xb77Tpo0CRMnTrTeTk5ORkREBKKjo+Hn51eoshaG0WhEbGwsunTpAsNvvwC3gXr16qNOVDfZzkH29azVapUuTqnGunYP1rN7sJ7dx1V1bel5KYwiBzdbt27FlClTHLY//vjj+OSTTwp9HJ1Oh6ioKMTGxqJv377W7bGxsejdu3eBZThz5gxGjhxZ4Hn0ej30er3Ddq1W65I3uFarhTo78aTx8ICG/0Qu4arXjxyxrt2D9ewerGf3kbuui3KsIgc3qampTrMlWq22SFEVAEycOBFDhgxB06ZN0bJlS3zzzTeIi4vDmDFjAEhZlytXruCHH36we9z8+fPRvHlz1KtXr6jFdw8u4kdERKSYIre+9erVw9KlSx22L1myBHXq1CnSsQYOHIjZs2dj2rRpaNSoEbZt24a1a9daBycnJCQ4rHlz584dLF++vFBZG8VwET8iIiLFFDlz88477+CJJ57A2bNn0alTJwDAn3/+iZ9//hm//vprkQswduxYjB071ul9CxcudNjm7+9fpEFFymDmhoiISClFDm569eqFVatW4b///S9+/fVXeHp6omHDhti0aZOsA3RLNC7iR0REpJj7mgrevXt3dO/eHQBw+/ZtLFq0CBMmTMChQ4dgMplkLWCJxDE3REREirnv1nfTpk145plnEB4eji+//BLdunXD3r175SxbycUxN0RERIopUubm8uXLWLhwIWJiYnD37l0MGDAARqMRy5cvL/Jg4tKNmRsiIiKlFLr17datG+rUqYPjx4/jiy++QHx8PL744gtXlq3k4pgbIiIixRQ6c7NhwwaMHz8eL7zwgiyXXSjVOOaGiIhIMYVufbdv346UlBQ0bdoUzZs3x5dffonr16+7smwll3XMDREREblboYObli1b4ttvv0VCQgJGjx6NJUuWoHz58jCbzYiNjUVKSoory1kyMXNDRETkdkVufb28vDBixAj89ddfOHLkCF599VV88MEHCA4ORq9evVxRxpKHY26IiIgU80CphZo1a+Kjjz7C5cuXsXjxYrnKVPJZgxtmboiIiNxNltZXo9GgT58+WL16tRyHK/k4oJiIiEgxbH1dgYv4ERERKYbBjUswc0NERKQUtr6uwAHFREREimFw4wocc0NERKQYtr6uwDE3REREimFw4xLM3BARESmFra8rWMfcKFsMIiKihxGDG1fITtwwc0NEROR+bH1dgWNuiIiIFMPgxiU45oaIiEgpbH1dgevcEBERKYbBjStwnRsiIiLFsPV1BY65ISIiUgyDG1ewdkuxeomIiNyNra9LsFuKiIhIKWx9XYEDiomIiBTD4MYVOKCYiIhIMWx9XUFYlyhWtBhEREQPIwY3LmHJ3DC4ISIicjcGN67AMTdERESKYXDjChxzQ0REpBi2vq7ARfyIiIgUw+DGJZi5ISIiUgpbX1fgmBsiIiLFMLhxBY65ISIiUgxbX1fgmBsiIiLFMLhxCWZuiIiIlMLW1xU45oaIiEgxDG5cgWNuiIiIFMPW1xWs15YiIiIid2Nw4wrWbilWLxERkbux9XUJdksREREpha2vK3BAMRERkWIY3LgCBxQTEREphq2vK3ARPyIiIsUwuHEJZm6IiIiUwtbXFTjmhoiISDEMblyBY26IiIgUw9bXJSyL+DFzQ0RE5G4MbuRmuzoxMzdERERup3jrO3fuXERGRsJgMCAqKgrbt2/Pd/+MjAy89dZbqFSpEvR6PapWrYqYmBg3lbYQrDOlwDE3RERECvBQ8uRLly7FhAkTMHfuXLRu3Rpff/01unbtiuPHj6NixYpOHzNgwABcu3YN8+fPR7Vq1ZCYmIisrCw3lzw/tpkbBjdERETupmhwM3PmTIwcORKjRo0CAMyePRt//PEH5s2bhxkzZjjsv379emzduhXnzp1DYGAgAKBy5cruLHLBbDM3HHNDRETkdooFN5mZmdi3bx/efPNNu+3R0dHYsWOH08esXr0aTZs2xUcffYQff/wR3t7e6NWrF95//314eno6fUxGRgYyMjKst5OTkwEARqMRRqNRpmcD67GMmRnQWraZzICM5yCbema9uhzr2j1Yz+7BenYfV9V1UY6nWHCTlJQEk8mEkJAQu+0hISG4evWq08ecO3cOf/31FwwGA1auXImkpCSMHTsWN2/ezHPczYwZM/Dee+85bN+wYQO8vLwe/Ink8ueff6KnzTmyNM6DLnowsbGxShfhocG6dg/Ws3uwnt1H7rpOS0sr9L6KdksBgCrXuBQhhMM2C7PZDJVKhUWLFsHf3x+A1LXVv39/zJkzx2n2ZtKkSZg4caL1dnJyMiIiIhAdHQ0/Pz/ZnofRaERsbCw6d+wIHJK2RT/2OKDzlu0clFPPXbp0gVarLfgBdN9Y1+7BenYP1rP7uKquLT0vhaFYcBMUFASNRuOQpUlMTHTI5liEhYWhfPny1sAGAGrXrg0hBC5fvozq1as7PEav10Ov1zts12q1LnmDa7Uam791AP+JXMJVrx85Yl27B+vZPVjP7iN3XRflWIpNBdfpdIiKinJIW8XGxqJVq1ZOH9O6dWvEx8cjNTXVuu3UqVNQq9WoUKGCS8tbaFznhoiISFGKtr4TJ07Ed999h5iYGJw4cQKvvPIK4uLiMGbMGABSl9LQoUOt+z/11FMoW7Ysnn32WRw/fhzbtm3D66+/jhEjRuQ5oNjtGNwQEREpStExNwMHDsSNGzcwbdo0JCQkoF69eli7di0qVaoEAEhISEBcXJx1fx8fH8TGxmLcuHFo2rQpypYtiwEDBmD69OlKPQVHXMSPiIhIUYoPKB47dizGjh3r9L6FCxc6bKtVq1YxH+3OzA0REZGS2PrKjYv4ERERKYrBjdwEL79ARESkJAY3cuOYGyIiIkUxuJFdduaG422IiIgUwRZYbtbMDbM2RERESmBwIzfLkBtmboiIiBTBFlh22ZkbjrchIiJSBIMbuQmOuSEiIlISW2C5ccwNERGRohjcyM0S3DBzQ0REpAi2wLKzdEsxc0NERKQEBjdyY+aGiIhIUWyB5SaYuSEiIlISgxu5Wa8txeCGiIhICQxuZMep4EREREpiCyw3wUX8iIiIlMTgRm5cxI+IiEhRbIHlxkX8iIiIFMXgRnbM3BARESmJLbDcOOaGiIhIUQxu5MYxN0RERIpiCywzFcfcEBERKYrBjeyYuSEiIlISW2C5WbullC0GERHRw4rBjdx44UwiIiJFsQWWHa8tRUREpCQGN3Jj5oaIiEhRbIHlZh1zw8wNERGREhjcyI2ZGyIiIkWxBZYdp4ITEREpiS2w3LiIHxERkaIY3MjNOlmKVUtERKQEtsBy44UziYiIFMXgRnacLUVERKQkBjdy45gbIiIiRTG4kZvZJP1m5oaIiEgRDG5kprpxWvrDP0LZghARET2kGNzITBW/X/qjQlNlC0JERPSQYnAjM1X8AemP8lHKFoSIiOghxeBGRjpjMlR34gCogLBGSheHiIjoocTgRkZexiTpD98wwOCnbGGIiIgeUgxuZKQS2TOlPHTKFoSIiOghxuBGRmrLNHC1VtmCEBERPcQY3MhILbKkPzTM3BARESmFwY2MrN1SGg9lC0JERPQQY3AjI2ZuiIiIlMcUg4ysmRuOuSEiUpzZbEZmZiYAwGg0wsPDA+np6TCZTAqXrHR7kLrW6XRQqx8878LgRkY5mRtWKxGRkjIzM3H+/HmYzdLFjIUQCA0NxaVLl6Ditf9c6kHqWq1WIzIyEjrdg/WAsBWWkdo65obdUkREShFCICEhARqNBhEREVCr1TCbzUhNTYWPj48smQHK2/3WtdlsRnx8PBISElCxYsUHCkIZ3MiI3VJERMrLyspCWloawsPD4eXlBSCni8pgMDC4cbEHqety5cohPj4eWVlZ0Grvvy1V/BWeO3cuIiMjYTAYEBUVhe3bt+e575YtW6BSqRx+/v33XzeWOG853VIMboiIlGIZ5/GgXRvkfpbX7EHHRSka3CxduhQTJkzAW2+9hQMHDqBt27bo2rUr4uLi8n3cyZMnkZCQYP2pXr26m0qcv5xuKQY3RERK49iakkeu10zR4GbmzJkYOXIkRo0ahdq1a2P27NmIiIjAvHnz8n1ccHAwQkNDrT8ajcZNJc6fNXPDbikiIioGOnTogAkTJihdDLdTLLjJzMzEvn37EB0dbbc9OjoaO3bsyPexjRs3RlhYGDp37ozNmze7sphFomLmhoiI7oOzIRe2P8OHD7+v465YsQLvv/++vIUtARQbUJyUlASTyYSQkBC77SEhIbh69arTx4SFheGbb75BVFQUMjIy8OOPP6Jz587YsmUL2rVr5/QxGRkZyMjIsN5OTk4GIM3DNxqNMj0b6XiWbimTSgOzjMemHJbXTM7XjpxjXbsH61l+RqMRQgiYzWa7qeCW35ZtxcmVK1esf//yyy+YMmUKTpw4Yd3m6elpV26j0VioAbdlypQBALc+5wepa7PZDCEEjEajQ69MUf5HFJ8tlbt/TQiRZ59bzZo1UbNmTevtli1b4tKlS/jkk0/yDG5mzJiB9957z2H7hg0brKPo5VI7u1vqwqV4HF27VtZjk73Y2Fili/DQYF27B+tZPh4eHggNDUVqaqp1ET+LlJQUhUqVP9v2yDKo1rItLi4ODRs2RExMDObPn4+9e/fi008/RdeuXfH666/jn3/+wa1bt1C5cmVMnDgR/fv3tx6rR48eqF+/PmbMmAEAaNCgAYYNG4bz58/jt99+g7+/P1577bV8M0MbN27EJ598ghMnTkCj0aBZs2b44IMPEBkZad3nypUreOedd7B582ZkZmaiRo0a+Pjjj9G0aVMAwNq1a/Hxxx/jxIkT8Pb2RqtWrfDjjz86nCszMxP37t3Dtm3bkJWVZXdfWlpaoetTseAmKCgIGo3GIUuTmJjokM3JT4sWLfDTTz/lef+kSZMwceJE6+3k5GREREQgOjoafn5+RS94HoxGI64sXAIAqFylGio+2k22Y1MOo9GI2NhYdOnS5YGmCVLBWNfuwXqWX3p6Oi5dugQfHx8YDAYIIZCWmYXUlFT4+Pq4daCxp1ZT5PMZDAaoVCprG+Xj4wMAmDZtGj7++GM0btwYer0eQgi0aNECb731Fvz8/LB27VqMGTMGdevWRfPmzQFIgZ5Op7MeS61WY+7cuZg2bRreffddLF++HK+++iqio6NRq1Ytp+URQuC1115D/fr1cffuXUyZMgXDhg3D/v37oVarkZqail69eqF8+fL47bffEBISgh07dsBgMMDPzw9r1qzB0KFDMXnyZPz000/IzMzE2rVrnbbB6enp8PT0RLt27WAwGOzus/S8FIZiwY1Op0NUVBRiY2PRt29f6/bY2Fj07t270Mc5cOAAwsLC8rxfr9dDr9c7bNdqtbJ/kFi6pTRaPTT8kHIpV7x+5Bzr2j1Yz/IxmUxQqVRQq9VQq9VIy8xC/fc2KlKW49Meg5euaJNeLGvD5P49YcIEu6wMALz++uvWv8ePH48//vgDy5cvR8uWLa3bLXVh0a1bN7z44osAgDfffBOzZ8/Gtm3bUKdOHaflefLJJ+1ux8TEIDg4GP/++y/q1auHJUuW4Pr169izZw8CAwNhNpsREhICPz8/qNVqzJgxA4MGDcK0adOsx2jcuHGez12lUjn9fyjK/4ei3VITJ07EkCFD0LRpU7Rs2RLffPMN4uLiMGbMGABS1uXKlSv44YcfAACzZ89G5cqVUbduXWRmZuKnn37C8uXLsXz5ciWfhhUvnElERK5i6eKxMJlM+OCDD7B06VJcuXLFOsbU29s73+M0aNDA+rdKpUJoaCgSExPz3P/s2bN455138M8//yApKck6jiYuLg716tXDwYMH0bhxYwQGBjp9/MGDB/Hcc88V9mnKQtHgZuDAgbhx4wamTZuGhIQE1KtXD2vXrkWlSpUAAAkJCXZr3mRmZuK1117DlStX4Onpibp162LNmjXo1q14dAFxhWIiouLHU6vB0aldkJKcAl8/X7euUOyplW+pktxBy6effopZs2Zh9uzZqF+/Pry9vTFhwgSHcUa55c6AqFSqfAf+9uzZExEREfj2228RHh4Os9mMevXqWc/j6emZ7/kKut8VFB9QPHbsWIwdO9bpfQsXLrS7/cYbb+CNN95wQ6nuD1coJiIqflQqFbx0HsjSaeCl8yg1l1/Yvn07evfujWeeeQaANNPo9OnTqF27tmznuHHjBk6cOIGvv/4abdu2BQD89ddfdvs0aNAA3333HW7evOk0e9OgQQP8+eefePbZZ2UrV0FKxytcTHCdGyIicpdq1aohNjYWO3bswIkTJzB69Og8l1K5XwEBAShbtiy++eYbnDlzBps2bbKbpAMAgwcPRmhoKPr06YO///4b586dw+rVq7Fz504AwJQpU7B48WLr9PYjR47go48+krWcuTG4kZGa3VJEROQm77zzDpo0aYLHHnsMHTp0sAYYclKr1ViyZAn27duHevXq4ZVXXsHHH39st49Op8OGDRsQHByMbt26oWHDhpg9e7Z1nZoOHTpg2bJlWL16NRo1aoROnTph165dspYzN8W7pUoTdksREdGDGj58uN26M5UrV7YujGcrMDAQq1atyvdYW7Zssbt94cIFh30OHjyY7zEeffRRHD9+3G5b7vJUqlQJv/76KwCpeyw5Odluqne/fv3Qr1+/fM8jJ2ZuZMRuKSIiIuUxuJERL5xJRESkPAY3MlIzc0NERKQ4BjcyYrcUERGR8hjcyIgrFBMRESmPwY2McqaCcxIaERGRUhjcyIjdUkRERMpjcCMjdksREREpj8GNjLhCMRERkfIY3MhIbeYKxUREpJwOHTpgwoQJShdDcQxuZKQCx9wQEVHR9ezZE48++qjT+3bu3AmVSoX9+/e7uVQlF4MbGbFbioiI7sfIkSOxadMmXLx40eG+mJgYNGrUCE2aNFGgZCUTgxsZqdgtRURE96FHjx4IDg7GwoUL7banpaVh6dKlGDlyJG7cuIHBgwejQoUK8PLyQv369bF48eIinefs2bPo3bs3QkJC4OPjg2bNmmHjxo12+2RkZOCNN95AREQE9Ho9qlevjvnz51vvP3bsGLp37w4/Pz/4+vqibdu2OHv27H0/d1fggiwy4uUXiIiKISGAzLuAMQ3I1ABqN36v13oBKlWBu3l4eGDo0KFYuHAh3n33XaiyH7Ns2TJkZmbi6aefRlpaGqKiovCf//wHfn5+WLNmDYYMGYIqVaqgefPmhSpOamoqunXrhunTp8NgMOD7779Hz549cfLkSVSsWBEAMHToUOzcuROff/45GjZsiPPnzyMpKQkAcOXKFbRr1w4dOnTApk2b4Ofnh7///htZWVn3WUGuweBGLkJAbR1zw6ngRETFhjEN6g8qoIwS554cD+i8C7XriBEj8PHHH2PLli3o2LEjAKlLql+/fggICEBAQABee+016/7jxo3D+vXrsWzZskIHNw0bNkTDhg2tt6dPn46VK1di9erVeOmll3Dq1Cn88ssviI2NtY4BqlKlinX/OXPmwN/fH0uWLIFWK32Rr1GjRqHO7U7slpKL2ZjzN1coJiKiIqpVqxZatWqFmJgYAFIX0vbt2zFixAgAgMlkwv/93/+hQYMGKFu2LHx8fLBhwwbExcUV+hx3797FG2+8gTp16qBMmTLw8fHBv//+az3GwYMHodFo0L59e6ePP3jwINq2bWsNbIortsJyMdkEN+yWIiIqPrReML95GckpKfDz9YXa3d1SRTBy5Ei89NJLmDNnDhYsWIBKlSqhc+fOAIBPP/0Us2bNwuzZs1G/fn14e3tjwoQJyMzMLPTxX3/9dfzxxx/45JNPUK1aNXh6eqJ///7WY3h6eub7+ILuLy4Y3MjFLrhhtxQRUbGhUkldQ1qT9NudwU0RDRgwAC+//DJ+/vlnfP/993juuees42+2b9+O3r1745lnngEAmM1mnD59GrVr1y708bdv347hw4ejb9++AKQxOBcuXLDeX79+fZjNZmzdutXp1PQGDRrg+++/h9FoLNbZm+L7Cpc07JYiIqIH5OPjg4EDB2Ly5MmIj4/H8OHDrfdVq1YNsbGx2LFjB06cOIHRo0fj6tWrRTp+tWrVsGLFChw8eBCHDh3CU089BbPZbL2/cuXKGDZsGEaMGIFVq1bh/Pnz2LJlC3755RcAwEsvvYTk5GQMGjQIe/fuxenTp/Hjjz/i5MmTsjx/uTC4kYvZhCy1HkLrXaiR8URERM6MHDkSt27dwqOPPmqdwQQA77zzDpo0aYLHHnsMHTp0QGhoKPr06VOkY8+aNQsBAQFo1aoVevbsiccee8xh/Zx58+ahf//+GDt2LGrVqoXnnnsOd+/eBQCULVsWmzZtQmpqKtq3b4+oqCh8++23xS6LwxSDXHxDsabht+jWrRuK10tMREQlScuWLSGEcNgeGBiIVatW5fvYLVu25Ht/5cqVsWnTJrttL774ot1tg8GAmTNnYubMmU6P0aBBA/zxxx/5nkdpzNwQERFRqcLghoiIiEoVBjdERERUqjC4ISIiolKFwQ0RERGVKgxuiIioVHI244iKN7leMwY3RERUqmg0GgAo0mUJqHiwvGaW1/B+cZ0bIiIqVTw8PODl5YXr169Dq9VCrVbDbDYjMzMT6enp7r221EPofuvabDbj+vXr8PLygofHg4UnDG6IiKhUUalUCAsLw/nz53Hx4kUAUnfHvXv34Onpab1WE7nGg9S1Wq1GxYoVH/g1YnBDRESljk6nQ/Xq1a3dHEajEdu2bUO7du2K3aUCSpsHqWudTidLZo3BDRERlUpqtRoGgwGANIYjKysLBoOBwY2LFYe6ZscjERERlSoMboiIiKhUYXBDREREpcpDN+bGskBQcnKyrMc1Go1IS0tDcnIy+3NdiPXsPqxr92A9uwfr2X1cVdeWdrswC/09dMFNSkoKACAiIkLhkhAREVFRpaSkwN/fP999VOIhW5/abDYjPj4evr6+sq51kJycjIiICFy6dAl+fn6yHZfssZ7dh3XtHqxn92A9u4+r6loIgZSUFISHhxc4Xfyhy9yo1WpUqFDBZcf38/PjP44bsJ7dh3XtHqxn92A9u48r6rqgjI0FBxQTERFRqcLghoiIiEoVBjcy0ev1mDJlCvR6vdJFKdVYz+7DunYP1rN7sJ7dpzjU9UM3oJiIiIhKN2ZuiIiIqFRhcENERESlCoMbIiIiKlUY3BAREVGpwuBGBnPnzkVkZCQMBgOioqKwfft2pYtU4mzbtg09e/ZEeHg4VCoVVq1aZXe/EAJTp05FeHg4PD090aFDBxw7dsxun4yMDIwbNw5BQUHw9vZGr169cPnyZTc+i+JtxowZaNasGXx9fREcHIw+ffrg5MmTdvuwnuUxb948NGjQwLqIWcuWLbFu3Trr/axn15gxYwZUKhUmTJhg3ca6lsfUqVOhUqnsfkJDQ633F7t6FvRAlixZIrRarfj222/F8ePHxcsvvyy8vb3FxYsXlS5aibJ27Vrx1ltvieXLlwsAYuXKlXb3f/DBB8LX11csX75cHDlyRAwcOFCEhYWJ5ORk6z5jxowR5cuXF7GxsWL//v2iY8eOomHDhiIrK8vNz6Z4euyxx8SCBQvE0aNHxcGDB0X37t1FxYoVRWpqqnUf1rM8Vq9eLdasWSNOnjwpTp48KSZPniy0Wq04evSoEIL17Aq7d+8WlStXFg0aNBAvv/yydTvrWh5TpkwRdevWFQkJCdafxMRE6/3FrZ4Z3DygRx55RIwZM8ZuW61atcSbb76pUIlKvtzBjdlsFqGhoeKDDz6wbktPTxf+/v7iq6++EkIIcfv2baHVasWSJUus+1y5ckWo1Wqxfv16t5W9JElMTBQAxNatW4UQrGdXCwgIEN999x3r2QVSUlJE9erVRWxsrGjfvr01uGFdy2fKlCmiYcOGTu8rjvXMbqkHkJmZiX379iE6Otpue3R0NHbs2KFQqUqf8+fP4+rVq3b1rNfr0b59e2s979u3D0aj0W6f8PBw1KtXj69FHu7cuQMACAwMBMB6dhWTyYQlS5bg7t27aNmyJevZBV588UV0794djz76qN121rW8Tp8+jfDwcERGRmLQoEE4d+4cgOJZzw/dhTPllJSUBJPJhJCQELvtISEhuHr1qkKlKn0sdemsni9evGjdR6fTISAgwGEfvhaOhBCYOHEi2rRpg3r16gFgPcvtyJEjaNmyJdLT0+Hj44OVK1eiTp061g9y1rM8lixZgv3792PPnj0O9/E9LZ/mzZvjhx9+QI0aNXDt2jVMnz4drVq1wrFjx4plPTO4kYFKpbK7LYRw2EYP7n7qma+Fcy+99BIOHz6Mv/76y+E+1rM8atasiYMHD+L27dtYvnw5hg0bhq1bt1rvZz0/uEuXLuHll1/Ghg0bYDAY8tyPdf3gunbtav27fv36aNmyJapWrYrvv/8eLVq0AFC86pndUg8gKCgIGo3GIepMTEx0iGDp/llG5OdXz6GhocjMzMStW7fy3Ick48aNw+rVq7F582ZUqFDBup31LC+dTodq1aqhadOmmDFjBho2bIjPPvuM9Syjffv2ITExEVFRUfDw8ICHhwe2bt2Kzz//HB4eHta6Yl3Lz9vbG/Xr18fp06eL5Xuawc0D0Ol0iIqKQmxsrN322NhYtGrVSqFSlT6RkZEIDQ21q+fMzExs3brVWs9RUVHQarV2+yQkJODo0aN8LbIJIfDSSy9hxYoV2LRpEyIjI+3uZz27lhACGRkZrGcZde7cGUeOHMHBgwetP02bNsXTTz+NgwcPokqVKqxrF8nIyMCJEycQFhZWPN/Tsg9RfshYpoLPnz9fHD9+XEyYMEF4e3uLCxcuKF20EiUlJUUcOHBAHDhwQAAQM2fOFAcOHLBOqf/ggw+Ev7+/WLFihThy5IgYPHiw02mGFSpUEBs3bhT79+8XnTp14nROGy+88ILw9/cXW7ZssZvOmZaWZt2H9SyPSZMmiW3btonz58+Lw4cPi8mTJwu1Wi02bNgghGA9u5LtbCkhWNdyefXVV8WWLVvEuXPnxD///CN69OghfH19rW1dcatnBjcymDNnjqhUqZLQ6XSiSZMm1qm1VHibN28WABx+hg0bJoSQphpOmTJFhIaGCr1eL9q1ayeOHDlid4x79+6Jl156SQQGBgpPT0/Ro0cPERcXp8CzKZ6c1S8AsWDBAus+rGd5jBgxwvqZUK5cOdG5c2drYCME69mVcgc3rGt5WNat0Wq1Ijw8XPTr108cO3bMen9xq2eVEELInw8iIiIiUgbH3BAREVGpwuCGiIiIShUGN0RERFSqMLghIiKiUoXBDREREZUqDG6IiIioVGFwQ0RERKUKgxsiIkgX/Vu1apXSxSAiGTC4ISLFDR8+HCqVyuHn8ccfV7poRFQCeShdACIiAHj88cexYMECu216vV6h0hBRScbMDREVC3q9HqGhoXY/AQEBAKQuo3nz5qFr167w9PREZGQkli1bZvf4I0eOoFOnTvD09ETZsmXx/PPPIzU11W6fmJgY1K1bF3q9HmFhYXjppZfs7k9KSkLfvn3h5eWF6tWrY/Xq1a590kTkEgxuiKhEeOedd/DEE0/g0KFDeOaZZzB48GCcOHECAJCWlobHH38cAQEB2LNnD5YtW4aNGzfaBS/z5s3Diy++iOeffx5HjhzB6tWrUa1aNbtzvPfeexgwYAAOHz6Mbt264emnn8bNmzfd+jyJSAYuuRwnEVERDBs2TGg0GuHt7W33M23aNCGEdEXzMWPG2D2mefPm4oUXXhBCCPHNN9+IgIAAkZqaar1/zZo1Qq1Wi6tXrwohhAgPDxdvvfVWnmUAIN5++23r7dTUVKFSqcS6detke55E5B4cc0NExULHjh0xb948u22BgYHWv1u2bGl3X8uWLXHw4EEAwIkTJ9CwYUN4e3tb72/dujXMZjNOnjwJlUqF+Ph4dO7cOd8yNGjQwPq3t7c3fH19kZiYeL9PiYgUwuCGiIoFb29vh26igqhUKgCAEML6t7N9PD09C3U8rVbr8Fiz2VykMhGR8jjmhohKhH/++cfhdq1atQAAderUwcGDB3H37l3r/X///TfUajVq1KgBX19fVK5cGX/++adby0xEymDmhoiKhYyMDFy9etVum4eHB4KCggAAy5YtQ9OmTdGmTRssWrQIu3fvxvz58wEATz/9NKZMmYJhw4Zh6tSpuH79OsaNG4chQ4YgJCQEADB16lSMGTMGwcHB6Nq1K1JSUvD3339j3Lhx7n2iRORyDG6IqFhYv349wsLC7LbVrFkT//77LwBpJtOSJUswduxYhIaGYtGiRahTpw4AwMvLC3/88QdefvllNGvWDF5eXnjiiScwc+ZM67GGDRuG9PR0zJo1C6+99hqCgoLQv39/9z1BInIblRBCKF0IIqL8qFQqrFy5En369FG6KERUAnDMDREREZUqDG6IiIioVOGYGyIq9th7TkRFwcwNERERlSoMboiIiKhUYXBDREREpQqDGyIiIipVGNwQERFRqcLghoiIiEoVBjdERERUqjC4ISIiolKFwQ0RERGVKv8PPVflhpe/IA8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40425/1421969170.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(best_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor     1.0000    0.2700    0.4252       100\n",
      "meningioma_tumor     0.6786    0.9913    0.8057       115\n",
      "        no_tumor     0.7914    1.0000    0.8836       129\n",
      " pituitary_tumor     1.0000    0.8108    0.8955        74\n",
      "\n",
      "        accuracy                         0.7895       418\n",
      "       macro avg     0.8675    0.7680    0.7525       418\n",
      "    weighted avg     0.8472    0.7895    0.7546       418\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 27  46  27   0]\n",
      " [  0 114   1   0]\n",
      " [  0   0 129   0]\n",
      " [  0   8   6  60]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import amp\n",
    "import json\n",
    "\n",
    "seed=2025\n",
    "set_seed(seed)\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "\n",
    "DATA_ROOT = data_root \n",
    "BATCH = 32\n",
    "EPOCHS = 500\n",
    "LR = 3e-4\n",
    "FREEZE_BACKBONE = False  # set True to train only the last layer first\n",
    "\n",
    "#trainset, valset, testset, class_names = build_datasets(DATA_ROOT, img_size=224, val_fraction=0.15)\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "scaler = amp.GradScaler('cuda', enabled=(device.type=='cuda'))\n",
    "\n",
    "train_loader = DataLoader(trainset,batch_size=BATCH,sampler=sampler,num_workers=WORKERS,pin_memory=True)                    # optional: keeps batch size stable)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH*2, shuffle=False,num_workers=WORKERS, pin_memory=True, drop_last=False)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH*2, shuffle=False,num_workers=WORKERS, pin_memory=True, drop_last=False)   \n",
    "\n",
    "print(\"Train uses sampler:\", train_loader.sampler.__class__.__name__)\n",
    "print(\"Val uses sampler:  \", val_loader.sampler.__class__.__name__)\n",
    "print(\"Test uses sampler: \", test_loader.sampler.__class__.__name__)\n",
    "\n",
    "model = build_model(n_classes=len(class_names), freeze_backbone=FREEZE_BACKBONE)\n",
    "model.to(device)\n",
    "\n",
    "#model = CNN512x3(num_classes=4, in_ch=3, dropout=0.5).to(device)\n",
    "#criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "#criterion = FocalLoss(\n",
    "#    gamma=1.5,    # 1.0 = CE; 1.5–2.0 is often best\n",
    "#    reduction='mean'\n",
    "#)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\":   [],\n",
    "    \"train_acc\":  [],\n",
    "    \"val_acc\":    [],\n",
    "    \"val_f1\":  [],\n",
    "    \"train_f1\": [],\n",
    "}\n",
    "\n",
    "best_loss, best_acc, best_f1 = 1e9, 0.0, 0.0\n",
    "best_path_loss = \"best_MobileNetV3_Loss_brain_tumor.pt\"\n",
    "best_path_acc  = \"best_MobileNetV3_Acc_brain_tumor.pt\"\n",
    "best_path_f1   = \"best_MobileNetV3_F1_brain_tumor.pt\"\n",
    "stopper = EarlyStopper(patience=7, min_delta=1e-4)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    #train_loss, train_acc = epoch_run(model, train_loader, criterion, device, train=True, use_amp=True)\n",
    "    #val_loss,   val_acc   = epoch_run(model, val_loader,   criterion, device, train=False, use_amp=True)\n",
    "    \n",
    "    train_loss, train_acc = epoch_run(model, train_loader, criterion, device, train=True, use_amp=True, optimizer=optimizer, scaler=scaler)\n",
    "    val_loss,   val_acc   = epoch_run(model, val_loader,   criterion, device, train=False, use_amp=True)\n",
    "    #scheduler.step(val_loss)\n",
    "    scheduler.step()\n",
    "    val_f1 = compute_f1(model, val_loader, device, average=\"macro\")\n",
    "\n",
    "    # log\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "    # Pretty printing setup\n",
    "    color = \"\"\n",
    "    end_color = \"\\033[0m\"\n",
    "    style_bold = \"\\033[1m\"\n",
    "    suffix_msgs = []\n",
    "\n",
    "    # Check for improvements and save checkpoints\n",
    "    improved_loss = val_loss < best_loss\n",
    "    improved_acc  = val_acc  > best_acc\n",
    "    improved_f1   = val_f1   > best_f1\n",
    "\n",
    "    if improved_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save({\"model\": model.state_dict(), \"classes\": class_names}, best_path_loss)\n",
    "        suffix_msgs.append(\"✅ best loss\")\n",
    "        color = \"\\033[92m\"  # green\n",
    "\n",
    "    if improved_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({\"model\": model.state_dict(), \"classes\": class_names}, best_path_acc)\n",
    "        suffix_msgs.append(\"💙 best acc\")\n",
    "        # prefer blue if only acc improved; keep cyan if multiple\n",
    "        color = \"\\033[94m\" if not (improved_loss or improved_f1) else \"\\033[96m\"\n",
    "\n",
    "    if improved_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save({\"model\": model.state_dict(), \"classes\": class_names}, best_path_f1)\n",
    "        suffix_msgs.append(\"🟣 best F1\")\n",
    "        # magenta if only F1; cyan if multiple\n",
    "        color = \"\\033[95m\" if not (improved_loss or improved_acc) else \"\\033[96m\"\n",
    "\n",
    "    # Compose suffix\n",
    "    suffix = \"\"\n",
    "    if suffix_msgs:\n",
    "        suffix = \" (\" + \" & \".join(suffix_msgs) + \")\"\n",
    "\n",
    "    # Print the formatted line (now includes F1)\n",
    "    print(f\"{color}{style_bold}\"\n",
    "          f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f} F1={val_f1:.4f}\"\n",
    "          f\"{suffix}{end_color}\")\n",
    "\n",
    "    # Early stopping still based on val_acc (unchanged)\n",
    "    if stopper.step(val_acc):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(f\"Best val loss: {best_loss:.4f}. Saved to: {best_path_loss}\")\n",
    "print(f\"Best val acc : {best_acc:.4f}. Saved to: {best_path_acc}\")\n",
    "print(f\"Best val F1  : {best_f1:.4f}. Saved to: {best_path_f1}\")\n",
    "\n",
    "with open(\"history_MobileNetV3.json\", \"w\") as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "\n",
    "\n",
    "# ---- after training: plots ----\n",
    "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"],   label=\"Val loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# (optional) accuracy plot\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"Train acc\")\n",
    "plt.plot(epochs, history[\"val_acc\"],   label=\"Val acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# ----------------------------\n",
    "# Evaluation on test (if available)\n",
    "# ----------------------------\n",
    "if test_loader is not None:\n",
    "    ckpt = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    all_preds, all_tgts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = to_device((xb, yb), device)\n",
    "            logits = model(xb)\n",
    "            all_preds.append(logits.argmax(1).cpu())\n",
    "            all_tgts.append(yb.cpu())\n",
    "\n",
    "\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_tgts).numpy()\n",
    "    print(\"\\nTest classification report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names= class_names, digits=4))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b6d4fa-fe3c-4e53-a3dc-ee67847ee63d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2122417545.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    ckpt = torch.load(best_path_f1, map_location=device)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Here we display the models check points for the different best evaluation metrics F1, Loss and accuracy\n",
    "if test_loader is not None:\n",
    "    ckpt = torch.load(best_path_f1, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    all_preds, all_tgts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = to_device((xb, yb), device)\n",
    "            logits = model(xb)\n",
    "            all_preds.append(logits.argmax(1).cpu())\n",
    "            all_tgts.append(yb.cpu())\n",
    "\n",
    "\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_tgts).numpy()\n",
    "    print(\"\\nTest classification report for best F1 Macro:\")\n",
    "    print(classification_report(y_true, y_pred, target_names= class_names, digits=4))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "if test_loader is not None:\n",
    "    ckpt = torch.load(best_path_acc, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    all_preds, all_tgts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = to_device((xb, yb), device)\n",
    "            logits = model(xb)\n",
    "            all_preds.append(logits.argmax(1).cpu())\n",
    "            all_tgts.append(yb.cpu())\n",
    "\n",
    "\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_tgts).numpy()\n",
    "    print(\"\\nTest classification report for best validation accuracy:\")\n",
    "    print(classification_report(y_true, y_pred, target_names= class_names, digits=4))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "if test_loader is not None:\n",
    "    ckpt = torch.load(best_path_loss, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    all_preds, all_tgts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = to_device((xb, yb), device)\n",
    "            logits = model(xb)\n",
    "            all_preds.append(logits.argmax(1).cpu())\n",
    "            all_tgts.append(yb.cpu())\n",
    "\n",
    "\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_tgts).numpy()\n",
    "    print(\"\\nTest classification report for best validation loss:\")\n",
    "    print(classification_report(y_true, y_pred, target_names= class_names, digits=4))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f3149f-ada0-4bb4-8892-f01037cf1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def epoch_run(model, loader, device, train: bool, use_amp: bool,\n",
    "              optimizer=None, scaler: GradScaler=None):\n",
    "    model.train() if train else model.eval()\n",
    "    ce = nn.CrossEntropyLoss(label_smoothing=0.05).to(device)\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    loss_sum = 0.0\n",
    "    preds_all, tgts_all = [], []\n",
    "\n",
    "    cm = torch.enable_grad if train else torch.no_grad\n",
    "    with cm():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            with autocast(device_type='cuda', enabled=(use_amp and device.type=='cuda')):\n",
    "                out = model(xb)\n",
    "                if isinstance(out, dict):\n",
    "                    logits = out.get(\"logits\", out.get(\"cls\"))\n",
    "                else:\n",
    "                    logits = out\n",
    "                loss = ce(logits, yb)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            total += bs\n",
    "            loss_sum += float(loss.detach()) * bs\n",
    "            pred = logits.argmax(1)\n",
    "            correct += int((pred == yb).sum().item())\n",
    "\n",
    "            preds_all.append(pred.detach().cpu())\n",
    "            tgts_all.append(yb.detach().cpu())\n",
    "\n",
    "    preds_all = torch.cat(preds_all).numpy()\n",
    "    tgts_all  = torch.cat(tgts_all).numpy()\n",
    "    avg_loss = loss_sum / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc, preds_all, tgts_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f2079e-1710-4505-86d3-ff4067c2167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cfed8b2a-ffe6-468c-b351-1cece2ed8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "===== Fold 1 =====\n",
      "Ep 001 | tr_loss 0.5630 acc 0.838 | va_loss 1.0426 acc 0.510 macroF1 0.531\n",
      "Ep 005 | tr_loss 0.2456 acc 0.987 | va_loss 0.4841 acc 0.903 macroF1 0.896\n",
      "Ep 010 | tr_loss 0.2164 acc 0.997 | va_loss 0.2893 acc 0.962 macroF1 0.964\n",
      "Ep 015 | tr_loss 0.2148 acc 0.996 | va_loss 0.2560 acc 0.979 macroF1 0.980\n",
      "Ep 020 | tr_loss 0.2072 acc 0.999 | va_loss 0.2735 acc 0.974 macroF1 0.975\n",
      "Ep 025 | tr_loss 0.2055 acc 1.000 | va_loss 0.2483 acc 0.984 macroF1 0.984\n",
      "Ep 030 | tr_loss 0.2030 acc 1.000 | va_loss 0.2572 acc 0.977 macroF1 0.977\n",
      "Ep 035 | tr_loss 0.2027 acc 1.000 | va_loss 0.2517 acc 0.983 macroF1 0.983\n",
      "Ep 040 | tr_loss 0.2027 acc 1.000 | va_loss 0.2507 acc 0.984 macroF1 0.985\n",
      "Ep 045 | tr_loss 0.2025 acc 1.000 | va_loss 0.2500 acc 0.979 macroF1 0.979\n",
      "Ep 050 | tr_loss 0.2025 acc 1.000 | va_loss 0.2508 acc 0.979 macroF1 0.980\n",
      "\n",
      "Fold 1 results:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor     0.9939    0.9880    0.9909       166\n",
      "meningioma_tumor     0.9810    0.9451    0.9627       164\n",
      "        no_tumor     0.9877    0.9877    0.9877        81\n",
      " pituitary_tumor     0.9593    1.0000    0.9792       165\n",
      "\n",
      "        accuracy                         0.9792       576\n",
      "       macro avg     0.9805    0.9802    0.9801       576\n",
      "    weighted avg     0.9795    0.9792    0.9791       576\n",
      "\n",
      "Confusion matrix:\n",
      " [[164   2   0   0]\n",
      " [  1 155   1   7]\n",
      " [  0   1  80   0]\n",
      " [  0   0   0 165]]\n",
      "\n",
      "===== Fold 2 =====\n",
      "Ep 001 | tr_loss 0.5610 acc 0.855 | va_loss 1.2235 acc 0.479 macroF1 0.417\n",
      "Ep 005 | tr_loss 0.2473 acc 0.990 | va_loss 0.6705 acc 0.778 macroF1 0.764\n",
      "Ep 010 | tr_loss 0.2228 acc 0.993 | va_loss 0.3367 acc 0.946 macroF1 0.948\n",
      "Ep 015 | tr_loss 0.2164 acc 0.997 | va_loss 0.2418 acc 0.984 macroF1 0.986\n",
      "Ep 020 | tr_loss 0.2050 acc 1.000 | va_loss 0.2246 acc 0.988 macroF1 0.988\n",
      "Ep 025 | tr_loss 0.2038 acc 1.000 | va_loss 0.2382 acc 0.983 macroF1 0.983\n",
      "Ep 030 | tr_loss 0.2057 acc 0.999 | va_loss 0.2255 acc 0.991 macroF1 0.991\n",
      "Ep 035 | tr_loss 0.2025 acc 1.000 | va_loss 0.2272 acc 0.990 macroF1 0.989\n",
      "Ep 040 | tr_loss 0.2025 acc 1.000 | va_loss 0.2234 acc 0.990 macroF1 0.989\n",
      "Ep 045 | tr_loss 0.2024 acc 1.000 | va_loss 0.2233 acc 0.991 macroF1 0.991\n",
      "Ep 050 | tr_loss 0.2024 acc 1.000 | va_loss 0.2293 acc 0.991 macroF1 0.991\n",
      "\n",
      "Fold 2 results:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor     0.9940    0.9940    0.9940       166\n",
      "meningioma_tumor     0.9818    0.9878    0.9848       164\n",
      "        no_tumor     0.9756    1.0000    0.9877        80\n",
      " pituitary_tumor     1.0000    0.9819    0.9909       166\n",
      "\n",
      "        accuracy                         0.9896       576\n",
      "       macro avg     0.9879    0.9909    0.9893       576\n",
      "    weighted avg     0.9897    0.9896    0.9896       576\n",
      "\n",
      "Confusion matrix:\n",
      " [[165   1   0   0]\n",
      " [  1 162   1   0]\n",
      " [  0   0  80   0]\n",
      " [  0   2   1 163]]\n",
      "\n",
      "===== Fold 3 =====\n",
      "Ep 001 | tr_loss 0.5599 acc 0.836 | va_loss 1.0356 acc 0.524 macroF1 0.495\n",
      "Ep 005 | tr_loss 0.2502 acc 0.987 | va_loss 0.7488 acc 0.708 macroF1 0.701\n",
      "Ep 010 | tr_loss 0.2194 acc 0.998 | va_loss 0.3331 acc 0.946 macroF1 0.949\n",
      "Ep 015 | tr_loss 0.2056 acc 1.000 | va_loss 0.2509 acc 0.981 macroF1 0.982\n",
      "Ep 020 | tr_loss 0.2078 acc 0.999 | va_loss 0.3135 acc 0.962 macroF1 0.965\n",
      "Ep 025 | tr_loss 0.2104 acc 0.997 | va_loss 0.2519 acc 0.977 macroF1 0.979\n",
      "Ep 030 | tr_loss 0.2034 acc 1.000 | va_loss 0.2429 acc 0.984 macroF1 0.985\n",
      "Ep 035 | tr_loss 0.2027 acc 1.000 | va_loss 0.2435 acc 0.984 macroF1 0.986\n",
      "Ep 040 | tr_loss 0.2026 acc 1.000 | va_loss 0.2338 acc 0.990 macroF1 0.991\n",
      "Ep 045 | tr_loss 0.2023 acc 1.000 | va_loss 0.2367 acc 0.988 macroF1 0.989\n",
      "Ep 050 | tr_loss 0.2024 acc 1.000 | va_loss 0.2398 acc 0.986 macroF1 0.987\n",
      "\n",
      "Fold 3 results:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor     0.9940    0.9940    0.9940       166\n",
      "meningioma_tumor     0.9702    0.9939    0.9819       164\n",
      "        no_tumor     1.0000    0.9875    0.9937        80\n",
      " pituitary_tumor     1.0000    0.9819    0.9909       166\n",
      "\n",
      "        accuracy                         0.9896       576\n",
      "       macro avg     0.9911    0.9893    0.9901       576\n",
      "    weighted avg     0.9898    0.9896    0.9896       576\n",
      "\n",
      "Confusion matrix:\n",
      " [[165   1   0   0]\n",
      " [  1 163   0   0]\n",
      " [  0   1  79   0]\n",
      " [  0   3   0 163]]\n",
      "\n",
      "===== Fold 4 =====\n",
      "Ep 001 | tr_loss 0.5579 acc 0.845 | va_loss 1.1704 acc 0.523 macroF1 0.487\n",
      "Ep 005 | tr_loss 0.2378 acc 0.990 | va_loss 0.5436 acc 0.845 macroF1 0.843\n",
      "Ep 010 | tr_loss 0.2170 acc 0.997 | va_loss 0.3665 acc 0.934 macroF1 0.937\n",
      "Ep 015 | tr_loss 0.2127 acc 0.998 | va_loss 0.2415 acc 0.983 macroF1 0.982\n",
      "Ep 020 | tr_loss 0.2065 acc 1.000 | va_loss 0.2457 acc 0.984 macroF1 0.983\n",
      "Ep 025 | tr_loss 0.2033 acc 1.000 | va_loss 0.2468 acc 0.977 macroF1 0.977\n",
      "Ep 030 | tr_loss 0.2031 acc 1.000 | va_loss 0.2433 acc 0.983 macroF1 0.983\n",
      "Ep 035 | tr_loss 0.2033 acc 1.000 | va_loss 0.2397 acc 0.981 macroF1 0.981\n",
      "Ep 040 | tr_loss 0.2027 acc 1.000 | va_loss 0.2316 acc 0.984 macroF1 0.984\n",
      "Ep 045 | tr_loss 0.2023 acc 1.000 | va_loss 0.2383 acc 0.984 macroF1 0.984\n",
      "Ep 050 | tr_loss 0.2023 acc 1.000 | va_loss 0.2374 acc 0.983 macroF1 0.982\n",
      "\n",
      "Fold 4 results:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor     0.9645    0.9879    0.9760       165\n",
      "meningioma_tumor     0.9812    0.9515    0.9662       165\n",
      "        no_tumor     0.9524    1.0000    0.9756        80\n",
      " pituitary_tumor     1.0000    0.9818    0.9908       165\n",
      "\n",
      "        accuracy                         0.9774       575\n",
      "       macro avg     0.9745    0.9803    0.9772       575\n",
      "    weighted avg     0.9778    0.9774    0.9774       575\n",
      "\n",
      "Confusion matrix:\n",
      " [[163   2   0   0]\n",
      " [  5 157   3   0]\n",
      " [  0   0  80   0]\n",
      " [  1   1   1 162]]\n",
      "\n",
      "===== Fold 5 =====\n",
      "Ep 001 | tr_loss 0.5410 acc 0.851 | va_loss 1.6541 acc 0.235 macroF1 0.207\n",
      "Ep 005 | tr_loss 0.2450 acc 0.990 | va_loss 0.8603 acc 0.678 macroF1 0.686\n",
      "Ep 010 | tr_loss 0.2178 acc 0.996 | va_loss 0.3436 acc 0.936 macroF1 0.939\n",
      "Ep 015 | tr_loss 0.2106 acc 0.998 | va_loss 0.2774 acc 0.967 macroF1 0.968\n",
      "Ep 020 | tr_loss 0.2071 acc 1.000 | va_loss 0.2471 acc 0.983 macroF1 0.981\n",
      "Ep 025 | tr_loss 0.2040 acc 1.000 | va_loss 0.2353 acc 0.990 macroF1 0.989\n",
      "Ep 030 | tr_loss 0.2029 acc 1.000 | va_loss 0.2327 acc 0.990 macroF1 0.989\n",
      "Ep 035 | tr_loss 0.2027 acc 1.000 | va_loss 0.2404 acc 0.983 macroF1 0.981\n",
      "Ep 040 | tr_loss 0.2024 acc 1.000 | va_loss 0.2340 acc 0.988 macroF1 0.987\n",
      "Ep 045 | tr_loss 0.2022 acc 1.000 | va_loss 0.2364 acc 0.986 macroF1 0.985\n",
      "Ep 050 | tr_loss 0.2026 acc 1.000 | va_loss 0.2338 acc 0.988 macroF1 0.986\n",
      "\n",
      "Fold 5 results:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor     0.9939    0.9879    0.9909       165\n",
      "meningioma_tumor     0.9702    0.9879    0.9790       165\n",
      "        no_tumor     0.9750    0.9750    0.9750        80\n",
      " pituitary_tumor     1.0000    0.9879    0.9939       165\n",
      "\n",
      "        accuracy                         0.9861       575\n",
      "       macro avg     0.9848    0.9847    0.9847       575\n",
      "    weighted avg     0.9862    0.9861    0.9861       575\n",
      "\n",
      "Confusion matrix:\n",
      " [[163   2   0   0]\n",
      " [  1 163   1   0]\n",
      " [  0   2  78   0]\n",
      " [  0   1   1 163]]\n",
      "\n",
      "===== CV Summary over 5 folds =====\n",
      "Accuracy  : mean 0.9844 ± 0.0052\n",
      "Macro-F1  : mean 0.9843 ± 0.0051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "\n",
    "def make_class_weights(y_train, beta=0.999):\n",
    "    counts = np.bincount(y_train, minlength=num_classes).astype(np.float32)\n",
    "    counts[counts==0] = 1.0\n",
    "    eff_num = 1.0 - np.power(beta, counts)\n",
    "    w = (1.0 - beta) / eff_num\n",
    "    w = w / w.sum() * num_classes\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "set_seed(2025)\n",
    "EPOCHS = 50                # <-- tune\n",
    "BATCH  = 32                # <-- tune\n",
    "WORKERS = 6                # <-- tune\n",
    "LR = 3e-4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2025)\n",
    "\n",
    "fold_results = []\n",
    "all_fold_reports = []\n",
    "all_fold_confmats = []\n",
    "fold = 0\n",
    "dataset=datasets.ImageFolder(tr_base, transform=train_tf)\n",
    "y_all = []\n",
    "for i in range(len(dataset)):\n",
    "    _, y = dataset[i]\n",
    "    y_all.append(int(y))\n",
    "y_all = np.array(y_all)\n",
    "\n",
    "for tr_idx, va_idx in skf.split(np.arange(len(dataset)), y_all):\n",
    "    fold += 1\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    ds_tr = Subset(dataset, tr_idx)\n",
    "    ds_va = Subset(dataset, va_idx)\n",
    "\n",
    "    # WeightedRandomSampler per fold (optional but good)\n",
    "    y_tr = y_all[tr_idx]\n",
    "    class_counts = np.bincount(y_tr, minlength=num_classes)\n",
    "    weights_per_class = 1.0 / np.maximum(class_counts, 1)\n",
    "    sample_weights = weights_per_class[y_tr]\n",
    "    sampler = WeightedRandomSampler(torch.from_numpy(sample_weights).double(),\n",
    "                                    len(sample_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(ds_tr, batch_size=BATCH, sampler=sampler,\n",
    "                              num_workers=WORKERS, pin_memory=True)\n",
    "    val_loader   = DataLoader(ds_va, batch_size=BATCH*2, shuffle=False,\n",
    "                              num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "    # Build model and criterion with fold-wise weights\n",
    "    model = build_model(num_classes).to(device)\n",
    "\n",
    "    # set classifier bias to log-priors (optional)\n",
    "    priors = torch.tensor(np.bincount(y_tr, minlength=num_classes) / len(y_tr), dtype=torch.float32).clamp_min(1e-6)\n",
    "    with torch.no_grad():\n",
    "        model.classifier[3].bias.copy_(priors.log().to(device))\n",
    "\n",
    "    class_w = make_class_weights(y_tr).to(device)\n",
    "    # plug into epoch_run via global CE? (kept inside epoch_run for brevity)\n",
    "    # If you want weights inside CE: modify epoch_run to ce = CrossEntropyLoss(weight=class_w, ...)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    scaler = GradScaler(device='cuda', enabled=(device.type == 'cuda'))\n",
    "    \n",
    "    best_macro_f1 = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        tr_loss, tr_acc, _, _ = epoch_run(model, train_loader, device, train=True, use_amp=True,\n",
    "                                          optimizer=optimizer, scaler=scaler)\n",
    "        va_loss, va_acc, va_pred, va_true = epoch_run(model, val_loader, device, train=False, use_amp=False)\n",
    "\n",
    "        macro_f1 = f1_score(va_true, va_pred, average='macro')\n",
    "        if macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = macro_f1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if ep % 5 == 0 or ep == 1:\n",
    "            print(f\"Ep {ep:03d} | tr_loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "                  f\"va_loss {va_loss:.4f} acc {va_acc:.3f} macroF1 {macro_f1:.3f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    # Evaluate best model on this fold’s val\n",
    "    model.load_state_dict(best_state)\n",
    "    va_loss, va_acc, va_pred, va_true = epoch_run(model, val_loader, device, train=False, use_amp=False)\n",
    "\n",
    "    report = classification_report(va_true, va_pred, target_names=class_names, digits=4, zero_division=0)\n",
    "    cm = confusion_matrix(va_true, va_pred, labels=np.arange(num_classes))\n",
    "\n",
    "    print(f\"\\nFold {fold} results:\")\n",
    "    print(report)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "    all_fold_reports.append(report)\n",
    "    all_fold_confmats.append(cm)\n",
    "    fold_results.append({\n",
    "        \"fold\": fold,\n",
    "        \"val_acc\": va_acc,\n",
    "        \"val_macro_f1\": f1_score(va_true, va_pred, average='macro')\n",
    "    })\n",
    "\n",
    "# Summary across folds\n",
    "val_accs = [r[\"val_acc\"] for r in fold_results]\n",
    "val_f1s  = [r[\"val_macro_f1\"] for r in fold_results]\n",
    "print(\"\\n===== CV Summary over 5 folds =====\")\n",
    "print(f\"Accuracy  : mean {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
    "print(f\"Macro-F1  : mean {np.mean(val_f1s):.4f} ± {np.std(val_f1s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21aaa261-5822-471f-b688-6c1e91d0c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pulp-web-svc.pulp-system.svc.cluster.local:24880/pypi/pypi/simple/\n",
      "Requirement already satisfied: grad-cam in /opt/conda/lib/python3.11/site-packages (1.5.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from grad-cam) (2.2.6)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from grad-cam) (10.4.0)\n",
      "Requirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.11/site-packages (from grad-cam) (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.11/site-packages (from grad-cam) (0.20.1+cu124)\n",
      "Requirement already satisfied: ttach in /opt/conda/lib/python3.11/site-packages (from grad-cam) (0.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from grad-cam) (4.66.5)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (from grad-cam) (4.12.0.88)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from grad-cam) (3.8.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from grad-cam) (1.7.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->grad-cam) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->grad-cam) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->grad-cam) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->grad-cam) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->grad-cam) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->grad-cam) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib->grad-cam) (2.9.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->grad-cam) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->grad-cam) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->grad-cam) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ca7a16-8aa0-40d3-a1ea-827015b35572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14450/1448334015.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(best_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "best_path =\"best_MobileNetV3_Loss_brain_tumor.pt\"\n",
    "\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "\n",
    "#ckpt = torch.load(best_path, map_location=\"cpu\")   # force CPU\n",
    "#model.load_state_dict(ckpt[\"model\"])\n",
    "#model.to(device)\n",
    "#model.eval()\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "target_module = model.features[-1]  # last conv block\n",
    "\n",
    "acts = []\n",
    "grads = []\n",
    "\n",
    "def fwd_hook(module, inp, out):\n",
    "    acts.append(out.detach())            # (B,C,H,W)\n",
    "\n",
    "def bwd_hook(module, grad_in, grad_out):\n",
    "    grads.append(grad_out[0].detach())   # (B,C,H,W)\n",
    "\n",
    "fh = target_module.register_forward_hook(fwd_hook)\n",
    "bh = target_module.register_full_backward_hook(bwd_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb2a700-3dca-4014-b1d7-3b1acde0d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_single(xb, class_idx=None):\n",
    "    acts.clear(); grads.clear()\n",
    "    xb = xb.to(device).requires_grad_(True)\n",
    "    out = model(xb)\n",
    "    logits = out[\"logits\"] if isinstance(out, dict) else out\n",
    "    if class_idx is None:\n",
    "        class_idx = int(logits.argmax(1).item())\n",
    "\n",
    "    score = logits[:, class_idx].sum()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    score.backward(retain_graph=False)\n",
    "\n",
    "    A = acts[0]          # (1,C,H,W)\n",
    "    G = grads[0]         # (1,C,H,W)\n",
    "    weights = G.mean(dim=(2,3), keepdim=True)   # (1,C,1,1)\n",
    "    cam = (weights * A).sum(dim=1, keepdim=True)  # (1,1,H,W)\n",
    "    cam = F.relu(cam)\n",
    "    cam = cam.squeeze().cpu().numpy()\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "    return cam  # (H,W) in [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c69579f-ac11-4c10-b1a6-bf8bdc346b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def overlay_cam(img_rgb_uint8, cam01):\n",
    "    # img_rgb_uint8: (H,W,3) uint8 original (for visualization)\n",
    "    img_float = img_rgb_uint8.astype(np.float32) / 255.0\n",
    "    vis = show_cam_on_image(img_float, cam01, use_rgb=True)\n",
    "    return vis  # uint8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b585787c-190d-4c2c-b74f-2a929519e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m xb \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (1,3,H,W)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/models/mobilenetv3.py:220\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/models/mobilenetv3.py:210\u001b[0m, in \u001b[0;36mMobileNetV3._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image  # if you use this helper\n",
    "\n",
    "# Suppose you have a dataset/transform; take one sample:\n",
    "img, y = testset[136]                 # img: tensor (3,H,W) after your eval transform (normalized)\n",
    "print(device)\n",
    "xb = img.unsqueeze(0)  # (1,3,H,W)\n",
    "\n",
    "model.to(device)\n",
    "out = model(xb).to(device)\n",
    "logits = out[\"logits\"] if isinstance(out, dict) else out\n",
    "print(logits)\n",
    "\n",
    "probs = torch.softmax(logits, dim=1).squeeze(0)      # [4]\n",
    "y_pred = int(probs.argmax().item())\n",
    "p_pred = float(probs[y_pred].item())\n",
    "\n",
    "true_label = class_names[y]            # y from your dataset[idx]\n",
    "pred_label = class_names[y_pred]\n",
    "print(true_label)\n",
    "print(pred_label)\n",
    "print(p_pred)\n",
    "\n",
    "# Grad-CAM for predicted class \n",
    "cam = gradcam_single(xb)\n",
    "\n",
    "# For a nice overlay, denormalize to get displayable RGB:\n",
    "mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]  # adjust if you use others\n",
    "img_denorm = img.cpu().permute(1,2,0).numpy()\n",
    "img_denorm = np.clip(img_denorm, 0, 1)\n",
    "img_uint8 = (img_denorm*255).astype('uint8')\n",
    "\n",
    "# cam: (Hc, Wc) in [0,1]\n",
    "# img_uint8: (H, W, 3) uint8\n",
    "H, W = img_uint8.shape[:2]\n",
    "cam_up = cv2.resize(cam, (W, H), interpolation=cv2.INTER_CUBIC)\n",
    "cam_up = np.clip(cam_up, 0, 1)\n",
    "\n",
    "# if you already have img_float in [0,1]:\n",
    "img_float = img_uint8.astype(np.float32) / 255.0\n",
    "overlay = show_cam_on_image(img_float, cam_up, use_rgb=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_uint8)\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Grad-CAM Overlay\")\n",
    "plt.text(10, 28,\n",
    "         f\"T: {true_label}\\nP: {pred_label} ({p_pred:.2f})\",\n",
    "         color='white', fontsize=11, weight='bold',\n",
    "         bbox=dict(facecolor='black', alpha=0.6, boxstyle='round,pad=0.4'))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"GradCam_test.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be2522a-db60-4dfd-85a2-1b03374f3dd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate best model on this fold’s val\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mbest_state\u001b[49m)\n\u001b[1;32m      3\u001b[0m va_loss, va_acc, va_pred, va_true \u001b[38;5;241m=\u001b[39m epoch_run(model, test_loader, device, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_amp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(va_true, va_pred, target_names\u001b[38;5;241m=\u001b[39mclass_names, digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_state' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate best model on this fold’s val\n",
    "model.load_state_dict(best_state)\n",
    "va_loss, va_acc, va_pred, va_true = epoch_run(model, test_loader, device, train=False, use_amp=False)\n",
    "\n",
    "report = classification_report(va_true, va_pred, target_names=class_names, digits=4, zero_division=0)\n",
    "cm = confusion_matrix(va_true, va_pred, labels=np.arange(num_classes))\n",
    "print(report)\n",
    "print(\"Confusion matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ae94a-e1a7-45a2-8ed5-17cb54443433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
